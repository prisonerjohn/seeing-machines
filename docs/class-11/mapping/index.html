<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://seeingmachines.betamovement.net/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://seeingmachines.betamovement.net/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://seeingmachines.betamovement.net/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=https://seeingmachines.betamovement.net/main.201417ef09ab1feedca13aa2c1421e70dafad1589ed8f36457fc63462610ceb90fb17b3e6fe16933d580ec7826300f5b3adf0ff68333dbf5687d81dc470dd423.css integrity="sha512-IBQX7wmrH+7coTqiwUIecNr60Vie2PNkV/xjRiYQzrkPsXs+b+FpM9WA7HgmMA9bOt8P9oMz2/VofYHcRw3UIw==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Mapping - Seeing Machines</title><meta name=description content="OpenCV includes many functions for calibrating and mapping points and images between different spaces.
Homography # A homography is a transformation that reorients one image to match or fit inside another:
The two images are usually two points of view of the same subject. Common features are found in both images and paired to make a data set. Feature Matching + Homography to find Objects The data set is then used with the cv::findHomography function, which returns a matrix that allows us to warp one image into the other."><link rel=canonical href=https://seeingmachines.betamovement.net/docs/class-11/mapping/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Mapping"><meta property="og:description" content="OpenCV includes many functions for calibrating and mapping points and images between different spaces.
Homography # A homography is a transformation that reorients one image to match or fit inside another:
The two images are usually two points of view of the same subject. Common features are found in both images and paired to make a data set. Feature Matching + Homography to find Objects The data set is then used with the cv::findHomography function, which returns a matrix that allows us to warp one image into the other."><meta property="og:url" content="https://seeingmachines.betamovement.net/docs/class-11/mapping/"><meta property="og:site_name" content="Seeing Machines"><meta property="article:published_time" content="2022-11-28T14:33:17-05:00"><meta property="article:modified_time" content="2022-11-28T14:33:17-05:00"><meta property="og:image" content="https://seeingmachines.betamovement.net/default-image.png"><meta property="og:image:alt" content="Seeing Machines"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content="@prisonerjohn"><meta name=twitter:title content="Mapping"><meta name=twitter:description content><meta name=twitter:image content="https://seeingmachines.betamovement.net/default-image.png"><meta name=twitter:image:alt content="Mapping"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://seeingmachines.betamovement.net/#/schema/person/1","name":"Seeing Machines","url":"https://seeingmachines.betamovement.net/","sameAs":[],"image":{"@type":"ImageObject","@id":"https://seeingmachines.betamovement.net/#/schema/image/1","url":"https://seeingmachines.betamovement.net/default-image.png","width":1024,"height":768,"caption":"Seeing Machines"}},{"@type":"WebSite","@id":"https://seeingmachines.betamovement.net/#/schema/website/1","url":"https://seeingmachines.betamovement.net/","name":"Seeing Machines","description":"A programming course where weâ€™ll explore various techniques and solutions for tracking and sensing people or objects in space.","publisher":{"@id":"https://seeingmachines.betamovement.net/#/schema/person/1"}},{"@type":"WebPage","@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/","url":"https://seeingmachines.betamovement.net/docs/class-11/mapping/","name":"Mapping","description":"","isPartOf":{"@id":"https://seeingmachines.betamovement.net/#/schema/website/1"},"about":{"@id":"https://seeingmachines.betamovement.net/#/schema/person/1"},"datePublished":"2022-11-28T14:33:17CET","dateModified":"2022-11-28T14:33:17CET","breadcrumb":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://seeingmachines.betamovement.net/docs/class-11/mapping/"]}]},{"@type":"BreadcrumbList","@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://seeingmachines.betamovement.net/","url":"https://seeingmachines.betamovement.net/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://seeingmachines.betamovement.net/docs/","url":"https://seeingmachines.betamovement.net/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://seeingmachines.betamovement.net/docs/class-11/","url":"https://seeingmachines.betamovement.net/docs/class-11/","name":"Class 11"}},{"@type":"ListItem","position":4,"item":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://seeingmachines.betamovement.net/#/schema/article/1","headline":"Mapping","description":"","isPartOf":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/"},"mainEntityOfPage":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/"},"datePublished":"2022-11-28T14:33:17CET","dateModified":"2022-11-28T14:33:17CET","author":{"@id":"https://seeingmachines.betamovement.net/#/schema/person/2"},"publisher":{"@id":"https://seeingmachines.betamovement.net/#/schema/person/1"},"image":{"@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://seeingmachines.betamovement.net/#/schema/person/2","name":"Elie Zananiri","sameAs":["https://twitter.com/prisonerjohn","https://www.linkedin.com/in/prisonerjohn/","https://github.com/prisonerjohn"]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://seeingmachines.betamovement.net/docs/class-11/mapping/#/schema/image/2","url":"https://seeingmachines.betamovement.net/default-image.png","contentUrl":"https://seeingmachines.betamovement.net/default-image.png","caption":"Mapping"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=https://seeingmachines.betamovement.net/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=https://seeingmachines.betamovement.net/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=https://seeingmachines.betamovement.net/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://seeingmachines.betamovement.net/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://seeingmachines.betamovement.net/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://seeingmachines.betamovement.net/site.webmanifest></head><body class="docs single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=https://seeingmachines.betamovement.net/ aria-label="Seeing Machines">Seeing Machines</a>
<button class="btn btn-link order-0 ms-auto d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasExample aria-controls=offcanvasExample><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-more-horizontal"><circle cx="12" cy="12" r="1"/><circle cx="19" cy="12" r="1"/><circle cx="5" cy="12" r="1"/></svg></button><div class="offcanvas offcanvas-start d-lg-none" tabindex=-1 id=offcanvasExample aria-labelledby=offcanvasExampleLabel><div class=header-bar></div><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasExampleLabel>Browse docs</h5><button type=button class=btn-close data-bs-dismiss=offcanvas aria-label=Close></button></div><div class=offcanvas-body><aside class="doks-sidebar mt-n3"><nav id=doks-docs-nav aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-f7e883054c348d897063924d41a7ec2d aria-expanded=false>
Class 0</button><div class=collapse id=section-f7e883054c348d897063924d41a7ec2d><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/foreword/>Foreword</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/intro-to-of/>Intro to OF</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e43c4a6cb52ad623673f8e77a5b10104 aria-expanded=false>
Class 1</button><div class=collapse id=section-e43c4a6cb52ad623673f8e77a5b10104><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/variables-and-arrays/>Variables and Arrays</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/images-and-video/>Images and Video</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-2ef5a7f02774e1be242988dba4c3056c aria-expanded=false>
Class 2</button><div class=collapse id=section-2ef5a7f02774e1be242988dba4c3056c><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-2/computer-vision/>Computer Vision</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a3be28bcd9707b6b8ff8ac55a6b8cb2 aria-expanded=false>
Class 3</button><div class=collapse id=section-6a3be28bcd9707b6b8ff8ac55a6b8cb2><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/intro-to-opencv/>Intro to OpenCV</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/object-tracking/>Object Tracking</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e01cc216a97c1db953e2304e6aa8998a aria-expanded=false>
Class 4</button><div class=collapse id=section-e01cc216a97c1db953e2304e6aa8998a><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/logging/>Logging</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/depth-sensing/>Depth Sensing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-cb8d2462ebd86364c3502e7084b7f391 aria-expanded=false>
Class 5</button><div class=collapse id=section-cb8d2462ebd86364c3502e7084b7f391><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/pointers/>Pointers</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/depth-images/>Depth Images</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c5400dee9e161c6d7e46af7661005794 aria-expanded=false>
Class 6</button><div class=collapse id=section-c5400dee9e161c6d7e46af7661005794><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-6/depth-world/>Depth World</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ff9150b580f893ddadd43394cb6173a3 aria-expanded=false>
Class 7</button><div class=collapse id=section-ff9150b580f893ddadd43394cb6173a3><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/networking/>Networking</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/texture-sharing/>Texture Sharing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c28d8545333b2785a226d11aa3b3b4ed aria-expanded=false>
Class 8</button><div class=collapse id=section-c28d8545333b2785a226d11aa3b3b4ed><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/draw-bounds/>Draw Bounds</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/frame-buffers/>Frame Buffers</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0bbdd9311c2cb50588b0a462a5438610 aria-expanded=false>
Class 9</button><div class=collapse id=section-0bbdd9311c2cb50588b0a462a5438610><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/classes/>Classes</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/mobile-development/>Mobile Development</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c75af093fd3f2e096e4fe21c7e51e7c7 aria-expanded=false>
Class 10</button><div class=collapse id=section-c75af093fd3f2e096e4fe21c7e51e7c7><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-10/machine-learning/>Machine Learning</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ddbaeabfbe390c84589d5d1c423b9284 aria-expanded=true>
Class 11</button><div class="collapse show" id=section-ddbaeabfbe390c84589d5d1c423b9284><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=https://seeingmachines.betamovement.net/docs/class-11/mapping/>Mapping</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-56c4b82e7a8861de86a5ebe5eaa62225 aria-expanded=false>
Assignments</button><div class=collapse id=section-56c4b82e7a8861de86a5ebe5eaa62225><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-1/>Assignment 1</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-2/>Assignment 2</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-3/>Assignment 3</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-4/>Assignment 4</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/final-project/>Final Project</a></li></ul></div></li></ul></nav></aside></div></div><button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=https://seeingmachines.betamovement.net/>Seeing Machines</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=https://seeingmachines.betamovement.net/docs/class-0/foreword>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://seeingmachines.betamovement.net/docs/assignments>Assignments</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=https://github.com/prisonerjohn/seeing-machines><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-lg-none">GitHub</small></a></li><li class=nav-item><a class="nav-link social-link" href=https://twitter.com/prisonerjohn><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-twitter"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg><small class="ms-2 d-lg-none">Twitter</small></a></li></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-f7e883054c348d897063924d41a7ec2d aria-expanded=false>
Class 0</button><div class=collapse id=section-f7e883054c348d897063924d41a7ec2d><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/foreword/>Foreword</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/intro-to-of/>Intro to OF</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e43c4a6cb52ad623673f8e77a5b10104 aria-expanded=false>
Class 1</button><div class=collapse id=section-e43c4a6cb52ad623673f8e77a5b10104><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/variables-and-arrays/>Variables and Arrays</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/images-and-video/>Images and Video</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-2ef5a7f02774e1be242988dba4c3056c aria-expanded=false>
Class 2</button><div class=collapse id=section-2ef5a7f02774e1be242988dba4c3056c><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-2/computer-vision/>Computer Vision</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a3be28bcd9707b6b8ff8ac55a6b8cb2 aria-expanded=false>
Class 3</button><div class=collapse id=section-6a3be28bcd9707b6b8ff8ac55a6b8cb2><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/intro-to-opencv/>Intro to OpenCV</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/object-tracking/>Object Tracking</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e01cc216a97c1db953e2304e6aa8998a aria-expanded=false>
Class 4</button><div class=collapse id=section-e01cc216a97c1db953e2304e6aa8998a><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/logging/>Logging</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/depth-sensing/>Depth Sensing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-cb8d2462ebd86364c3502e7084b7f391 aria-expanded=false>
Class 5</button><div class=collapse id=section-cb8d2462ebd86364c3502e7084b7f391><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/pointers/>Pointers</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/depth-images/>Depth Images</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c5400dee9e161c6d7e46af7661005794 aria-expanded=false>
Class 6</button><div class=collapse id=section-c5400dee9e161c6d7e46af7661005794><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-6/depth-world/>Depth World</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ff9150b580f893ddadd43394cb6173a3 aria-expanded=false>
Class 7</button><div class=collapse id=section-ff9150b580f893ddadd43394cb6173a3><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/networking/>Networking</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/texture-sharing/>Texture Sharing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c28d8545333b2785a226d11aa3b3b4ed aria-expanded=false>
Class 8</button><div class=collapse id=section-c28d8545333b2785a226d11aa3b3b4ed><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/draw-bounds/>Draw Bounds</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/frame-buffers/>Frame Buffers</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0bbdd9311c2cb50588b0a462a5438610 aria-expanded=false>
Class 9</button><div class=collapse id=section-0bbdd9311c2cb50588b0a462a5438610><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/classes/>Classes</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/mobile-development/>Mobile Development</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c75af093fd3f2e096e4fe21c7e51e7c7 aria-expanded=false>
Class 10</button><div class=collapse id=section-c75af093fd3f2e096e4fe21c7e51e7c7><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-10/machine-learning/>Machine Learning</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ddbaeabfbe390c84589d5d1c423b9284 aria-expanded=true>
Class 11</button><div class="collapse show" id=section-ddbaeabfbe390c84589d5d1c423b9284><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=https://seeingmachines.betamovement.net/docs/class-11/mapping/>Mapping</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-56c4b82e7a8861de86a5ebe5eaa62225 aria-expanded=false>
Assignments</button><div class=collapse id=section-56c4b82e7a8861de86a5ebe5eaa62225><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-1/>Assignment 1</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-2/>Assignment 2</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-3/>Assignment 3</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-4/>Assignment 4</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/final-project/>Final Project</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-f7e883054c348d897063924d41a7ec2d aria-expanded=false>
Class 0</button><div class=collapse id=section-f7e883054c348d897063924d41a7ec2d><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/foreword/>Foreword</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-0/intro-to-of/>Intro to OF</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e43c4a6cb52ad623673f8e77a5b10104 aria-expanded=false>
Class 1</button><div class=collapse id=section-e43c4a6cb52ad623673f8e77a5b10104><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/variables-and-arrays/>Variables and Arrays</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-1/images-and-video/>Images and Video</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-2ef5a7f02774e1be242988dba4c3056c aria-expanded=false>
Class 2</button><div class=collapse id=section-2ef5a7f02774e1be242988dba4c3056c><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-2/computer-vision/>Computer Vision</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a3be28bcd9707b6b8ff8ac55a6b8cb2 aria-expanded=false>
Class 3</button><div class=collapse id=section-6a3be28bcd9707b6b8ff8ac55a6b8cb2><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/intro-to-opencv/>Intro to OpenCV</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-3/object-tracking/>Object Tracking</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e01cc216a97c1db953e2304e6aa8998a aria-expanded=false>
Class 4</button><div class=collapse id=section-e01cc216a97c1db953e2304e6aa8998a><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/logging/>Logging</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-4/depth-sensing/>Depth Sensing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-cb8d2462ebd86364c3502e7084b7f391 aria-expanded=false>
Class 5</button><div class=collapse id=section-cb8d2462ebd86364c3502e7084b7f391><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/pointers/>Pointers</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-5/depth-images/>Depth Images</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c5400dee9e161c6d7e46af7661005794 aria-expanded=false>
Class 6</button><div class=collapse id=section-c5400dee9e161c6d7e46af7661005794><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-6/depth-world/>Depth World</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ff9150b580f893ddadd43394cb6173a3 aria-expanded=false>
Class 7</button><div class=collapse id=section-ff9150b580f893ddadd43394cb6173a3><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/networking/>Networking</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-7/texture-sharing/>Texture Sharing</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c28d8545333b2785a226d11aa3b3b4ed aria-expanded=false>
Class 8</button><div class=collapse id=section-c28d8545333b2785a226d11aa3b3b4ed><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/draw-bounds/>Draw Bounds</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-8/frame-buffers/>Frame Buffers</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0bbdd9311c2cb50588b0a462a5438610 aria-expanded=false>
Class 9</button><div class=collapse id=section-0bbdd9311c2cb50588b0a462a5438610><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/classes/>Classes</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-9/mobile-development/>Mobile Development</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c75af093fd3f2e096e4fe21c7e51e7c7 aria-expanded=false>
Class 10</button><div class=collapse id=section-c75af093fd3f2e096e4fe21c7e51e7c7><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/class-10/machine-learning/>Machine Learning</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-ddbaeabfbe390c84589d5d1c423b9284 aria-expanded=true>
Class 11</button><div class="collapse show" id=section-ddbaeabfbe390c84589d5d1c423b9284><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=https://seeingmachines.betamovement.net/docs/class-11/mapping/>Mapping</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-56c4b82e7a8861de86a5ebe5eaa62225 aria-expanded=false>
Assignments</button><div class=collapse id=section-56c4b82e7a8861de86a5ebe5eaa62225><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-1/>Assignment 1</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-2/>Assignment 2</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-3/>Assignment 3</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/assignment-4/>Assignment 4</a></li><li><a class="docs-link rounded" href=https://seeingmachines.betamovement.net/docs/assignments/final-project/>Final Project</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#homography>Homography</a></li><li><a href=#world-to-projector-mapping>World to Projector Mapping</a><ul><li><a href=#model-view-projection>Model View Projection</a></li><li><a href=#pinhole-camera-model>Pinhole Camera Model</a></li><li><a href=#ofxkinectprojectortoolkit>ofxKinectProjectorToolkit</a></li><li><a href=#projected-blob-clipping>Projected Blob Clipping</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#homography>Homography</a></li><li><a href=#world-to-projector-mapping>World to Projector Mapping</a><ul><li><a href=#model-view-projection>Model View Projection</a></li><li><a href=#pinhole-camera-model>Pinhole Camera Model</a></li><li><a href=#ofxkinectprojectortoolkit>ofxKinectProjectorToolkit</a></li><li><a href=#projected-blob-clipping>Projected Blob Clipping</a></li></ul></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Mapping</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#homography>Homography</a></li><li><a href=#world-to-projector-mapping>World to Projector Mapping</a><ul><li><a href=#model-view-projection>Model View Projection</a></li><li><a href=#pinhole-camera-model>Pinhole Camera Model</a></li><li><a href=#ofxkinectprojectortoolkit>ofxKinectProjectorToolkit</a></li><li><a href=#projected-blob-clipping>Projected Blob Clipping</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#homography>Homography</a></li><li><a href=#world-to-projector-mapping>World to Projector Mapping</a><ul><li><a href=#model-view-projection>Model View Projection</a></li><li><a href=#pinhole-camera-model>Pinhole Camera Model</a></li><li><a href=#ofxkinectprojectortoolkit>ofxKinectProjectorToolkit</a></li><li><a href=#projected-blob-clipping>Projected Blob Clipping</a></li></ul></li></ul></nav></div></nav><p>OpenCV includes many functions for calibrating and mapping points and images between different spaces.</p><h2 id=homography>Homography <a href=#homography class=anchor aria-hidden=true>#</a></h2><p>A homography is a transformation that reorients one image to match or fit inside another:</p><ul><li>The two images are usually two points of view of the same subject.</li><li>Common features are found in both images and paired to make a data set.<figure style='display:block;margin:1em auto;width:600px'><img style='display:block;margin:0 auto' src=https://docs.opencv.org/3.0-beta/_images/homography_findobj.jpg alt="Feature Matching + Homography to find Objects"><figcaption><a href=https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html><em>Feature Matching + Homography to find Objects</em></a></figcaption></figure></li><li>The data set is then used with the <a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography"><code>cv::findHomography</code></a> function, which returns a matrix that allows us to warp one image into the other.</li><li>We can then use this homography in <a href=https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective>cv::warpPerspective`</a>, which takes in an image and <em>warps</em> it to match the other&rsquo;s perspective.</li></ul><figure style='display:block;margin:1em auto;width:600px'><img style='display:block;margin:0 auto' src=https://www.learnopencv.com/wp-content/uploads/2016/01/homography-example.jpg alt="Homography Examples using OpenCV"><figcaption><a href=https://www.learnopencv.com/homography-examples-using-opencv-python-c/><em>Homography Examples using OpenCV</em></a></figcaption></figure><p>This operation can have many uses, like perspective correction, embedding images into one another, or combining many to create large panoramas. The feature pairs can be manually or automatically selected, using OpenCV feature detectors.</p><figure style="width:600px;height:420px;display:block;margin:0 auto"><iframe width=600 height=375 src=https://www.youtube.com/embed/GH1p1HtNegY frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><figcaption><i>video panorama stitching with stabilizing homography estimation</i></figcaption></figure><p>Let&rsquo;s use homography to map our bouncing ball sketch to a projection surface.</p><p>We will start with a couple of changes:</p><ul><li>The canvas will be the size of the projection screen. We will use two macros <code>PROJECTOR_RESOLUTION_X</code> and <code>PROJECTOR_RESOLUTION_Y</code> to refer to these values across the app.</li><li>We will add a second window for the projector in <code>main.cpp</code>.</li></ul><pre><code class=language-cpp>// main.cpp
#include &quot;ofMain.h&quot;
#include &quot;ofApp.h&quot;

int main()
{
  ofGLFWWindowSettings settings;

  settings.setSize(1280, 720);
  settings.setPosition(ofVec2f(100, 100));
  settings.resizable = true;
  shared_ptr&lt;ofAppBaseWindow&gt; mainWindow = ofCreateWindow(settings);

  settings.setSize(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y);
  settings.setPosition(ofVec2f(ofGetScreenWidth(), 0));
  settings.resizable = false;
  settings.decorated = false;
  settings.shareContextWith = mainWindow;
  shared_ptr&lt;ofAppBaseWindow&gt; secondWindow = ofCreateWindow(settings);
  secondWindow-&gt;setVerticalSync(false);

  shared_ptr&lt;ofApp&gt; mainApp(new ofApp);
  ofAddListener(secondWindow-&gt;events().draw, mainApp.get(), &amp;ofApp::drawProjector);

  ofRunApp(mainWindow, mainApp);
  ofRunMainLoop();
}
</code></pre><ul><li>We can update the ball bouncing code to use these macros for the wall coordinates.</li></ul><pre><code class=language-cpp>// ezBall.h
#pragma once

#include &quot;ofMain.h&quot;

class ezBall
{
public:
  void setup(int x, int y);

  void update(glm::vec2 force);
  void draw();

private:
  glm::vec2 pos;
  glm::vec2 vel;
  glm::vec2 acc;

  float mass;

  ofColor color;
};
</code></pre><pre><code class=language-cpp>// ezBall.cpp
#include &quot;ezBall.h&quot;

#include &quot;ofApp.h&quot;

void ezBall::setup(int x, int y)
{
  pos = glm::vec2(x, y);
  mass = ofRandom(10, 30);
  color = ofColor(ofRandom(127, 255), ofRandom(127, 255), ofRandom(127, 255));
}

void ezBall::update(glm::vec2 force)
{
  // Add force.
  acc += force / mass;

  if (glm::length(vel) &gt; 0)
  {
    // Add friction.
    glm::vec2 friction = glm::normalize(vel * -1) * 0.01f;
    acc += friction;
  }

  // Apply acceleration, then reset it!
  vel += acc;
  acc = glm::vec2(0.0f);

  // Move object.
  pos += vel;

  // Bounce off walls, taking radius into consideration.
  if (pos.x - mass &lt; 0 || pos.x + mass &gt; PROJECTOR_RESOLUTION_X)
  {
    pos.x = ofClamp(pos.x, mass, PROJECTOR_RESOLUTION_X - mass);
    vel.x *= -1;
  }
  if (pos.y - mass &lt; 0 || pos.y + mass &gt; PROJECTOR_RESOLUTION_Y)
  {
    pos.y = ofClamp(pos.y, mass, PROJECTOR_RESOLUTION_Y - mass);
    vel.y *= -1;
  }
}

void ezBall::draw()
{
  ofSetColor(color);
  ofDrawCircle(pos, mass);
}
</code></pre><ul><li>We will render our bouncing balls canvas into an <code>ofFbo</code>, and draw that FBO out to both our main and projector windows.</li><li>We will interact using the mouse on the main screen, so we will have to remap our values in <code>mouseDragged()</code>. We only want to add balls when the mouse is dragging on top of the canvas, so we will check if the cursor is in the right spot using <a href=https://openframeworks.cc/documentation/math/ofMath/#!show_ofInRange><code>ofInRange()</code></a>.</li></ul><pre><code class=language-cpp>// ofApp.h
#pragma once

#include &quot;ofMain.h&quot;

#include &quot;ezBall.h&quot;

// This must match the display resolution of our projector
#define PROJECTOR_RESOLUTION_X 1920
#define PROJECTOR_RESOLUTION_Y 1080

class ofApp : public ofBaseApp
{
public:
  void setup();
  
  void update();

  void draw();
  void drawProjector(ofEventArgs&amp; args);

  void keyPressed(int key);
  void mouseDragged(int x, int y, int button);
  void mousePressed(int x, int y, int button);

  void addBall(int x, int y);

  std::vector&lt;ezBall&gt; balls;

  ofFbo renderFbo;
};
</code></pre><pre><code class=language-cpp>// ofApp.cpp
#include &quot;ofApp.h&quot;

void ofApp::setup()
{
  ofBackground(0);

  renderFbo.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y);
}

void ofApp::update()
{
  glm::vec2 gravity = glm::vec2(0, 9.8f);
  renderFbo.begin();
  {
    ofClear(255, 255);

    for (int i = 0; i &lt; balls.size(); i++)
    {
      balls[i].update(gravity);
      balls[i].draw();
    }
  }
  renderFbo.end();
}

void ofApp::draw()
{
  ofSetColor(255);

  // Draw unwarped image on the left.
  renderFbo.draw(0, 0, 640, 360);
}

void ofApp::drawProjector(ofEventArgs&amp; args)
{
  ofBackground(0);
  ofSetColor(255);

  renderFbo.draw(0, 0);
}

void ofApp::addBall(int x, int y)
{
  // Add a new ezBall.
  balls.push_back(ezBall());
  // Setup the last added ezBall.
  balls.back().setup(x, y);
}

void ofApp::keyPressed(int key)
{
  if (key == ' ')
  {
    balls.clear();
  }
}

void ofApp::mouseDragged(int x, int y, int button)
{
  // Only add a ball if we're dragging in the preview window.
  if (ofInRange(x, 0, 640) &amp;&amp; ofInRange(y, 0, 360))
  {
    // Remap the ball to the FBO resolution.
    int ballX = ofMap(x, 0, 640, 0, renderFbo.getWidth());
    int ballY = ofMap(y, 0, 360, 0, renderFbo.getHeight());
    addBall(ballX, ballY);
  }
}

void ofApp::mousePressed(int x, int y, int button)
{
  // Simply call the mouseDragged handler.
  mouseDragged(x, y, button);
}
</code></pre><p>Next, let uss add an interface for mapping feature points from one image to the next.</p><ul><li>The points will be normalized (between 0 and 1) to make them independent of image position or resolution. This will allow us to draw them properly in both windows with minimal headaches.</li><li>The source points will never change, we will just use the four corners of our image.</li><li>The destination points can be changed by dragging them with the mouse. We will write a simple system where the point nearest to the mouse gets selected on click.</li><li>The destination points will also be drawn in the projection window, so that we can use them to against actual physical features we are projecting on.</li></ul><pre><code class=language-cpp>// ofApp.h
#pragma once

#include &quot;ofMain.h&quot;

#include &quot;ofxGui.h&quot;

#include &quot;ezBall.h&quot;

// This must match the display resolution of your projector
#define PROJECTOR_RESOLUTION_X 1920
#define PROJECTOR_RESOLUTION_Y 1080

class ofApp : public ofBaseApp
{
public:
  void setup();
  
  void update();

  void draw();
  void drawProjector(ofEventArgs&amp; args);

  void keyPressed(int key);
  void mouseDragged(int x, int y, int button);
  void mousePressed(int x, int y, int button);
  void mouseReleased(int x, int y, int button);

  void addBall(int x, int y);

  std::vector&lt;ezBall&gt; balls;

  ofFbo renderFbo;

  std::vector&lt;glm::vec2&gt; srcPoints;
  std::vector&lt;glm::vec2&gt; dstPoints;

  int activePoint;

  ofParameter&lt;bool&gt; adjustMapping;

  ofxPanel guiPanel;
};
</code></pre><pre><code class=language-cpp>// ofApp.cpp
#include &quot;ofApp.h&quot;

void ofApp::setup()
{
  ofBackground(0);

  renderFbo.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y);

  srcPoints.push_back(glm::vec2(0, 0));
  srcPoints.push_back(glm::vec2(1, 0));
  srcPoints.push_back(glm::vec2(0, 1));
  srcPoints.push_back(glm::vec2(1, 1));

  dstPoints.push_back(glm::vec2(0, 0));
  dstPoints.push_back(glm::vec2(1, 0));
  dstPoints.push_back(glm::vec2(0, 1));
  dstPoints.push_back(glm::vec2(1, 1));

  activePoint = -1;

  adjustMapping.set(&quot;Adjust Mapping&quot;, false);

  guiPanel.setup(&quot;Homography&quot;, &quot;settings.json&quot;);
  guiPanel.add(adjustMapping);
}

void ofApp::update()
{
  glm::vec2 gravity = glm::vec2(0, 9.8f);
  renderFbo.begin();
  {
    ofClear(255, 255);

    for (int i = 0; i &lt; balls.size(); i++)
    {
      balls[i].update(gravity);
      balls[i].draw();
    }
  }
  renderFbo.end();
}

void ofApp::draw()
{
  ofSetColor(255);

  // Draw unwarped image on the left.
  renderFbo.draw(0, 0, 640, 360);

  if (adjustMapping)
  {
    // Draw mapping points.
    for (int i = 0; i &lt; srcPoints.size(); i++)
    {
      ofSetColor(0, 0, 255);
      glm::vec2 srcPt = glm::vec2(ofMap(srcPoints[i].x, 0, 1, 0, 640), ofMap(srcPoints[i].y, 0, 1, 0, 360));
      ofDrawCircle(srcPt, 10);

      ofSetColor(255, 0, 0);
      glm::vec2 dstPt = glm::vec2(ofMap(dstPoints[i].x, 0, 1, 640, 1280), ofMap(dstPoints[i].y, 0, 1, 0, 360));
      ofDrawCircle(dstPt, 10);

      ofSetColor(255, 0, 255);
      ofDrawLine(srcPt, dstPt);
    }
  }

  guiPanel.draw();
}

void ofApp::drawProjector(ofEventArgs&amp; args)
{
  ofBackground(0);
  ofSetColor(255);

  renderFbo.draw(0, 0);
  
  if (adjustMapping)
  {
    // Draw mapping dst points.
    for (int i = 0; i &lt; dstPoints.size(); i++)
    {
      ofSetColor(255, 0, 0);
      glm::vec2 dstPt = glm::vec2(dstPoints[i].x * PROJECTOR_RESOLUTION_X, dstPoints[i].y * PROJECTOR_RESOLUTION_Y);
      ofDrawCircle(dstPt, 20);
    }
  }
}

void ofApp::addBall(int x, int y)
{
  // Add a new ezBall.
  balls.push_back(ezBall());
  // Setup the last added ezBall.
  balls.back().setup(x, y);
}

void ofApp::keyPressed(int key)
{
  if (key == ' ')
  {
    balls.clear();
  }
}

void ofApp::mouseDragged(int x, int y, int button)
{
  if (adjustMapping)
  {
    if (activePoint &gt; -1)
    {
      // Move the active Point under the mouse, but stick to edges.
      glm::vec2 normPt = glm::vec2(ofMap(x, 640, 1280, 0, 1, true), ofMap(y, 0, 360, 0, 1, true));
      dstPoints[activePoint] = normPt;
    }
  }
  else
  {
    // Only add a ball if we're dragging in the preview window.
    if (ofInRange(x, 0, 640) &amp;&amp; ofInRange(y, 0, 360))
    {
      // Remap the ball to the FBO resolution.
      int ballX = ofMap(x, 0, 640, 0, renderFbo.getWidth());
      int ballY = ofMap(y, 0, 360, 0, renderFbo.getHeight());
      addBall(ballX, ballY);
    }
  }
}

void ofApp::mousePressed(int x, int y, int button)
{
  if (adjustMapping)
  {
    // Try to snap to a dst point.
    for (int i = 0; i &lt; dstPoints.size(); i++)
    {
      glm::vec2 dstPt = glm::vec2(ofMap(dstPoints[i].x, 0, 1, 640, 1280), ofMap(dstPoints[i].y, 0, 1, 0, 360));
      glm::vec2 mousePt = glm::vec2(x, y);
      if (glm::distance(dstPt, mousePt) &lt; 20)
      {
        // Close enough, let's grab this one.
        activePoint = i;
        break;
      }
    }
  }
  else
  {
    // Simply call the mouseDragged handler.
    mouseDragged(x, y, button);
  }
}

void ofApp::mouseReleased(int x, int y, int button)
{
  if (adjustMapping)
  {
    activePoint = -1;
  }
}
</code></pre><p>Finally, we will use these points to first get a homography transformation, then use this transformation to warp our image on the fly.</p><p>Note that this might run slowly unless we are running our app in Release mode. This is because our projection FBO is quite large (1920x1080) and we need to read back from the GPU every frame to warp the image on the CPU.</p><pre><code class=language-cpp>// ofApp.h
#pragma once

#include &quot;ofMain.h&quot;

#include &quot;ofxCv.h&quot;
#include &quot;ofxGui.h&quot;

#include &quot;ezBall.h&quot;

// This must match the display resolution of your projector
#define PROJECTOR_RESOLUTION_X 1920
#define PROJECTOR_RESOLUTION_Y 1080

class ofApp : public ofBaseApp
{
public:
  void setup();
  
  void update();

  void draw();
  void drawProjector(ofEventArgs&amp; args);

  void keyPressed(int key);
  void mouseDragged(int x, int y, int button);
  void mousePressed(int x, int y, int button);
  void mouseReleased(int x, int y, int button);

  void addBall(int x, int y);

  std::vector&lt;ezBall&gt; balls;

  ofFbo renderFbo;
  ofPixels renderPixels;
  ofImage warpedImg;

  std::vector&lt;glm::vec2&gt; srcPoints;
  std::vector&lt;glm::vec2&gt; dstPoints;

  int activePoint;

  cv::Mat homographyMat;
  bool homographyReady;

  ofParameter&lt;bool&gt; adjustMapping;
  ofParameter&lt;bool&gt; projectWarped;

  ofxPanel guiPanel;
};
</code></pre><pre><code class=language-cpp>// ofApp.cpp
#include &quot;ofApp.h&quot;

void ofApp::setup()
{
  ofBackground(0);

  renderFbo.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y);
  warpedImg.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y, OF_IMAGE_COLOR);

  srcPoints.push_back(glm::vec2(0, 0));
  srcPoints.push_back(glm::vec2(1, 0));
  srcPoints.push_back(glm::vec2(0, 1));
  srcPoints.push_back(glm::vec2(1, 1));

  dstPoints.push_back(glm::vec2(0, 0));
  dstPoints.push_back(glm::vec2(1, 0));
  dstPoints.push_back(glm::vec2(0, 1));
  dstPoints.push_back(glm::vec2(1, 1));

  activePoint = -1;
  homographyReady = false;

  adjustMapping.set(&quot;Adjust Mapping&quot;, false);
  projectWarped.set(&quot;Project Warped&quot;, true);

  guiPanel.setup(&quot;Homography&quot;, &quot;settings.json&quot;);
  guiPanel.add(adjustMapping);
  guiPanel.add(projectWarped);
}

void ofApp::update()
{
  if (adjustMapping)
  {
    // Copy points from glm to cv format.
    std::vector&lt;cv::Point2f&gt; cvSrcPoints;
    std::vector&lt;cv::Point2f&gt; cvDstPoints;
    for (int i = 0; i &lt; srcPoints.size(); i++) 
    {
      // Scale points to projector dimensions.
      cvSrcPoints.push_back(cv::Point2f(srcPoints[i].x * PROJECTOR_RESOLUTION_X, srcPoints[i].y * PROJECTOR_RESOLUTION_Y));
      cvDstPoints.push_back(cv::Point2f(dstPoints[i].x * PROJECTOR_RESOLUTION_X, dstPoints[i].y * PROJECTOR_RESOLUTION_Y));
    }

    // Generate a homography from the two sets of points.
    homographyMat = cv::findHomography(cv::Mat(cvSrcPoints), cv::Mat(cvDstPoints));
    homographyReady = true;
  }

  glm::vec2 gravity = glm::vec2(0, 9.8f);
  renderFbo.begin();
  {
    ofClear(255, 255);

    for (int i = 0; i &lt; balls.size(); i++)
    {
      balls[i].update(gravity);
      balls[i].draw();
    }
  }
  renderFbo.end();

  if (homographyReady) 
  {
    // Read the FBO to pixels.
    renderFbo.readToPixels(renderPixels);

    // Warp the pixels into a new image.
    warpedImg.setFromPixels(renderPixels);
    ofxCv::warpPerspective(renderPixels, warpedImg, homographyMat, CV_INTER_LINEAR);
    warpedImg.update();
  }
}

void ofApp::draw()
{
  ofSetColor(255);

  // Draw unwarped image on the left.
  renderFbo.draw(0, 0, 640, 360);

  if (homographyReady)
  {
    // Draw warped image on the right.
    warpedImg.draw(640, 0, 640, 360);
  }

  if (adjustMapping)
  {
    // Draw mapping points.
    for (int i = 0; i &lt; srcPoints.size(); i++)
    {
      ofSetColor(0, 0, 255);
      glm::vec2 srcPt = glm::vec2(ofMap(srcPoints[i].x, 0, 1, 0, 640), ofMap(srcPoints[i].y, 0, 1, 0, 360));
      ofDrawCircle(srcPt, 10);

      ofSetColor(255, 0, 0);
      glm::vec2 dstPt = glm::vec2(ofMap(dstPoints[i].x, 0, 1, 640, 1280), ofMap(dstPoints[i].y, 0, 1, 0, 360));
      ofDrawCircle(dstPt, 10);

      ofSetColor(255, 0, 255);
      ofDrawLine(srcPt, dstPt);
    }
  }

  guiPanel.draw();
}

void ofApp::drawProjector(ofEventArgs&amp; args)
{
  ofBackground(0);
  ofSetColor(255);

  if (homographyReady &amp;&amp; projectWarped)
  {
    warpedImg.draw(0, 0);
  }
  else
  {
    renderFbo.draw(0, 0);
  }

  if (adjustMapping)
  {
    // Draw mapping dst points.
    for (int i = 0; i &lt; dstPoints.size(); i++)
    {
      ofSetColor(255, 0, 0);
      glm::vec2 dstPt = glm::vec2(dstPoints[i].x * PROJECTOR_RESOLUTION_X, dstPoints[i].y * PROJECTOR_RESOLUTION_Y);
      ofDrawCircle(dstPt, 20);
    }
  }
}

void ofApp::addBall(int x, int y)
{
  // Add a new ezBall.
  balls.push_back(ezBall());
  // Setup the last added ezBall.
  balls.back().setup(x, y);
}

void ofApp::keyPressed(int key)
{
  if (key == ' ')
  {
    balls.clear();
  }
}

void ofApp::mouseDragged(int x, int y, int button)
{
  if (adjustMapping)
  {
    if (activePoint &gt; -1)
    {
      // Move the active Point under the mouse, but stick to edges.
      glm::vec2 normPt = glm::vec2(ofMap(x, 640, 1280, 0, 1, true), ofMap(y, 0, 360, 0, 1, true));
      dstPoints[activePoint] = normPt;
    }
  }
  else
  {
    // Only add a ball if we're dragging in the preview window.
    if (ofInRange(x, 0, 640) &amp;&amp; ofInRange(y, 0, 360))
    {
      // Remap the ball to the FBO resolution.
      int ballX = ofMap(x, 0, 640, 0, renderFbo.getWidth());
      int ballY = ofMap(y, 0, 360, 0, renderFbo.getHeight());
      addBall(ballX, ballY);
    }
  }
}

void ofApp::mousePressed(int x, int y, int button)
{
  if (adjustMapping)
  {
    // Try to snap to a dst point.
    for (int i = 0; i &lt; dstPoints.size(); i++)
    {
      glm::vec2 dstPt = glm::vec2(ofMap(dstPoints[i].x, 0, 1, 640, 1280), ofMap(dstPoints[i].y, 0, 1, 0, 360));
      glm::vec2 mousePt = glm::vec2(x, y);
      if (glm::distance(dstPt, mousePt) &lt; 20)
      {
        // Close enough, let's grab this one.
        activePoint = i;
        break;
      }
    }
  }
  else
  {
    mouseDragged(x, y, button);
  }
}

void ofApp::mouseReleased(int x, int y, int button)
{
  if (adjustMapping)
  {
    activePoint = -1;
  }
}
</code></pre><div class="alert alert-info d-flex" role=alert><div class="flex-shrink-1 alert-icon">âœŒï¸</div><div class=w-100><p><strong>Homography vs Quad Warping</strong></p><p>You may be wondering why we are not just binding our FBO texture to a quad mesh, positioning the corner points and rendering that out instead of going through all the homography steps outlined above.</p><p>The main difference is that the homography operation is going to look more accurate because it is performing a perspective shift; it is looking at the image from a different <em>virtual</em> point of view, which is essentially what we are doing in the <em>physical</em> world.</p><p>Warping a quad will just perform a simple interpolation on the image pixels to make them fit into the new shape. It might look fine for subtle transformations, but will break down quickly as the transformation becomes more extreme.</p><figure style="width:600px;height:200px;display:block;margin:0 auto"><video src=homography-warp.mp4 controls width=100%></video><figcaption><i>Homography (left) / Quad Warp (right)</i></figcaption></figure></div></div><h2 id=world-to-projector-mapping>World to Projector Mapping <a href=#world-to-projector-mapping class=anchor aria-hidden=true>#</a></h2><p>Homography is useful for 2D mapping (i.e. images to images), but we can also attempt to map the 3D space we are projecting onto back into the projector image.</p><figure style="width:600px;height:420px;display:block;margin:0 auto"><iframe width=600 height=375 src=https://www.youtube.com/embed/CE1B7tdGCw0 frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><figcaption><i>UCLA's Augmented Reality Sandbox</i></figcaption></figure><p>The idea is similar:</p><ul><li>We will collect pairs of corresponding points from both spaces. However, since one space is our world, we will need its points in 3D. We will use a depth sensor for this.</li><li>We can then pass this data set to a solver function, which will give us a transformation we can apply to other points in the same space, and have them <em>project</em> to an appropriate position on screen.</li></ul><h3 id=model-view-projection>Model View Projection <a href=#model-view-projection class=anchor aria-hidden=true>#</a></h3><p>We have already been doing something similar when rendering 3D objects on screen, like point clouds. You may have heard of the Model View Projection Matrix (or MVP) when working with computer graphics.</p><ul><li>The Model View Projection is actually a stack of three matrices that are used to transform a 3D point from its local space to screen space.</li><li>The <em>model matrix</em> maps a point from its local space to the world space.</li><li>The <em>view matrix</em> maps a point from world space to camera space (from the point of view of the camera).</li><li>The <em>projection matrix</em> maps a point from camera space to clip space, which is essentially what the camera projects onto a surface. Depending on the camera parameters, this projection can have perspective or be orthographic.</li></ul><figure style='display:block;margin:1em auto;width:600px'><img style='display:block;margin:0 auto' src=https://www.maa.org/sites/default/files/images/cms_upload/figure256786.jpg alt="Geometric Photo Manipulation - Projections"><figcaption><a href=https://www.maa.org/press/periodicals/loci/joma/geometric-photo-manipulation-projections><em>Geometric Photo Manipulation - Projections</em></a></figcaption></figure><p>What we are essentially doing is coming up with a similar matrix, but with an external camera and projector.</p><p><a href=https://jsantell.com/model-view-projection>Model View Projection</a> is a good reference for in-depth info.</p><h3 id=pinhole-camera-model>Pinhole Camera Model <a href=#pinhole-camera-model class=anchor aria-hidden=true>#</a></h3><p>We use a pinhole camera model to calculate this projection. Using a perspective model, the line of sight from the camera to a 3D point will intersect a plane, which we can consider our canvas. The position at which it intersects the plane is its projection in 2D space.</p><figure style='display:block;margin:1em auto;width:600px'><img style='display:block;margin:0 auto' src=https://docs.opencv.org/2.4/_images/pinhole_camera_model.png alt="Camera Calibration and 3D Reconstruction"><figcaption><a href=https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html><em>Camera Calibration and 3D Reconstruction</em></a></figcaption></figure><h3 id=ofxkinectprojectortoolkit>ofxKinectProjectorToolkit <a href=#ofxkinectprojectortoolkit class=anchor aria-hidden=true>#</a></h3><p><a href=https://github.com/genekogan/ofxKinectProjectorToolkit>ofxKinectProjectorToolkit</a> is one of the many addons available for OF to create a correspondence between the 3D world and a 2D projector.</p><ul><li>This addon was originally written by Gene Kogan but it&rsquo;s a little out of date. I&rsquo;ve made a fork <a href=https://github.com/prisonerjohn/ofxKinectProjectorToolkit/tree/feature/refactor-0.10>here</a> with updates which should get you started faster.</li><li>As the name implies, this addon is meant to work with the Kinect sensor. However, the calibration functions are sensor-agnostic. You just need to provide point pairs and it does the rest.</li><li>A chessboard pattern is used for feature detection. The chessboard is commonly used in OpenCV because it is high contrast and easy to track. The <a href=https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findchessboardcorners><code>cv::findChessboardCorners()</code></a> function is used to track the intersection points in the pattern.</li><li>The chessboard is drawn out of the projector. Because we are drawing the points, we know their 2D position on screen, in projector space.</li><li>The chessboard is detected using the depth sensor&rsquo;s color camera, which is looking at the projection surface. The tracked points are then fed to the sensor&rsquo;s world coordinate mapper to extract corresponding 3D world points.</li><li>The two sets of points are then fed to a solver, which determines the transformation matrix from one space to the other.</li><li><code>ofxKinectProjectorToolkit</code> uses <a href=http://dlib.net/>dlib</a> for calibration, which is another commonly used image processing library.</li><li>If we were to use OpenCV, we would probably use the <a href=https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#calibratecamera><code>cv::calibrateCamera()</code></a> function.</li></ul><h3 id=projected-blob-clipping>Projected Blob Clipping <a href=#projected-blob-clipping class=anchor aria-hidden=true>#</a></h3><p>The following is UNTESTED code that uses this addon to attempt to segment out a blob from a projected image. We&rsquo;ll try it in class and see what happens :)</p><p>Here is the version for Kinect.</p><pre><code class=language-cpp>// ofApp.h
#pragma once

#include &quot;ofMain.h&quot;

#include &quot;ofxCv.h&quot;
#include &quot;ofxGui.h&quot;
#include &quot;ofxKinect.h&quot;
#include &quot;ofxKinectProjectorToolkit.h&quot;

#define CHESSBOARD_COLS 5
#define CHESSBOARD_ROWS 4

// This must match the display resolution of your projector
#define PROJECTOR_RESOLUTION_X 1920
#define PROJECTOR_RESOLUTION_Y 1080

class ofApp : public ofBaseApp
{
public:
  void setup();
  void update();
  void draw();

  void keyPressed(int key);
  //void keyReleased(int key);
  //void mouseMoved(int x, int y);
  void mouseDragged(int x, int y, int button);
  void mousePressed(int x, int y, int button);
  //void mouseReleased(int x, int y, int button);
  //void mouseEntered(int x, int y);
  //void mouseExited(int x, int y);
  //void windowResized(int w, int h);
  //void dragEvent(ofDragInfo dragInfo);
  //void gotMessage(ofMessage msg);

  void drawProjector(ofEventArgs&amp; args);

  void renderChessboard();
  void renderTestPoint(glm::vec2 projectedPoint);
  void renderContours();

  void addPointPairs();

  void calibrateSpaces();
  void saveCalibration();
  void loadCalibration();

  ofxKinect kinect;
  ofxKinectProjectorToolkit kpToolkit;

  int deviceWidth;
  int deviceHeight;

  ofFbo fboProjection;

  ofImage colorImg;
  cv::Mat colorMat;

  vector&lt;glm::vec2&gt; currProjectorPoints;
  vector&lt;cv::Point2f&gt; cvPoints;
  vector&lt;glm::vec3&gt; pairsWorld;
  vector&lt;glm::vec2&gt; pairsProjector;

  glm::vec2 testPoint;

  ofImage thresholdImg;
  ofxCv::ContourFinder contourFinder;

  ofParameter&lt;int&gt; appMode;

  ofParameter&lt;float&gt; chessboardX;
  ofParameter&lt;float&gt; chessboardY;
  ofParameter&lt;int&gt; chessboardSize;
  ofParameter&lt;void&gt; addPairs;
  ofParameter&lt;void&gt; calibrate;
  ofParameter&lt;void&gt; saveCalib;
  ofParameter&lt;void&gt; loadCalib;

  ofParameter&lt;float&gt; nearThreshold;
  ofParameter&lt;float&gt; farThreshold;
  ofParameter&lt;float&gt; minArea;
  ofParameter&lt;float&gt; maxArea;
  ofParameter&lt;bool&gt; flipX;
  ofParameter&lt;bool&gt; flipY;

  ofxPanel guiPanel;
};
</code></pre><pre><code class=language-cpp>// ofApp.cpp
#include &quot;ofApp.h&quot;

void ofApp::setup()
{
  // Texture coordinates from RealSense are normalized (between 0-1).
  // This call normalizes all OF texture coordinates so that they match.
  ofDisableArbTex();

  appMode.set(&quot;App Mode&quot;, 0, 0, 2);

  chessboardX.set(&quot;Chessboard X&quot;, 0.5, 0.0, 1.0);
  chessboardY.set(&quot;Chessboard Y&quot;, 0.5, 0.0, 1.0);
  chessboardSize.set(&quot;Chessboard Size&quot;, 300, 20, 1280);

  addPairs.set(&quot;Add Pairs&quot;);
  addPairs.addListener(this, &amp;ofApp::addPointPairs);
  calibrate.set(&quot;Calibrate&quot;);
  calibrate.addListener(this, &amp;ofApp::calibrateSpaces);
  saveCalib.set(&quot;Save Calib&quot;);
  saveCalib.addListener(this, &amp;ofApp::saveCalibration);
  loadCalib.set(&quot;Load Calib&quot;);
  loadCalib.addListener(this, &amp;ofApp::loadCalibration);

  nearThreshold.set(&quot;Near Threshold&quot;, 0.01f, 0.0f, 0.1f);
  farThreshold.set(&quot;Far Threshold&quot;, 0.02f, 0.0f, 0.1f);
  minArea.set(&quot;Min Area&quot;, 0.01f, 0, 0.5f);
  maxArea.set(&quot;Max Area&quot;, 0.05f, 0, 0.5f);

  flipX.set(&quot;Flip X&quot;, false);
  flipY.set(&quot;Flip Y&quot;, false);

  fboProjection.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y, GL_RGBA);

  guiPanel.setup(&quot;RS Projection&quot;, &quot;settings.json&quot;);
  guiPanel.add(appMode);
  guiPanel.add(chessboardX);
  guiPanel.add(chessboardY);
  guiPanel.add(chessboardSize);
  guiPanel.add(addPairs);
  guiPanel.add(calibrate);
  guiPanel.add(saveCalib);
  guiPanel.add(loadCalib);
  guiPanel.add(nearThreshold);
  guiPanel.add(farThreshold);
  guiPanel.add(minArea);
  guiPanel.add(maxArea);
  guiPanel.add(flipX);
  guiPanel.add(flipY);

  // Start the Kinect context.
  kinect.setRegistration(true);
  kinect.init();
  kinect.open();

  deviceWidth = kinect.getWidth();
  deviceHeight = kinect.getHeight();
  colorImg.allocate(deviceWidth, deviceHeight, OF_IMAGE_COLOR);
}


void ofApp::update()
{
  kinect.update();

  if (kinect.isFrameNew())
  {
    colorImg.setFromPixels(kinect.getPixels());
    if (appMode == 0) // Searching.
    {
      renderChessboard();

      colorMat = ofxCv::toCv(colorImg.getPixels());
      cv::Size patternSize = cv::Size(CHESSBOARD_COLS - 1, CHESSBOARD_ROWS - 1);
      int chessFlags = cv::CALIB_CB_ADAPTIVE_THRESH + cv::CALIB_CB_FAST_CHECK;
      bool foundChessboard = cv::findChessboardCorners(colorMat, patternSize, cvPoints, chessFlags);
      if (foundChessboard)
      {
        cv::Mat grayMat;
        cv::cvtColor(colorMat, grayMat, CV_RGB2GRAY);
        cv::cornerSubPix(grayMat, cvPoints, cv::Size(11, 11), cv::Size(-1, -1), cv::TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
        cv::drawChessboardCorners(colorMat, patternSize, cv::Mat(cvPoints), foundChessboard);
        colorImg.update();
      }
    }
    else if (appMode == 1) // Testing.
    {
      // Map points in both world space and projector space and draw them.
      // If they are calibrated correctly they should be drawn on top of one another.
      glm::vec2 clampedTestPoint = glm::vec2(
          ofClamp(testPoint.x, 0, deviceWidth - 1),
          ofClamp(testPoint.y, 0, deviceHeight - 1));
      glm::vec3 worldPoint = kinect.getWorldCoordinateAt(clampedTestPoint.x, clampedTestPoint.y);
      glm::vec2 projectedPoint = kpToolkit.getProjectedPoint(worldPoint);

      renderTestPoint(projectedPoint);
    }
    else // Rendering.
    {
      // Threshold the depth.
      ofFloatPixels rawDepthPix = kinect.getRawDepthPixels();
      ofFloatPixels thresholdNear, thresholdFar, thresholdResult;
      ofxCv::threshold(rawDepthPix, thresholdNear, nearThreshold);
      ofxCv::threshold(rawDepthPix, thresholdFar, farThreshold, true);
      ofxCv::bitwise_and(thresholdNear, thresholdFar, thresholdResult);
      thresholdImg.setFromPixels(thresholdResult);

      // Find contours.
      contourFinder.setMinAreaNorm(minArea);
      contourFinder.setMaxAreaNorm(maxArea);
      contourFinder.findContours(thresholdImg);

      renderContours();
    }
  }
}

void ofApp::draw()
{
  ofSetColor(255);
  colorImg.draw(0, 0);

  kinect.getDepthTexture().draw(colorImg.getWidth(), 0);

  if (thresholdImg.isAllocated())
  {
    ofPushMatrix();
    ofTranslate(colorImg.getWidth(), kinect.getDepthTexture().getHeight());
    {
      thresholdImg.draw(0, 0);
      contourFinder.draw();
    }
    ofPopMatrix();
  }

  if (appMode == 0) // Searching.
  {
    // Use a string stream to print a multi-line message.
    std::ostringstream oss;
    oss &lt;&lt; &quot;SEARCHING MODE&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Position the chessboard by dragging the mouse over the RGB image.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Adjust the size of the chessboard using LEFT / RIGHT or the GUI sliders.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;When the pattern is recognized, hit SPACE to save a set of point-pairs.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Once a few point-pairs have been detected, calibrate using the GUI button.&quot; &lt;&lt; std::endl
        &lt;&lt; std::endl
        &lt;&lt; ofToString(pairsWorld.size()) &lt;&lt; &quot; point-pairs collected.&quot;;

    ofSetColor(255);
    ofDrawBitmapStringHighlight(oss.str(), 10, 380);
  }
  else if (appMode == 1) // Testing.
  {
    // Use a string stream to print a multi-line message.
    std::ostringstream oss;
    oss &lt;&lt; &quot;TESTING MODE&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Click on the RGB image to test a calibrated point.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;The world point will be drawn in RED, the projected point will be drawn in GREEN.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;If the calibration is successful, both points will be drawn on top of each other.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Save the calibration using the GUI button.&quot;;

    ofSetColor(255);
    ofDrawBitmapStringHighlight(oss.str(), 10, 380);

    // Draw the test point on screen.
    ofSetColor(255, 0, 0);
    float pointSize = ofMap(cos(ofGetFrameNum() * 0.1), -1, 1, 3, 40);
    ofDrawCircle(testPoint.x, testPoint.y, pointSize);
  }
  else // Rendering.
  {
    // Use a string stream to print a multi-line message.
    std::ostringstream oss;
    oss &lt;&lt; &quot;RENDERING MODE&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;Adjust the depth threshold using the GUI sliders.&quot; &lt;&lt; std::endl
        &lt;&lt; &quot;The thresholded silhouette will mask the rest of the projected image.&quot;;
  }

  guiPanel.draw();
}

void ofApp::drawProjector(ofEventArgs&amp; args)
{
  ofBackground(255);
  ofSetColor(255);
  fboProjection.draw(0, 0);
}

void ofApp::keyPressed(int key)
{
  if (key == ' ')
  {
    addPointPairs();
  }
  else if (key == 'c')
  {
    calibrateSpaces();
  }
  else if (key == 's')
  {
    saveCalibration();
  }
  else if (key == 'l')
  {
    loadCalibration();
  }
}

void ofApp::mousePressed(int x, int y, int button)
{
  if (appMode == 0) // Searching
  {
    if (ofGetMousePressed() &amp;&amp;
        ofInRange(x, 0, deviceWidth) &amp;&amp;
        ofInRange(y, 0, deviceHeight))
    {
      // Save the normalized point position.
      chessboardX = ofMap(x, 0, deviceWidth, 0, 1);
      chessboardY = ofMap(y, 0, deviceHeight, 0, 1);
    }
  }
  else if (appMode == 1) // Testing
  {
    testPoint = glm::vec2(x, y);
  }
  else
  {
    // ?
  }
}

void ofApp::mouseDragged(int x, int y, int button)
{
  if (appMode == 0) // Searching
  {
    if (ofGetMousePressed() &amp;&amp;
        ofInRange(x, 0, deviceWidth) &amp;&amp;
        ofInRange(y, 0, deviceHeight))
    {
      // Save the normalized point position.
      chessboardX = ofMap(x, 0, deviceWidth, 0, 1);
      chessboardY = ofMap(y, 0, deviceHeight, 0, 1);
    }
  }
  else if (appMode == 1) // Testing
  {
    testPoint = glm::vec2(x, y);
  }
}

void ofApp::renderChessboard()
{
  float cellSize = chessboardSize / CHESSBOARD_COLS;

  currProjectorPoints.clear();

  fboProjection.begin();
  {
    // Remap top-left to projection space.
    int boardX = ofMap(chessboardX, 0, 1, 0, fboProjection.getWidth());
    int boardY = ofMap(chessboardY, 0, 1, 0, fboProjection.getHeight());

    // Clear white and draw black cells.
    ofClear(255, 0);
    ofSetColor(0);

    for (int j = 0; j &lt; CHESSBOARD_ROWS; j++)
    {
      for (int i = 0; i &lt; CHESSBOARD_COLS; i++)
      {
        int cellX = boardX + i * cellSize;
        int cellY = boardY + j * cellSize;

        if ((i + j) % 2 == 0)
        {
          // Only draw black cells.
          ofDrawRectangle(cellX, cellY, cellSize, cellSize);
        }

        if (i &gt; 0 &amp;&amp; j &gt; 0)
        {
          // Add normalized intersection points to the list.
          float normX = ofMap(cellX, 0, fboProjection.getWidth(), 0, 1);
          float normY = ofMap(cellY, 0, fboProjection.getHeight(), 0, 1);
          currProjectorPoints.push_back(glm::vec2(normX, normY));
        }
      }
    }
  }
  fboProjection.end();
}

void ofApp::renderTestPoint(glm::vec2 projectedPoint)
{
  float pointSize = ofMap(sin(ofGetFrameNum() * 0.1), -1, 1, 3, 40);

  fboProjection.begin();
  {
    ofBackground(255);

    // Point is normalized, so it needs to be mapped to the projector size.
    float projX = ofMap(projectedPoint.x, 0, 1, 0, fboProjection.getWidth());
    float projY = ofMap(projectedPoint.y, 0, 1, 0, fboProjection.getHeight());

    ofSetColor(0, 255, 0);
    ofDrawCircle(projX, projY, pointSize);
  }
  fboProjection.end();
}

void ofApp::renderContours()
{
  fboProjection.begin();
  {
    // Clear white and draw black contours.
    ofClear(255, 0);
    ofSetColor(0);

    for (int i = 0; i &lt; contourFinder.size(); i++)
    {
      // Map contour using calibration and draw to main window
      ofBeginShape();
      std::vector&lt;cv::Point&gt; points = contourFinder.getContour(i);
      for (int j = 0; j &lt; points.size(); j++)
      {
        glm::vec3 worldPoint = kinect.getWorldCoordinateAt(points[j].x, points[j].y);
        if (worldPoint.z &gt; 0)
        {
          glm::vec2 projectedPoint = kpToolkit.getProjectedPoint(worldPoint);
          if (ofInRange(projectedPoint.x, 0, 1) &amp;&amp; ofInRange(projectedPoint.y, 0, 1))
          {
            if (flipX)
            {
              projectedPoint.x = 1.0 - projectedPoint.x;
            }
            if (flipY)
            {
              projectedPoint.y = 1.0 - projectedPoint.y;
            }
            ofVertex(projectedPoint.x * fboProjection.getWidth(), projectedPoint.y * fboProjection.getHeight());
            //ofLog() &lt;&lt; &quot;Adding world &quot; &lt;&lt; worldPoint &lt;&lt; &quot; // point &quot; &lt;&lt; projectedPoint &lt;&lt; &quot; // proj &quot; &lt;&lt; (projectedPoint.x * fboProjection.getWidth()) &lt;&lt; &quot;, &quot; &lt;&lt; (projectedPoint.y * fboProjection.getHeight());
          }
        }
      }
      ofEndShape();
    }
  }
  fboProjection.end();
}

void ofApp::addPointPairs()
{
  // Count the number of world points.
  int numDepthPoints = 0;
  for (int i = 0; i &lt; cvPoints.size(); i++)
  {
    glm::vec3 worldPoint = kinect.getWorldCoordinateAt(cvPoints[i].x, cvPoints[i].y);
    if (worldPoint.z &gt; 0)
    {
      numDepthPoints++;
    }
  }

  int chessboardTotal = (CHESSBOARD_COLS - 1) * (CHESSBOARD_ROWS - 1);
  if (numDepthPoints != chessboardTotal)
  {
    ofLogError(__FUNCTION__) &lt;&lt; &quot;Only found &quot; &lt;&lt; numDepthPoints &lt;&lt; &quot; / &quot; &lt;&lt; chessboardTotal &lt;&lt; &quot; points!&quot;;
    return;
  }

  // Found all chessboard points in the world, add both sets to the lists we will use for calibration.
  for (int i = 0; i &lt; cvPoints.size(); i++)
  {
    glm::vec3 worldPoint = kinect.getWorldCoordinateAt(cvPoints[i].x, cvPoints[i].y);
    pairsWorld.push_back(worldPoint);
    pairsProjector.push_back(currProjectorPoints[i]);
    ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Adding pair i: &quot; &lt;&lt; worldPoint &lt;&lt; &quot; =&gt; &quot; &lt;&lt; currProjectorPoints[i];
  }

  ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Added &quot; &lt;&lt; chessboardTotal &lt;&lt; &quot; point-pairs.&quot;;
}

void ofApp::calibrateSpaces()
{
  kpToolkit.calibrate(pairsWorld, pairsProjector);
  appMode = 1;

  pairsWorld.clear();
  pairsProjector.clear();
}

void ofApp::saveCalibration()
{
  if (kpToolkit.saveCalibration(&quot;calibration.json&quot;))
  {
    ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Calibration saved!&quot;;
  }
}

void ofApp::loadCalibration()
{
  if (kpToolkit.loadCalibration(&quot;calibration.json&quot;))
  {
    ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Calibration loaded!&quot;;
    appMode = 2;
  }
}
</code></pre><p>And the version for RealSense.</p><pre><code class=language-cpp>// ofApp.h
#pragma once

#include &quot;ofMain.h&quot;

#include &quot;ofxCv.h&quot;
#include &quot;ofxGui.h&quot;
#include &quot;ofxKinectProjectorToolkit.h&quot;
#include &quot;ofxRealSense2.h&quot;

#define CHESSBOARD_COLS 5
#define CHESSBOARD_ROWS 4

// This must match the display resolution of your projector
#define PROJECTOR_RESOLUTION_X 1920
#define PROJECTOR_RESOLUTION_Y 1080

class ofApp : public ofBaseApp
{
public:
  void setup();
  void update();
  void draw();

  void keyPressed(int key);
  //void keyReleased(int key);
  //void mouseMoved(int x, int y);
  void mouseDragged(int x, int y, int button);
  void mousePressed(int x, int y, int button);
  //void mouseReleased(int x, int y, int button);
  //void mouseEntered(int x, int y);
  //void mouseExited(int x, int y);
  //void windowResized(int w, int h);
  //void dragEvent(ofDragInfo dragInfo);
  //void gotMessage(ofMessage msg);

  void deviceAdded(std::string&amp; serialNumber);

  void drawProjector(ofEventArgs&amp; args);

  void renderChessboard();
  void renderTestPoint(glm::vec2 projectedPoint);
  void renderContours();

  void addPointPairs();

  void calibrateSpaces();
  void saveCalibration();
  void loadCalibration();

  ofxRealSense2::Context rsContext;
  ofxKinectProjectorToolkit kpToolkit;

  int deviceWidth;
  int deviceHeight;

  ofFbo fboProjection;

  ofImage colorImg;
  cv::Mat colorMat;

  vector&lt;glm::vec2&gt; currProjectorPoints;
  vector&lt;cv::Point2f&gt; cvPoints;
  vector&lt;glm::vec3&gt; pairsWorld;
  vector&lt;glm::vec2&gt; pairsProjector;

  glm::vec2 testPoint;

  ofImage thresholdImg;
  ofxCv::ContourFinder contourFinder;

  ofParameter&lt;int&gt; appMode;

  ofParameter&lt;float&gt; chessboardX;
  ofParameter&lt;float&gt; chessboardY;
  ofParameter&lt;int&gt; chessboardSize;
  ofParameter&lt;void&gt; addPairs;
  ofParameter&lt;void&gt; calibrate;
  ofParameter&lt;void&gt; saveCalib;
  ofParameter&lt;void&gt; loadCalib;

  ofParameter&lt;float&gt; nearThreshold;
  ofParameter&lt;float&gt; farThreshold;
  ofParameter&lt;float&gt; minArea;
  ofParameter&lt;float&gt; maxArea;
  ofParameter&lt;bool&gt; flipX;
  ofParameter&lt;bool&gt; flipY;

  ofxPanel guiPanel;
};
</code></pre><pre><code class=language-cpp>// ofApp.cpp
#include &quot;ofApp.h&quot;

void ofApp::setup()
{
    // Texture coordinates from RealSense are normalized (between 0-1).
    // This call normalizes all OF texture coordinates so that they match.
    ofDisableArbTex();

    appMode.set(&quot;App Mode&quot;, 0, 0, 2);

    chessboardX.set(&quot;Chessboard X&quot;, 0.5, 0.0, 1.0);
    chessboardY.set(&quot;Chessboard Y&quot;, 0.5, 0.0, 1.0);
    chessboardSize.set(&quot;Chessboard Size&quot;, 300, 20, 1280);

    addPairs.set(&quot;Add Pairs&quot;);
    addPairs.addListener(this, &amp;ofApp::addPointPairs);
    calibrate.set(&quot;Calibrate&quot;);
    calibrate.addListener(this, &amp;ofApp::calibrateSpaces);
    saveCalib.set(&quot;Save Calib&quot;);
    saveCalib.addListener(this, &amp;ofApp::saveCalibration);
    loadCalib.set(&quot;Load Calib&quot;);
    loadCalib.addListener(this, &amp;ofApp::loadCalibration);

    nearThreshold.set(&quot;Near Threshold&quot;, 0.01f, 0.0f, 0.1f);
    farThreshold.set(&quot;Far Threshold&quot;, 0.02f, 0.0f, 0.1f);
    minArea.set(&quot;Min Area&quot;, 0.01f, 0, 0.5f);
    maxArea.set(&quot;Max Area&quot;, 0.05f, 0, 0.5f);

    flipX.set(&quot;Flip X&quot;, false);
    flipY.set(&quot;Flip Y&quot;, false);

    fboProjection.allocate(PROJECTOR_RESOLUTION_X, PROJECTOR_RESOLUTION_Y, GL_RGBA);

    guiPanel.setup(&quot;RS Projection&quot;, &quot;settings.json&quot;);
    guiPanel.add(appMode);
    guiPanel.add(chessboardX);
    guiPanel.add(chessboardY);
    guiPanel.add(chessboardSize);
    guiPanel.add(addPairs);
    guiPanel.add(calibrate);
    guiPanel.add(saveCalib);
    guiPanel.add(loadCalib);
    guiPanel.add(nearThreshold);
    guiPanel.add(farThreshold);
    guiPanel.add(minArea);
    guiPanel.add(maxArea);
    guiPanel.add(flipX);
    guiPanel.add(flipY);

    // Start the RealSense context.
    // Devices are added in the deviceAdded() callback function.
    ofAddListener(rsContext.deviceAddedEvent, this, &amp;ofApp::deviceAdded);
    rsContext.setup(false);
}

void ofApp::deviceAdded(std::string&amp; serialNumber)
{
    ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Starting device &quot; &lt;&lt; serialNumber;
    auto device = rsContext.getDevice(serialNumber);
    device-&gt;enableInfrared();
    device-&gt;enableDepth();
    device-&gt;enableColor();
    device-&gt;startPipeline();

    // Work in device depth space (should be 640x360).
    device-&gt;alignMode = ofxRealSense2::Device::Align::Color;
    deviceWidth = device-&gt;getColorPix().getWidth();
    deviceHeight = device-&gt;getColorPix().getHeight();
    colorImg.allocate(deviceWidth, deviceHeight, OF_IMAGE_COLOR);

    // Uncomment this to add the device specific settings to the GUI.
    //guiPanel.add(device-&gt;params);
}

//--------------------------------------------------------------
void ofApp::update()
{
    rsContext.update();

    std::shared_ptr&lt;ofxRealSense2::Device&gt; rsDevice = rsContext.getDevice(0);
    if (rsDevice)
    {
        colorImg.setFromPixels(rsDevice-&gt;getColorPix());
        if (appMode == 0) // Searching.
        {
            renderChessboard();

            colorMat = ofxCv::toCv(colorImg.getPixels());
            cv::Size patternSize = cv::Size(CHESSBOARD_COLS - 1, CHESSBOARD_ROWS - 1);
            int chessFlags = cv::CALIB_CB_ADAPTIVE_THRESH + cv::CALIB_CB_FAST_CHECK;
            bool foundChessboard = cv::findChessboardCorners(colorMat, patternSize, cvPoints, chessFlags);
            if (foundChessboard)
            {
                cv::Mat grayMat;
                cv::cvtColor(colorMat, grayMat, CV_RGB2GRAY);
                cv::cornerSubPix(grayMat, cvPoints, cv::Size(11, 11), cv::Size(-1, -1), cv::TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
                cv::drawChessboardCorners(colorMat, patternSize, cv::Mat(cvPoints), foundChessboard);
                colorImg.update();
            }
        }
        else if (appMode == 1) // Testing.
        {
            // Map points in both world space and projector space and draw them.
            // If they are calibrated correctly they should be drawn on top of one another.
            glm::vec2 clampedTestPoint = glm::vec2(
                ofClamp(testPoint.x, 0, deviceWidth - 1),
                ofClamp(testPoint.y, 0, deviceHeight - 1));
            glm::vec3 worldPoint = rsDevice-&gt;getWorldPosition(clampedTestPoint.x, clampedTestPoint.y);
            glm::vec2 projectedPoint = kpToolkit.getProjectedPoint(worldPoint);

            renderTestPoint(projectedPoint);
        }
        else // Rendering.
        {
            // Threshold the depth.
            ofFloatPixels rawDepthPix = rsDevice-&gt;getRawDepthPix();
            ofFloatPixels thresholdNear, thresholdFar, thresholdResult;
            ofxCv::threshold(rawDepthPix, thresholdNear, nearThreshold);
            ofxCv::threshold(rawDepthPix, thresholdFar, farThreshold, true);
            ofxCv::bitwise_and(thresholdNear, thresholdFar, thresholdResult);
            thresholdImg.setFromPixels(thresholdResult);

            // Find contours.
            contourFinder.setMinAreaNorm(minArea);
            contourFinder.setMaxAreaNorm(maxArea);
            contourFinder.findContours(thresholdImg);

            renderContours();
        }
    }
}

void ofApp::draw()
{
    std::shared_ptr&lt;ofxRealSense2::Device&gt; rsDevice = rsContext.getDevice(0);
    if (rsDevice)
    {
        ofSetColor(255);
        colorImg.draw(0, 0);

        rsDevice-&gt;getDepthTex().draw(colorImg.getWidth(), 0);

        if (thresholdImg.isAllocated())
        {
            ofPushMatrix();
            ofTranslate(colorImg.getWidth(), rsDevice-&gt;getDepthTex().getHeight());
            {
                thresholdImg.draw(0, 0);
                contourFinder.draw();
            }
            ofPopMatrix();
        }

        if (appMode == 0) // Searching.
        {
            // Use a string stream to print a multi-line message.
            std::ostringstream oss;
            oss &lt;&lt; &quot;SEARCHING MODE&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Position the chessboard by dragging the mouse over the RGB image.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Adjust the size of the chessboard using LEFT / RIGHT or the GUI sliders.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;When the pattern is recognized, hit SPACE to save a set of point-pairs.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Once a few point-pairs have been detected, calibrate using the GUI button.&quot; &lt;&lt; std::endl
                &lt;&lt; std::endl
                &lt;&lt; ofToString(pairsWorld.size()) &lt;&lt; &quot; point-pairs collected.&quot;;

            ofSetColor(255);
            ofDrawBitmapStringHighlight(oss.str(), 10, 380);
        }
        else if (appMode == 1) // Testing.
        {
            // Use a string stream to print a multi-line message.
            std::ostringstream oss;
            oss &lt;&lt; &quot;TESTING MODE&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Click on the RGB image to test a calibrated point.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;The world point will be drawn in RED, the projected point will be drawn in GREEN.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;If the calibration is successful, both points will be drawn on top of each other.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Save the calibration using the GUI button.&quot;;

            ofSetColor(255);
            ofDrawBitmapStringHighlight(oss.str(), 10, 380);

            // Draw the test point on screen.
            ofSetColor(255, 0, 0);
            float pointSize = ofMap(cos(ofGetFrameNum() * 0.1), -1, 1, 3, 40);
            ofDrawCircle(testPoint.x, testPoint.y, pointSize);
        }
        else // Rendering.
        {
            // Use a string stream to print a multi-line message.
            std::ostringstream oss;
            oss &lt;&lt; &quot;RENDERING MODE&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;Adjust the depth threshold using the GUI sliders.&quot; &lt;&lt; std::endl
                &lt;&lt; &quot;The thresholded silhouette will mask the rest of the projected image.&quot;;
        }
    }

    guiPanel.draw();
}

void ofApp::drawProjector(ofEventArgs&amp; args)
{
    ofBackground(255);
    ofSetColor(255);
    fboProjection.draw(0, 0);
}

void ofApp::keyPressed(int key)
{
    if (key == ' ')
    {
        addPointPairs();
    }
    else if (key == 'c') 
    {
        calibrateSpaces();
    }
    else if (key == 's')
    {
        saveCalibration();
    }
    else if (key == 'l')
    {
        loadCalibration();
    }
}

void ofApp::mousePressed(int x, int y, int button)
{
    if (appMode == 0) // Searching
    {
        if (ofGetMousePressed() &amp;&amp;
            ofInRange(x, 0, deviceWidth) &amp;&amp;
            ofInRange(y, 0, deviceHeight))
        {
            // Save the normalized point position.
            chessboardX = ofMap(x, 0, deviceWidth, 0, 1);
            chessboardY = ofMap(y, 0, deviceHeight, 0, 1);
        }
    }
    else if (appMode == 1) // Testing
    {
        testPoint = glm::vec2(x, y);
    }
    else
    {

    }
}

void ofApp::mouseDragged(int x, int y, int button)
{
    if (appMode == 0) // Searching
    {
        if (ofGetMousePressed() &amp;&amp;
            ofInRange(x, 0, deviceWidth) &amp;&amp;
            ofInRange(y, 0, deviceHeight))
        {
            // Save the normalized point position.
            chessboardX = ofMap(x, 0, deviceWidth, 0, 1);
            chessboardY = ofMap(y, 0, deviceHeight, 0, 1);
        }
    }
    else if (appMode == 1) // Testing
    {
        testPoint = glm::vec2(x, y);
    }
}

void ofApp::renderChessboard()
{
    float cellSize = chessboardSize / CHESSBOARD_COLS;

    currProjectorPoints.clear();

    fboProjection.begin();
    {
        // Remap top-left to projection space.
        int boardX = ofMap(chessboardX, 0, 1, 0, fboProjection.getWidth());
        int boardY = ofMap(chessboardY, 0, 1, 0, fboProjection.getHeight());

        // Clear white and draw black cells.
        ofClear(255, 0);
        ofSetColor(0);

        for (int j = 0; j &lt; CHESSBOARD_ROWS; j++)
        {
            for (int i = 0; i &lt; CHESSBOARD_COLS; i++)
            {
                int cellX = boardX + i * cellSize;
                int cellY = boardY + j * cellSize;

                if ((i + j) % 2 == 0)
                {
                    // Only draw black cells.
                    ofDrawRectangle(cellX, cellY, cellSize, cellSize);
                }

                if (i &gt; 0 &amp;&amp; j &gt; 0)
                {
                    // Add normalized intersection points to the list.
                    float normX = ofMap(cellX, 0, fboProjection.getWidth(), 0, 1);
                    float normY = ofMap(cellY, 0, fboProjection.getHeight(), 0, 1);
                    currProjectorPoints.push_back(glm::vec2(normX, normY));
                }
            }
        }
    }
    fboProjection.end();
}

void ofApp::renderTestPoint(glm::vec2 projectedPoint)
{
    float pointSize = ofMap(sin(ofGetFrameNum() * 0.1), -1, 1, 3, 40);

    fboProjection.begin();
    {
        ofBackground(255);

        // Point is normalized, so it needs to be mapped to the projector size.
        float projX = ofMap(projectedPoint.x, 0, 1, 0, fboProjection.getWidth());
        float projY = ofMap(projectedPoint.y, 0, 1, 0, fboProjection.getHeight());

        ofSetColor(0, 255, 0);
        ofDrawCircle(projX, projY, pointSize);
    }
    fboProjection.end();
}

void ofApp::renderContours()
{
    fboProjection.begin();
    {
        // Clear white and draw black contours.
        ofClear(255, 0);
        ofSetColor(0);

        std::shared_ptr&lt;ofxRealSense2::Device&gt; rsDevice = rsContext.getDevice(0);
        if (rsDevice)
        {
            for (int i = 0; i &lt; contourFinder.size(); i++)
            {
                // Map contour using calibration and draw to main window
                ofBeginShape();
                std::vector&lt;cv::Point&gt; points = contourFinder.getContour(i);
                for (int j = 0; j &lt; points.size(); j++)
                {
                    glm::vec3 worldPoint = rsDevice-&gt;getWorldPosition(points[j].x, points[j].y);
                    if (worldPoint.z &gt; 0)
                    {
                        glm::vec2 projectedPoint = kpToolkit.getProjectedPoint(worldPoint);
                        if (ofInRange(projectedPoint.x, 0, 1) &amp;&amp; ofInRange(projectedPoint.y, 0, 1))
                        {
                            if (flipX)
                            {
                                projectedPoint.x = 1.0 - projectedPoint.x;
                            }
                            if (flipY)
                            {
                                projectedPoint.y = 1.0 - projectedPoint.y;
                            }
                            ofVertex(projectedPoint.x * fboProjection.getWidth(), projectedPoint.y * fboProjection.getHeight());
                            ofLog() &lt;&lt; &quot;Adding world &quot; &lt;&lt; worldPoint &lt;&lt; &quot; // point &quot; &lt;&lt; projectedPoint &lt;&lt; &quot; // proj &quot; &lt;&lt; (projectedPoint.x * fboProjection.getWidth()) &lt;&lt; &quot;, &quot; &lt;&lt; (projectedPoint.y * fboProjection.getHeight());
                        }
                    }
                }
                ofEndShape();
            }
        }
    }
    fboProjection.end();
}

void ofApp::addPointPairs()
{
    std::shared_ptr&lt;ofxRealSense2::Device&gt; rsDevice = rsContext.getDevice(0);
    if (!rsDevice)
    {
        ofLogError(__FUNCTION__) &lt;&lt; &quot;No RealSense detected!&quot;;
        return;
    }

    // Count the number of world points.
    int numDepthPoints = 0;
    for (int i = 0; i &lt; cvPoints.size(); i++)
    {
        glm::vec3 worldPoint = rsDevice-&gt;getWorldPosition(cvPoints[i].x, cvPoints[i].y);
        if (worldPoint.z &gt; 0)
        {
            numDepthPoints++;
        }
    }

    int chessboardTotal = (CHESSBOARD_COLS - 1) * (CHESSBOARD_ROWS - 1);
    if (numDepthPoints != chessboardTotal)
    {
        ofLogError(__FUNCTION__) &lt;&lt; &quot;Only found &quot; &lt;&lt; numDepthPoints &lt;&lt; &quot; / &quot; &lt;&lt; chessboardTotal &lt;&lt; &quot; points!&quot;;
        return;
    }

    // Found all chessboard points in the world, add both sets to the lists we will use for calibration.
    for (int i = 0; i &lt; cvPoints.size(); i++)
    {
        glm::vec3 worldPoint = rsDevice-&gt;getWorldPosition(cvPoints[i].x, cvPoints[i].y);
        pairsWorld.push_back(worldPoint);
        pairsProjector.push_back(currProjectorPoints[i]);
        ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Adding pair i: &quot; &lt;&lt; worldPoint &lt;&lt; &quot; =&gt; &quot; &lt;&lt; currProjectorPoints[i];
    }

    ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Added &quot; &lt;&lt; chessboardTotal &lt;&lt; &quot; point-pairs.&quot;;
}

void ofApp::calibrateSpaces()
{
    kpToolkit.calibrate(pairsWorld, pairsProjector);
    appMode = 1;

    pairsWorld.clear();
    pairsProjector.clear();
}

void ofApp::saveCalibration()
{
    if (kpToolkit.saveCalibration(&quot;calibration.json&quot;))
    {
        ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Calibration saved!&quot;;
    }
}

void ofApp::loadCalibration()
{
    if (kpToolkit.loadCalibration(&quot;calibration.json&quot;))
    {
        ofLogNotice(__FUNCTION__) &lt;&lt; &quot;Calibration loaded!&quot;;
        appMode = 2;
    }
}
</code></pre><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=https://seeingmachines.betamovement.net/docs/class-8/frame-buffers/><div class="card my-1"><div class="card-body py-2">&larr; Frame Buffers</div></div></a><a class=ms-auto href=https://seeingmachines.betamovement.net/docs/class-10/machine-learning/><div class="card my-1"><div class="card-body py-2">Machine Learning &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a class=text-muted href=https://github.com/>GitHub</a>, <a class=text-muted href=https://gohugo.io/>Hugo</a>, and <a class=text-muted href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=https://seeingmachines.betamovement.net/js/bootstrap.min.7315382e899a7d7132d93fdf0d6682c67a93f0e72ee1a757f33f3207de3b14e2460a935c9d4cec78f86d94ab892d053c70540695eed0bbb7bf5bdc979e6f5a9f.js integrity="sha512-cxU4LomafXEy2T/fDWaCxnqT8Ocu4adX8z8yB947FOJGCpNcnUzsePhtlKuJLQU8cFQGle7Qu7e/W9yXnm9anw==" crossorigin=anonymous defer></script>
<script src=https://seeingmachines.betamovement.net/js/highlight.min.93c197e7097c47fc0788b21721b3c308e18e43299f1e45e8ff2697d13cd62908cc5949a053c1fb7242d7b4a60eb07bd106061252f7aa925ef7e91033ea59d9b9.js integrity="sha512-k8GX5wl8R/wHiLIXIbPDCOGOQymfHkXo/yaX0TzWKQjMWUmgU8H7ckLXtKYOsHvRBgYSUveqkl736RAz6lnZuQ==" crossorigin=anonymous defer></script>
<script src=https://seeingmachines.betamovement.net/main.min.162c56a0426544de0d010e66c56e321579655c400c9aae06a6823e7682de379adadf2165bd416fea191e4e7e410fbf1fd2c35a759aa43ff2e3787067669bf81b.js integrity="sha512-FixWoEJlRN4NAQ5mxW4yFXllXEAMmq4GpoI+doLeN5ra3yFlvUFv6hkeTn5BD78f0sNadZqkP/LjeHBnZpv4Gw==" crossorigin=anonymous defer></script>
<script src=https://seeingmachines.betamovement.net/index.min.a727b3a77ecb7daff052ddee894a0a999926ff0e0fcb1805c1f4e63ab960b0cb82e6e29eef42851593d5db840c7662e11d10bf2e03614dbaf473204fce5aad1e.js integrity="sha512-pyezp37Lfa/wUt3uiUoKmZkm/w4PyxgFwfTmOrlgsMuC5uKe70KFFZPV24QMdmLhHRC/LgNhTbr0cyBPzlqtHg==" crossorigin=anonymous defer></script></body></html>