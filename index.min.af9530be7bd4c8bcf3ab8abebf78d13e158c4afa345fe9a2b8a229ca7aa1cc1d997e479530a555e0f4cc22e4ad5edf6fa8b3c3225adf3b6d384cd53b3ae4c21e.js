var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/assignments/assignment-1/",title:"Assignment 1",description:`Conway\u0026rsquo;s Game of Life # Conway\u0026rsquo;s Game of Life is a cellular automata simulation where a 2D grid of cells evolve between alive and dead states.
The Game of Life is a zero-player game, where you just set the initial state and the rules, and watch the evolution happen. A cell\u0026rsquo;s state in the next frame depends on the state of its immediate neighbors in the current frame. It is set by counting the number of live or dead neighbors and applying the corresponding rule.`,content:`Conway\u0026rsquo;s Game of Life # Conway\u0026rsquo;s Game of Life is a cellular automata simulation where a 2D grid of cells evolve between alive and dead states.
The Game of Life is a zero-player game, where you just set the initial state and the rules, and watch the evolution happen. A cell\u0026rsquo;s state in the next frame depends on the state of its immediate neighbors in the current frame. It is set by counting the number of live or dead neighbors and applying the corresponding rule. Depending on this initial state and rules, interesting patterns can emerge where cells can appear to oscillate or travel across the board. Instructions # Your assignment is to build your version of the Game of Life using openFrameworks. This will give you an opportunity to play with an image\u0026rsquo;s pixel data.
Use an ofImage as your 2D canvas. Set black 0 as the pixel value for a dead cell, and white 255 as the pixel value for a live cell. Set the initial values to whatever you want. You can use random values or try a pattern. Neighbors are the pixels on the top-left, top-center, top-right, middle-left, middle-right, bottom-left, bottom-center, and bottom-right (8 neighbors total). Make sure to handle edge cases appropriately! You can either ignore the invalid neighbors or wrap around the texture. We will follow the same rules as the original game of life:
Isolation: a live â¬œ cell with less than 2 live neighbors will die ðŸ˜¥. Overcrowding: a live â¬œ cell with 4 or more neighbors will die ðŸ˜µ. Reproduction: a dead â¬› cell with exactly 3 live neighbors will live ðŸ£. Here is some pseudo-code representing my rules:
for (each cell in image): count live neighbors if (cell is live): if (num live neighbors \u0026lt; 2): cell dies if (num live neighbors \u0026gt; 3): cell dies if (cell is dead): if (num live neighbors == 3): cell lives Sorry, your browser doesn't support embedded videos. Hint: How can we fill an ofImage with values? ofImage.allocate() will allocate memory for the image without having to load a file from disk. ofImage.getPixels() will return an ofPixels object which we can use to access the pixel data. ofImage.getPixels() makes a copy of the pixel data. After we make our edits, we need to save the new data back to the ofImage using ofImage.setFromPixels(). lifeImg.allocate(40, 30, OF_IMAGE_GRAYSCALE); ofPixels lifePix = lifeImg.getPixels(); for (int y = 0; y \u0026lt; lifeImg.getHeight(); y++) { for (int x = 0; x \u0026lt; lifeImg.getWidth(); x++) { if (ofRandomuf() \u0026lt; 0.5) { lifePix.setColor(x, y, ofColor(0)); } else { lifePix.setColor(x, y, ofColor(255)); } } } // getPixels() makes a copy of the pixels, so we need to // use setFromPixels to set the new values back on the image. lifeImg.setFromPixels(dogPix); Bonus Points! # If you are looking for an additional challenge, try adding the following extra features:
Press a key to pause / start the simulation. Press a key to reset the grid to random values. Press a key to set the grid to all live cells, and another for all dead cells. Click the mouse on a cell and toggle its state! Delivery # Name your project SM01-FirstLast where First is your first name and Last is your last name. - OF/ - apps/ - seeing-machines/ - SM01-ElieZananiri/ - src/ - bin/ - addons.make - SM01-ElieZananiri.sln - SM01-ElieZananiri.vcxproj - ... Only submit the necessary files to rebuild your project.
This includes sources, the addons.make file, and any resources in your data folder. No project or compiled files. In the example above, you would only keep the src folder, addons.make file, and bin/data if you are using any external assets. Zip the SM01-ElieZananiri parent directory. - seeing-machines/ - SM01-ElieZananiri/ - src/ - addons.make OPTIONAL In true ITP fashion, you can make a blog post about your project. If you do, please send me the link!
Post your project link to the #assignments channel on our Discord server. Do not send it by email. Do not send it as a DM.
Attach the packaged ZIP to your message. If that does not work, upload it to Google Drive and send the link. If you made a blog post or added your project to GitHub, send a link to that too. Come to class with a working project on a working computer, and be prepared to talk and answer questions about it. Time allowing, some of you will demo your projects to the class!
Thank you!
Solution # Here are example projects for a basic solution and a fancy solution (with all the bonus features).
A few things to watch out for:
While iterating through the pixels, we do not want to read values from the same array we are writing to. If we do this, we will be reading values that we are modifying and will get unexpected results. When counting neighbors, we want to make sure to skip the current pixel. We need to look at the 8 surrounding pixels only. One line to look at closely in the example solution is:
lifeImg.getTexture().setTextureMinMagFilter(GL_NEAREST, GL_NEAREST); When an image is scaled up, it needs additional pixels to fill in the extra resolution. Conversely, when an image is scaled down, it removes some of its original pixels because the resolution is smaller. The min/mag filters define how the renderer should handle these situations.
The default mode uses linear interpolation GL_LINEAR. This blends the nearby pixels together to make new pixels and may look fuzzy, which is not what we want. The nearest neighbor mode GL_NEAREST uses the nearest pixel value for the added pixels without any blending. This keeps the image sharp at any resolution, but it may look pixelated. `}),e.add({id:1,href:"/docs/class-0/",title:"Class 0",description:"Class 0",content:""}),e.add({id:2,href:"/docs/class-0/foreword/",title:"Foreword",description:`Introductions # A bit about me:
Beta Movement A bit about you:
What did you do before ITP? Tell me about your programming experience. What are you hoping to get out of the class? Senses # What is a sense? # A capacity that allows organisms to perceive the conditions or properties of things, either around them or internally.
Human senses # We have traditionally only considered five human senses:`,content:`Introductions # A bit about me:
Beta Movement A bit about you:
What did you do before ITP? Tell me about your programming experience. What are you hoping to get out of the class? Senses # What is a sense? # A capacity that allows organisms to perceive the conditions or properties of things, either around them or internally.
Human senses # We have traditionally only considered five human senses:
Sight Hearing Smell Taste Touch Which of these would you say we use more predominantly? Neurologist Dr. Wilder Penfield conceived the Sensory Homuncilus, a physical representation of how the human body would look if the various body parts were sized in proportion to the cortical area used for their specific sensory functions.
A 2-D cortical sensory homunculus 3-D interpretation by Sharon Price James This is a simplification, but demonstrates that touch is the most predominant sense, followed by taste, hearing, smell, and finally sight.
We also have many other senses, which we use in our daily life but are less obvious:
Equilibrium Temperature Pain Thirst and hunger Direction Time Etc. Modeling machines # In order to get machines to understand their environment, we tend to outfit them with sensors that are similar to our own senses.
What are some sensors that we use on computers? Sight Digital camera IR receiver Hearing Microphone Touch Trackpad Pressure sensor Keyboard Equilibrium Gyroscope Direction Magnetometer Compass You\u0026rsquo;ve probably used some of these in your previous classes and projects.
The right tool for the job # The focus of Seeing Machines will be to use sensors with computers (rather than microcontrollers), for the purpose of building successful interactive experiences.
The devices we will use will have SDKs (software development kits) and interfaces for many platforms and languages. This is great as it allows us to use something we are already familiar with, however some tools are better suited than others for specific tasks. For example, Python is great at text and language processing, Max is best at sound analysis, and Unity is ideal to get up and running with VR.
A lot of these platforms use very similar paradigms, and the difficulty of moving from one to the other tends to be more about getting familiar with a new environment and different coding syntax than anything else.
The majority of the programming for this class will be done in openFrameworks (OF) and we will sometimes detour to another platform when it makes sense. While C++ can be daunting, it is a very high performance language that is widely used, and OF takes a lot of the initial hurdles away!
About halfway through the semester, we will have a lecture on communication, where we will learn various methods for different pieces of software and hardware \u0026ldquo;talk\u0026rdquo; to each other.
`}),e.add({id:3,href:"/docs/class-0/intro-to-of/",title:"Intro to OF",description:`What is openFrameworks? # openFrameworks (OF) is an open source cross-platform C++ toolkit designed to assist the creative process, by providing a simple and intuitive framework for experimentation.
OF is distributed under the MIT License, which gives everyone the freedoms to use openFrameworks in any context:
Commercial or non-commercial. Public or private. Open or closed source. What is C++? # A programming language. General purpose. Fairly low level, but can be programmed in a high level way.`,content:`What is openFrameworks? # openFrameworks (OF) is an open source cross-platform C++ toolkit designed to assist the creative process, by providing a simple and intuitive framework for experimentation.
OF is distributed under the MIT License, which gives everyone the freedoms to use openFrameworks in any context:
Commercial or non-commercial. Public or private. Open or closed source. What is C++? # A programming language. General purpose. Fairly low level, but can be programmed in a high level way. Compiled (it\u0026rsquo;s really fast). Widely used. Libraries # OF is written in C++. It makes it easier to interface with the many libraries that have been written in C and C++ without needing to rely on a wrapper for another language.
Libraries are collections of code that do something common or useful. For example:
OpenGL for drawing graphics. FreeType for loading and rendering fonts. FreeImage for loading image files. AVFoundation for playing videos. OF is the glue that ensures these libraries work together well.
It is a consistent and intuitive interface to these libraries.
For example, loading a font using FreeType directly would look something like this:
FT_New_Face(...); FT_Set_Char_Size(...); And with OF would look like this:
ofTrueTypeFont font; font.load(...); Loading an image using FreeImage directly:
FreeImage_OpenMemory(...); FreeImage_LoadFromMemory(...); FreeImage_GetBits(...); And with OF:
ofImage img; img.load(...); Open Source # OF is distributed as source code.
An open book, giving the curious a good starting point for learning about C++ library wrangling. A work in progress, keeping the code visible allowing for easier changes and feedback. An invitation for users to modify the toolkit to their taste or needs. Over 70 people have contributed to the core, and there are more than 1500 addons extending the base functionality of the toolkit.
Comparisons with Processing # openFrameworks and Processing have many similarities. In fact, OF is inspired by Processing!
When possible, openFrameworks tries to maintain parity with Processing, making moving from one to the other very easy. Compare the following code snippets:
void setup() { frameRate(60); background(0); } void draw() { fill(255, 0, 0); rect(10, 10, 50, 50); } void ofApp::setup() { ofSetFrameRate(60); ofBackground(0); } void ofApp::draw() { ofSetColor(255, 0, 0); ofDrawRectangle(10, 10, 50, 50); } âœŒï¸ What does the :: mean?
:: is a scope resolution operator in C++. It is used to show the relationship between methods (functions) and classes. Methods can be defined anywhere in the source code, so we need a way to know where they belong when they are defined.
For example, void ofApp::draw() means \u0026ldquo;define the draw() function that belongs to the ofApp class\u0026rdquo;.
Getting Started # Installation # Download the openFrameworks package for your environment.
Follow the corresponding setup guide.
Unlike Processing, OF does not come with its own development environment (IDE). Instructions to set this up will be included in the guide. You will use Xcode for development under macOS and Visual Studio for development under Windows. âš ï¸ If you encounter a build system error when compiling for Xcode, try changing the Build System dropdown in the Project Settings.
Project Generation # To create a new project, you are strongly encouraged to use the OF Project Generator.
This application can be found in your downloaded package, under /path/to/OF/projectGenerator-XXX. The Project Generator will take care of adding any files and libraries needed to build your applications. The first time you run the Project Generator, you\u0026rsquo;ll be asked to set the path to the openFrameworks installation on your system.
You can then create a project by giving it a name and a save path. It is recommended to save your projects under path/to/OF/apps/seeing-machines/.
Click Generate to create the project files. Once that is complete, you can click on the Open in IDE button to open the project.
Anatomy of an OF Project # A basic OF project will include three files you can edit.
main.cpp contains the main() function. This is the entry point to the program.
The main function is where the application window is set. You can set up the window dimensions, renderer used, graphics quality, additional windows, etc.
#include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofApp.h\u0026quot; int main() { ofSetupOpenGL(1920, 1080, OF_WINDOW); ofRunApp(new ofApp()); } âœŒï¸ What does #include mean?
The # symbol is used to indicate a compiler directive. When a file has the line #include \u0026quot;someFile.h\u0026quot;, this tells the compiler to insert the code from that specific file into the source code.
If we want to use any classes or functions defined in other files, we need to #include these in our code so that the compiler knows where to look for them.
The other two files define the ofApp class. You can think of ofApp as the main class that holds and runs all the components belonging to your program, kind of like a sketch in Processing.
In C++, classes are defined in two parts: the header (declaration) and the implementation (definition). The header defines what a class is, and the implementation defines how a class operates.
The header will usually have extension .h or .hpp. This is where all variables and methods in the class are listed. You can think of this as a table of contents for the class. When classes link to each other using #include, they will only refer to the header class as they only need to \u0026ldquo;know\u0026rdquo; what variables and methods are available to them, but not how these are implemented. This reduces dependencies and in turn compilation times.
#pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void keyPressed(int key); void keyReleased(int key); void mouseMoved(int x, int y); void mouseDragged(int x, int y, int button); void mousePressed(int x, int y, int button); void mouseReleased(int x, int y, int button); void mouseEntered(int x, int y); void mouseExited(int x, int y); void windowResized(int w, int h); void dragEvent(ofDragInfo dragInfo); void gotMessage(ofMessage msg); }; âœŒï¸ What does #pragma once mean?
We now know #include will insert the contents of another file into our code, however we only want to include every piece of code once in our application. You will notice many files will have #include \u0026quot;ofMain.h\u0026quot; at the top, but that code cannot be inserted over and over, as this will give us duplicate classes and functions with the same name.
This is where the #pragma once directive comes in. It tells the compiler to only include the contents of the file once, no matter how many times it is referenced with #include.
As a general rule, you should always start your header files with the line #pragma once.
The implementation will have extension .cpp. This is where all the methods declared in the header are defined.
#include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Called once at app startup. } void ofApp::update() { // Called at the start of every frame. } void ofApp::draw() { // Called every frame after update. } void ofApp::keyPressed(int key) { // Called when a key is pressed. } void ofApp::keyReleased(int key) { // Called when a key is released. } void ofApp::mouseMoved(int x, int y) { // Called when the mouse is moved and no buttons are pressed. } void ofApp::mouseDragged(int x, int y, int button) { // Called when the mouse is moved while a button is pressed down. } void ofApp::mousePressed(int x, int y, int button) { // Called when a mouse button is pressed. } void ofApp::mouseReleased(int x, int y, int button) { // Called when a mouse button is released. } void ofApp::mouseEntered(int x, int y) { // Called when the mouse cursor enters the application window. } void ofApp::mouseExited(int x, int y) { // Called when the mouse cursor exits the application window. } void ofApp::windowResized(int w, int h) { // Called when the application window is resized. } void ofApp::gotMessage(ofMessage msg) { // I have no idea :/ } void ofApp::dragEvent(ofDragInfo dragInfo) { // Called when a file or set of files are dragged onto the application window. } Note that the placeholder ofApp already has stubs for common methods you may want to use. You can keep these in or delete them, but if you get rid of them you\u0026rsquo;ll need to do so in both the header and the implementation files.
Reference # OF ships with a multitude of examples in the path/to/OF/examples folder, and this is the best way to get familiar with the tool. Note that project files need to be created for these using the Project Generator before they can be built.
OF also has comprehensive documentation on its website, as well as an active user forum, which are other great places to get information.
If you are more of a visual learner, Lewis Lepton\u0026rsquo;s openFrameworks Tutorial Series on YouTube is an excellent resource.
`}),e.add({id:4,href:"/docs/assignments/assignment-2/",title:"Assignment 2",description:`Video Effect # Instructions # Your assignment is to create an original video effect using openFrameworks and OpenCV. This will give you an opportunity to play with an imageâ€™s pixel data, and to get familiar with computer vision algorithms.
You can use some of the algorithms we covered in class (background subtraction, color tracking, etc.) or any other CV algorithm. Make sure to browse through ofxCv examples to see what is available to you!`,content:`Video Effect # Instructions # Your assignment is to create an original video effect using openFrameworks and OpenCV. This will give you an opportunity to play with an imageâ€™s pixel data, and to get familiar with computer vision algorithms.
You can use some of the algorithms we covered in class (background subtraction, color tracking, etc.) or any other CV algorithm. Make sure to browse through ofxCv examples to see what is available to you! If you have an idea but are not sure how to do it, ask about it on Discord and we can break it down and figure it out together. Expose your parameters and use a GUI or some form of controller to tweak them for best results. Some of you will demo your project in class. Your effect should therefore work in the conditions of our classroom (size, layout, light, etc.) Bonus Points! # If you are looking for an additional challenge, try adding the following extra features:
Press a key or GUI button to freeze / restart the video capture. Press a key or GUI toggle to switch between live video (ofVideoGrabber) and on-disk video (ofVideoPlayer) as the input. Delivery # Name your project SM02-FirstLast where First is your first name and Last is your last name. - OF/ - apps/ - seeing-machines/ - SM02-ElieZananiri/ - src/ - bin/ - addons.make - SM02-ElieZananiri.sln - SM02-ElieZananiri.vcxproj - ... Only submit the necessary files to rebuild your project.
This includes sources, the addons.make file, and any resources in your data folder. No project or compiled files. In the example above, you would only keep the src folder, addons.make file, and bin/data if you are using any external assets. Zip the SM02-ElieZananiri parent directory. - seeing-machines/ - SM02-ElieZananiri/ - src/ - addons.make OPTIONAL In true ITP fashion, you can make a blog post about your project. If you do, please send me the link!
Post your project link to the #assignments channel on our Discord server. Do not send it by email. Do not send it as a DM.
Attach the packaged ZIP to your message. If that does not work, upload it to Google Drive and send the link. If you made a blog post or added your project to GitHub, send a link to that too. Come to class with a working project on a working computer, and be prepared to talk and answer questions about it. Time allowing, some of you will demo your projects to the class!
Thank you!
`}),e.add({id:5,href:"/docs/class-1/",title:"Class 1",description:"Class 1",content:""}),e.add({id:6,href:"/docs/class-1/variables-and-arrays/",title:"Variables and Arrays",description:`Data Types # Let\u0026rsquo;s start with the basics and review data types in C++.
Main Primitives # int # 32 bits of data (usually but not always) represents a whole number between -2,147,483,648 and 2,147,483,647 int videoWidth = 1920; int videoHeight = 1080; int numVideoPixels = videoWidth * videoHeight; integers do not support decimal points âœŒï¸ Integer division / and modulo % operators
Operations on integers return integers. This is particularly important to remember with division /.`,content:`Data Types # Let\u0026rsquo;s start with the basics and review data types in C++.
Main Primitives # int # 32 bits of data (usually but not always) represents a whole number between -2,147,483,648 and 2,147,483,647 int videoWidth = 1920; int videoHeight = 1080; int numVideoPixels = videoWidth * videoHeight; integers do not support decimal points âœŒï¸ Integer division / and modulo % operators
Operations on integers return integers. This is particularly important to remember with division /.
int numStudents = 17; int studentsPerTeam = 4; int numTeams = numStudents / studentsPerTeam; // 4 not 4.25! The modulo % operator is used on integers to get the remainder of a division.
int leftoverStudents = numStudents % studentsPerTeam; // 1 student without a team :( unsigned int # the unsigned (no +/- sign) version of int, only positive numbers represents 0 and positive whole numbers up to 4,294,967,295 char # 8 bits of data represents a whole number between -128 and 127 char numStudents = 17; char studentsPerTeam = 4; char numTeams = numStudents / studentsPerTeam; // 4 not 4.25! if we try to use a char to represent a larger number, we will end up with the wrong value but C++ will not flag an error! char videoWidth = 1920; // ? unsigned char # the unsigned (no +/- sign) version of char, only positive numbers represents 0 and positive whole numbers up to 255 often used to represent characters in a string, using the ASCII table for conversion unsigned char H = 72; unsigned char i = 'i'; cout \u0026lt;\u0026lt; H \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; âœŒï¸ What do cout and endl do?
cout is a command to send text output to the console. The \u0026lt;\u0026lt; (left shift) operator is used to send data to the output, and can be used multiple times to add more text to the output.
New lines are not automatically added. The endl command is used to send a new line. This will usually be found at the end of a cout line of code.
bool # 1 bit of data represents true or false, 0 or 1, \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no\u0026rdquo;, etc. we can use the keywords true and false to set a boolean value bool isTheSkyBlue = true; bool isThisBoring = false; we can also use numbers, where 0 evaluates to false and any other number evaluates to true bool numStudents = 17; // true if (numStudents) { cout \u0026lt;\u0026lt; \u0026quot;Class is in session!\u0026quot; \u0026lt;\u0026lt; endl; } âš ï¸ Note that even though you can use numbers to represent a boolean, the bool data type only has enough memory to represent 0 or 1.
bool numStudents = 17; cout \u0026lt;\u0026lt; \u0026quot;There are \u0026quot; \u0026lt;\u0026lt; numStudents \u0026lt;\u0026lt; \u0026quot; students in class\u0026quot; \u0026lt;\u0026lt; endl; // 1 float # 32 bits of data represents a decimal number with ~7 significant digits float is short for \u0026ldquo;floating point\u0026rdquo;, which means that the decimal point can move positions (e.g. we can represent 1.23456 and 123.456 with the same amount of memory) float videoWidth = 1920; float videoHeight = 1080; float aspectRatio = videoWidth / videoHeight; // 1.777778 Additional Primitives # The following primitive types are not used as often but are still useful if we need to optimize and use less memory, or increase precision and use more memory.
short and unsigned short # 16 bits of data represents whole numbers between -32,768 and 32,767 (signed) or 0 and 65,535 (unsigned) long and unsigned long # 64 bits of data represents whole numbers between -9M and 9M (signed) or 0 and 18M (unsigned) double # 64 bits of data represents floating point numbers with ~15 significant digits Strings # Like in most programming languages, strings (sequences of characters) are a complex class type, but they have special rules and optimizations applied to them since they are used very often.
string name = \u0026quot;John Doe\u0026quot;; string objects have a variety of methods (class functions) we can use to access their properties.
string name; if (name.empty()) { cout \u0026lt;\u0026lt; \u0026quot;No name has been set, using default...\u0026quot; \u0026lt;\u0026lt; endl; name = \u0026quot;John Doe\u0026quot;; } cout \u0026lt;\u0026lt; \u0026quot;Hello, \u0026quot; \u0026lt;\u0026lt; name \u0026lt;\u0026lt; endl; We can iterate through a string to get the characters it is made up of.
string name = \u0026quot;John Doe\u0026quot;; cout \u0026lt;\u0026lt; \u0026quot;Your name is: \u0026quot;; for (int i = 0; i \u0026lt; name.size(); i++) { cout \u0026lt;\u0026lt; name.at(i); } cout \u0026lt;\u0026lt; endl; Concatenation # string objects can be concatenated using the + operator.
string first = \u0026quot;John\u0026quot;; string last = \u0026quot;Doe\u0026quot;; string name = first + \u0026quot; \u0026quot; + last; This also sometimes works with non-string types, but not always as the compiler might not know how to use the + operator.
int videoWidth = 1920; int videoHeight = 1080; string resolution = videoWidth + \u0026quot;x\u0026quot; + videoHeight; // OK? float videoWidthf = 1920; float videoHeightf = 1080; string resolutionf = videoWidthf + \u0026quot;x\u0026quot; + videoHeightf; // ERROR! OF has ofToString() helper functions, which can be used to convert other variable types into string.
int videoWidth = 1920; int videoHeight = 1080; string resolution = ofToString(videoWidth) + \u0026quot;x\u0026quot; + ofToString(videoHeight); float videoWidthf = 1920; float videoHeightf = 1080; string resolutionf = ofToString(videoWidthf, 1) + \u0026quot;x\u0026quot; + ofToString(videoHeightf, 1); Arrays # Arrays are used to store multiple values of the same type under a single variable (vs declaring one variable per value).
// An array containing 10 integers (uninitialized). int values[10]; Array elements are accessed using the square bracket [] operator. We can pass in an index to access the corresponding element in the array. Note that arrays are 0-indexed (the index of the first element is 0) in C++.
int values[10]; for (int i = 0; i \u0026lt; 10; i++) { values[i] = i + 1; } Strings as Arrays # A string is an array of char under the hood (along with some extra functionality). Each character in a string is an element in the array and can be accessed using the [] notation.
How would we print out the value of a string one character at a time, using array notation? string name = \u0026quot;John Doe\u0026quot;; cout \u0026lt;\u0026lt; \u0026quot;The name '\u0026quot;; for (int i = 0; i \u0026lt; name.size(); i++) { cout \u0026lt;\u0026lt; name[i]; } cout \u0026lt;\u0026lt; \u0026quot;' has \u0026quot; \u0026lt;\u0026lt; name.size() \u0026lt;\u0026lt; \u0026quot; characters\u0026quot; \u0026lt;\u0026lt; endl; 2D Arrays # Arrays of other arrays are called multidimensional arrays. Instead of each array position holding a single element, it holds an entire array of elements.
Although arrays can have any number of dimensions, we will most often work with two-dimensional arrays.
// An array containing 10 arrays each containing 2 integers (uninitialized). int values[10][2]; Array elements are accessed using multiple square brackets [][] (one per dimension). Nested for-loops can be used to access all the elements.
int values[10][2]; // 10 columns by 2 rows for (int y = 0; y \u0026lt; 2; y++) { for (int x = 0; x \u0026lt; 10; x++) { values[x][y] = x + y; } } How would we set each value to a sequential index (from 0 to 19)? We need to consider each dimension separately as columns and rows, and look at the array as \u0026ldquo;columns of rows\u0026rdquo;. To access a row index (0-1), we always need to add the column offset first (0-9). Each column has 10 rows, so the offset must be multiplied by 10.
int values[10][2]; // 10 columns by 2 rows for (int y = 0; y \u0026lt; 2; y++) { for (int x = 0; x \u0026lt; 10; x++) { values[x][y] = y * 10 + x; } } How would we fill an array of 40 columns by 30 rows with a random true or false value? How could we print it out to the console as a grid layout? We can use the ofRandomuf() OF function, which returns a random value between 0 and 1 (the \u0026ldquo;uf\u0026rdquo; stands for unsigned float). We will set our element to false if the random value is less than 0.5, and set it to true if the value is greater than 0.5.
bool values[40][30]; // Fill the 2D array with 0s and 1s. for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { if (ofRandomuf() \u0026lt; 0.5) { values[x][y] = false; } else { values[x][y] = true; } } } To output the values as a grid, we can once again use nested for-loops to go through the 2D array, print out the characters one at a time, and print out a new line whenever we increment the row index.
// Read back the values as a grid. for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { cout \u0026lt;\u0026lt; values[x][y]; } cout \u0026lt;\u0026lt; endl; // Add a new line after every row. } How would we visualize this grid as pixels on screen? We can draw a cell for each grid position using ofDrawRectangle(...), setting the color for each cell in the loop using ofSetColor(...).
// Draw the array as a grid. int gridSize = 10; for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { if (values[x][y]) { ofSetColor(255); } else { ofSetColor(0); } ofDrawRectangle(x * gridSize, y * gridSize, gridSize, gridSize); } } How would we only set the edge values (border pixels) to true and the remaining pixels to false?
The left edge elements have their column index set to 0. The top edge elements have their row index set to 0. The right edge elements have their column index set to the number of columns (the width) minus 1. In our case, this is 39. The bottom edge elements have their row index set to the number of rows (the height) minus 1. In our case, this is 29. If any of the above conditions are true, our value should also be true.
for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { if (x == 0 || y == 0 || x == 39 || y == 29) { values[x][y] = true; } else { values[x][y] = false; } // The following does the same thing as a single line of code. //values[x][y] = (x == 0 || y == 0 || x == 39 || y == 29); } } How would we create a grid pattern where each cell is 10x10 units?
We want every 10th element in each direction to be true, and the remaining values to be false. We can use the modulo % operator, which is ideal for whenever we want to count things periodically (e.g. every X count, do something).
for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { if (x % 10 == 0 || y % 10 == 0) { values[x][y] = true; } else { values[x][y] = false; } // The following does the same thing as a single line of code. //values[x][y] = (x % 10 == 0 || y % 10 == 0); } } We do not end up with a border on the right or the bottom because 39 and 29 are not divisible by 10. If this is something we wanted, one option would be to increase our 2D array size to 41 by 31, making the last column index 40 and the last row index 30.
1D to 2D Interpretation # There is no difference in computer between a 1D and a 2D array, they are both just many indexed elements in sequence.
We could re-write some of our previous examples using a one-dimensional array, but using two-dimensional access.
bool values[40*30]; // Fill the 2D array with 0s and 1s. for (int y = 0; y \u0026lt; 30; y++) // rows { for (int x = 0; x \u0026lt; 40; x++) // columns { // Calculate the index using the column value, the row value, and the row offset. int idx = y * 40 + x; if (x == 0 || y == 0 || x == 39 || y == 29) { values[idx] = true; } else { values[idx] = false; } // The following does the same thing as a single line of code. //values[idx] = (x == 0 || y == 0 || x == 39 || y == 29); } } In the last few examples, we have been using arrays to generate images. We have interpreted the array element values as colors. This is, in fact, how images are usually stored in computer memory. We will explore this further in the next section.
`}),e.add({id:7,href:"/docs/class-1/images-and-video/",title:"Images and Video",description:`File Formats # Digital images come in a variety of formats, each with their own properties.
Vector Graphics # Vector formats define a set of points and instructions on how to draw them. The instructions are run by a program to raster the image in order to view it.
Some of the more common vector formats are SVG, EPS, PDF, and AI.
If we open the following SVG file in a text editor, we will notice that it is fairly easy to read the format.`,content:`File Formats # Digital images come in a variety of formats, each with their own properties.
Vector Graphics # Vector formats define a set of points and instructions on how to draw them. The instructions are run by a program to raster the image in order to view it.
Some of the more common vector formats are SVG, EPS, PDF, and AI.
If we open the following SVG file in a text editor, we will notice that it is fairly easy to read the format. It almost reads like a Processing program ðŸ˜‰
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot; standalone=\u0026quot;no\u0026quot;?\u0026gt; \u0026lt;svg ... height=\u0026quot;512\u0026quot; width=\u0026quot;512\u0026quot;\u0026gt; ... \u0026lt;g transform=\u0026quot;translate(0,-161.53332)\u0026quot; id=\u0026quot;layer1\u0026quot;\u0026gt; \u0026lt;circle style=\u0026quot;stroke-width:0.26458332;fill:#00ffff;fill-opacity:1\u0026quot; r=\u0026quot;52.916664\u0026quot; cy=\u0026quot;229.26665\u0026quot; cx=\u0026quot;67.73333\u0026quot; id=\u0026quot;path3713\u0026quot; /\u0026gt; \u0026lt;rect y=\u0026quot;228.20831\u0026quot; x=\u0026quot;5.2916665\u0026quot; height=\u0026quot;63.5\u0026quot; width=\u0026quot;63.5\u0026quot; id=\u0026quot;rect4520\u0026quot; style=\u0026quot;fill:#ff0000;fill-opacity:1;stroke-width:0.25843021\u0026quot; /\u0026gt; \u0026lt;path id=\u0026quot;path4524\u0026quot; d=\u0026quot;M 49.514879,171.88985 123.5982,282.2589 Z\u0026quot; style=\u0026quot;fill:none;stroke:#00b400;stroke-width:2.64583325;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\u0026quot; /\u0026gt; \u0026lt;/g\u0026gt; \u0026lt;/svg\u0026gt; Pros:
Small file sizes, because minimal information is being stored. Images can be scaled up without any quality loss or increase in file size. This is because the instruction set does not change, the only thing that changes is the point values. Cons:
Low level of detail. Limited types of effects, because we don\u0026rsquo;t have all the image data available in the format. Raster Graphics # Raster formats define pixel values in a rectangular grid of pixels. The bigger the image, the greater the data set, and thus the larger the file size.
Some of the more common vector formats are JPG, PNG, GIF, and TIF.
Pros:
High quality and detail, especially at high resolutions. More advanced image effects, because every pixel can be edited. Cons:
File sizes tend to be bigger. Images lose quality when scaled up. In order not to end up with huge file sizes, many raster formats are compressed. Some compression methods are lossy, meaning that some of the data is lost when it is compressed, and others are lossless, meaning that all the data is recovered once the data is uncompressed.
Video # Videos are just a series of images that need to be processed and displayed very quickly.
Video formats are always rasters and are mostly compressed.
Some formats are simply extensions of their image counterparts, like Motion JPG for example, which is just a series of JPG-compressed frames. Others are specific to video, like H.264, which has a form of compression over time, where some pixels are predicted based on known pixels in previous and future key frames. This is called temporal compression. Efficient compression is necessary for video because of the huge amount of data that it carries. While film used to run at 24 frames per second, high definition video now runs standard at 60 frames per second, and sometimes goes as high as 240 fps! Combining these fast frame rates with large resolutions like 4K means that hundreds of millions of pixels need to be processed every second for a video to play smoothly.
Processing Images # When working with image data, we will usually want to work with rasterized uncompressed images. This is because many algorithms require looping efficiently through all pixels in an image, or doing quick look-ups between neighboring pixels.
The good news is that this usually happens in the image loader or video codec, before an image or video frame gets to us. For example in OF, FreeImage will automatically decompress JPG or PNG images and provide us the \u0026ldquo;final\u0026rdquo; pixels in the frame.
While we will almost never have to worry about decoding an image or a video frame ourselves, we should still be mindful of what format the data comes in, and make sure that it is suitable for our application.
Images in OF # The data Folder # The simplest way to access files in an OF app is to include them in the project\u0026rsquo;s data folder. If this looks familiar, it\u0026rsquo;s because this idea is borrowed from Processing. The data folder is located in \u0026lt;project\u0026gt;/bin/data and each project will have its own dedicated data folder.
If we drop our files in the data folder, they can be accessed in the app without having to figure out the full path on disk where the file is located, which can be very handy.
ofImage # ofImage is the general type to use to work with images in openFrameworks. ofImage includes methods to load files from disk, draw images to the screen, access pixel data, etc.
ofImage is a type, which we can create like any other variable type.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void draw(); ofImage dogImg; }; âœŒï¸ Declaring variables in the header file
Variables we want to use in any method (function) of our class should be declared in the header. This means that they are in the entire class\u0026rsquo; scope.
If we declare a variable inside one of our methods like ofApp::setup() or ofApp::draw(), that variable will only be part of that specifc method\u0026rsquo;s scope, and will not be accessible outside of it.
We will load an image named dog-grass.jpg from our data folder in the ofApp::setup() function. We only need to load the image into memory once, so we do it when the app starts up.
We want to draw the image every frame, so we will do that in the ofApp::draw() function.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { dogImg.load(\u0026quot;dog-grass.jpg\u0026quot;); } void ofApp::draw() { dogImg.draw(0, 0); } âœŒï¸ What happened to the other built-in ofApp methods?
In this specific example, we are not using most of the ofApp placeholder methods like ofApp::update(), ofApp::keyPressed(), ofApp::mouseMoved(), etc. We can remove them from our class if we will not be using them, and this will increase readability because the code will not be filled with stub functions.
However, note that the method needs to be removed from both the header .h and the implementation .cpp files or else the compiler will assume something is missing and will throw an error.
If we navigate under the hood and see what ofImage.load() is actually doing, we see that it calls many functions from the FreeImage library to determine the file\u0026rsquo;s format, uncompress the data, and load it into values for each pixel.
Image Attributes # An image data structure usually comprises of:
a size (a width and height) a pixel format a value for each pixel This looks a lot like the arrays we have been exploring in the previous section. This makes arrays great options to represent image data in a computer program.
Even though an image has two dimensions (a width and a height), the pixel array is usually one-dimensional, packing the rows one after the other in sequence.
Some frameworks allow accessing pixels using the column x and row y, like PImage.get() in Processing and ofImage.getColor() in openFrameworks. These convenience functions are very useful as they take care of figuring out all the index arithmetic for us.
The following example reads the value of a pixel under the mouse cursor.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Load the dog image. dogImg.load(\u0026quot;dog-grass.jpg\u0026quot;); // Set the window size to match the image. ofSetWindowShape(dogImg.getWidth(), dogImg.getHeight()); } void ofApp::draw() { // Draw the image as the background. ofSetColor(255); dogImg.draw(0, 0); // Get a reference to the image pixels. ofPixels dogPix = dogImg.getPixels(); // Get the color value under the mouse. ofColor color = dogPix.getColor(mouseX, mouseY); // Draw a rectangle under the mouse using the pixel color. ofFill(); ofSetColor(color); ofDrawRectangle(mouseX - 25, mouseY - 25, 50, 50); // Add an outline so we can see the rectangle better. ofNoFill(); ofSetColor(0); ofDrawRectangle(mouseX - 25, mouseY - 25, 50, 50); } ofSetWindowShape() resizes the window to the size of the loaded image. Note that this function can be called any time while the app is running, and can override the starting window dimensions that are set in main.cpp. ofImage.getPixels() returns an ofPixels object containing the pixel color values. ofPixels is a class backed by an array, with helper methods to access the data it contains. ofPixels.getColor() is one of these helper methods, which returns an ofColor value at a specified column and row index. ofColor is a data structure used to access the different channels that make up a color value. Pixel Access # A standard color pixel will have 3 color channels: red, green, and blue (RGB). While Processing packs all channels into a single int, this is not common practice.
The color values are usually packed sequentially in the array. Instead of each pixel holding a single value, it will hold 3.
The pixel array then has total size:
size = width * height * channels In order to access the pixel in a 1D array using a 2D index, we first need to convert it.
index = y * width + x How do we access a pixel index in an RGB image?
Because each pixel has three color values (for each RGB channel), we need to multiply our pixel index by 3 to take that offset into account.
pixel = y * width + x index = pixel * 3 index = (y * width + x) * 3 ofPixels.getColor() can also accept a single argument for the index (instead of two arguments for the column and row). How can we modify the previous example to use the single index version of getColor()? We can use the formula above to convert our column and row to an index value in the color array.
// ofApp.cpp // ... void ofApp::draw() { // ... // Get a reference to the image pixels. ofPixels dogPix = dogImg.getPixels(); // Get the color value under the mouse. //ofColor color = dogPix.getColor(mouseX, mouseY); int index = (mouseY * dogPix.getWidth() + mouseX) * dogPix.getNumChannels(); ofColor color = dogPix.getColor(index); // ... } Note the use of ofPixels.getNumChannels() instead of the literal 3. This ensures the code will work with all image types and not just RGB images.
Conversely, if we want to get a 2D value from a 1D index, we can use integer division:
x = index % width y = index / width The following example reads the value of a pixel sequentially, based on the sketch frame number.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Load the dog image. dogImg.load(\u0026quot;dog-grass.jpg\u0026quot;); // Set the window size to match the image. ofSetWindowShape(dogImg.getWidth(), dogImg.getHeight()); } void ofApp::draw() { // Draw the image as the background. ofSetColor(255); dogImg.draw(0, 0); // Cache the image dimensions in variables for easy access. int imgWidth = dogImg.getWidth(); int imgHeight = dogImg.getHeight(); // Use the modulo operator to make sure the frame index is never // greater than the max number of pixels in the image. int frameIndex = ofGetFrameNum() % (imgWidth * imgHeight); int x = frameIndex % imgWidth; int y = frameIndex / imgWidth; // Get a reference to the image pixels. ofPixels dogPix = dogImg.getPixels(); // Get the color value for this frame. int pixelIndex = frameIndex * dogPix.getNumChannels(); ofColor color = dogPix.getColor(pixelIndex); // Draw a rectangle under the mouse using the pixel color. ofFill(); ofSetColor(color); ofDrawRectangle(x - 25, y - 25, 50, 50); // Add an outline so we can see the rectangle better. ofNoFill(); ofSetColor(0); ofDrawRectangle(x - 25, y - 25, 50, 50); } Image Format # The most common image type we will work with is RGB color images.
We will also work with single-channel formats, usually called grayscale or luminance. These are particularly handy for devices that only capture a brightness level, like infrared cameras or depth sensors.
Some images also have an alpha channel for transparency, like RGBA. Our example image happens to have transparency, but we will encounter this rarely in this class as most sensors do not use the alpha channel.
Another format worth mentioning is YUV, which is a color encoding that is based on the range of human perception. Instead of using three channels for color, it uses one for brightness and two for color shift. This gives similar results to RGB but at much smaller sizes (usually a third), and this is why YUV formats are often used for webcam streams.
Pixel Format # Pixel color values can be stored in a few different formats. The more bits a format can hold, the more range the values can have, and the larger the size of the frame gets.
unsigned char is the most common format. It uses integers and each channel has 8 bits of data and values range from 0 to 255. float uses floating point 32 bit data. The usual range is from 0.0 to 1.0 but this format can be used for HDR effects, where the values can extend past 1.0 or for storing non-color data, where we can even use negative values. We will use float when working with depth sensors and when storing non-color data inside our pixels. unsigned short is another integer format but with 16 bits of data, meaning values range from 0 to 65535. We will also use this format when working with depth sensors, where precision is very important and we need more than the 256 distinct values that we get from unsigned char. The following example demonstrates how to access the pixel array data directly, using ofPixels.getData().
This is a bit more complicated, and may not be necessary in most applications. However, it tends to be the fastest way to manipulate pixel values and is the recommended approach when having to process large images pixel by pixel.
// ofApp.cpp // ... void ofApp::draw() { // ... // Get a reference to the image pixels. unsigned char* dogData = dogImg.getPixels().getData(); // Get the color value for this frame. int numChannels = dogImg.getPixels().getNumChannels(); int pixelIndex = mouseY * dogImg.getWidth() + mouseX; ofColor color = ofColor( dogData[pixelIndex * numChannels + 0], // R dogData[pixelIndex * numChannels + 1], // G dogData[pixelIndex * numChannels + 2] // B ); // ... } âœŒï¸ What does the * after unsigned char mean?
The * represents something called a pointer. Pointers are a complex topic that we will cover in depth later in the course, but for now just think of them as representing arrays with a variable size (or arrays with a size we do not know at compile time).
The code above needs to work for any image of any size, so we cannot assume that the unsigned char array will have a specific number of elements in it. Using unsigned char* tells the compiler that the array size will be dynamically allocated when it is created.
`}),e.add({id:8,href:"/docs/assignments/assignment-3/",title:"Assignment 3",description:`Depth Effect # Instructions # Your assignment is to create an original application using openFrameworks and a depth sensing camera. This will give you an opportunity to get familiar with combined depth and color images, and explore point clouds in 3D.
You can use any depth sensing device you want. There should be plenty of options in the ER. Your application can be either 2D or 3D (or any-D), but should use depth data as input.`,content:`Depth Effect # Instructions # Your assignment is to create an original application using openFrameworks and a depth sensing camera. This will give you an opportunity to get familiar with combined depth and color images, and explore point clouds in 3D.
You can use any depth sensing device you want. There should be plenty of options in the ER. Your application can be either 2D or 3D (or any-D), but should use depth data as input. You can use any of the computer vision (CV) algorithms and functions we have seen so far in this course. This is not required, but might be useful depending on what you are trying to do. If you have an idea but are not sure how to do it, ask about it on Discord and we can break it down and figure it out together. Expose your parameters and use a GUI or some form of controller to tweak them for best results. Some of you will demo your project in class. Your effect should therefore work in the conditions of our classroom (size, layout, light, etc.) Delivery # Name your project SM03-FirstLast where First is your first name and Last is your last name. - OF/ - apps/ - seeing-machines/ - SM03-ElieZananiri/ - src/ - bin/ - addons.make - SM03-ElieZananiri.sln - SM03-ElieZananiri.vcxproj - ... Only submit the necessary files to rebuild your project.
This includes sources, the addons.make file, and any resources in your data folder. No project or compiled files. In the example above, you would only keep the src folder, addons.make file, and bin/data if you are using any external assets. Zip the SM03-ElieZananiri parent directory. - seeing-machines/ - SM03-ElieZananiri/ - src/ - addons.make OPTIONAL In true ITP fashion, you can make a blog post about your project. If you do, please send me the link!
Post your project link to the #assignments channel on our Discord server. Do not send it by email. Do not send it as a DM.
Attach the packaged ZIP to your message. If that does not work, upload it to Google Drive and send the link. If you made a blog post or added your project to GitHub, send a link to that too. Come to class with a working project on a working computer, and be prepared to talk and answer questions about it. Time allowing, some of you will demo your projects to the class!
Thank you!
`}),e.add({id:9,href:"/docs/class-2/",title:"Class 2",description:"Class 2",content:""}),e.add({id:10,href:"/docs/class-2/computer-vision/",title:"Computer Vision",description:`Computer vision allows computers to \u0026ldquo;see\u0026rdquo;, and to understand what they are seeing. This is done by reading and interpreting digital images and video.
Operations # What are some common computer vision operations?
Image filtering Convert from one color space to another (e.g. RGB to grayscale). Adjust brightness and contrast. Blur or sharpen the image. Edge detection. Understanding Edge Detection (Sobel Operator) Background subtraction Detect moving objects by comparing them to a reference frame.`,content:`Computer vision allows computers to \u0026ldquo;see\u0026rdquo;, and to understand what they are seeing. This is done by reading and interpreting digital images and video.
Operations # What are some common computer vision operations?
Image filtering Convert from one color space to another (e.g. RGB to grayscale). Adjust brightness and contrast. Blur or sharpen the image. Edge detection. Understanding Edge Detection (Sobel Operator) Background subtraction Detect moving objects by comparing them to a reference frame. IHDC: Idiap Human Detection Code Object recognition Blob detection Contour finding OpenCV Contours \u0026amp; Convex Hull 2 Structure plugin Motion estimation Track pixel movement between consecutive frames and infer the direction objects are moving into. Dense Realtime Optical Flow on the GPU Face detection Feature recognition Smile detection Object Detection : Face Detection using Haar Cascade Classifiers Camera and projector calibration Procamcalib Box by Bot \u0026 Dolly Image Segmentation # One of the most common operations we will have to perform when working with computer vision is image segmentation. Image segmentation simply means dividing up the image pixels into meaningful groups. These meaningful groups depend on the application we are creating. For example, we may want to only consider the brightest pixels in an image, pixels of a certain color, clusters of pixels of a specific size, etc.
Shape-based hand recognition approach using the morphological pattern spectrum Image segmentation is the first step into many applications as it is a way to discard unwanted data and only keep what we need to focus on.
Video Capture # Let\u0026rsquo;s begin with a simple app to stream data from a connected webcam.
We will use an ofVideoGrabber to capture frames from video.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); } void ofApp::update() { grabber.update(); } void ofApp::draw() { grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); } We will use an ofImage to store our thresholded image. Let\u0026rsquo;s start with a stub procedure that just copies the video pixels into the image one at a time.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage resultImg; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); } void ofApp::update() { grabber.update(); ofPixels grabberPix = grabber.getPixels(); ofPixels resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); resultPix.setColor(x, y, pixColor); } } } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } âš ï¸ Why is the drawn image not updating?
An ofImage is made up of two parts: ofPixels, which is the data component of the image (on the CPU), and ofTexture, which is the graphics component of the image (on the GPU). For an image to get drawn to the screen, the data in ofPixels must be copied over to the ofTexture.
When manipulating pixels directly as we are doing, this process needs to be triggered manually by calling ofImage.update().
Pass by Reference vs. Pass by Value # You\u0026rsquo;ll notice that this still does not appear to be working even after adding the call to ofImage.update().
In C++, there are a few ways to pass data between objects and functions. You can pass data by reference, by value, or by pointer.
Pass by reference means that we are passing the actual data object itself. Any changes we make to the received object will be kept in the original reference, as it is the same object. Pass by value means that we are just passing the value of the data, not the data object itself. This usually means making a copy of the original object and passing that copy. This does not make a difference when the data is a number (like an int or a float) but it does matter when the data is an object as any changes we make to the received object are only occurring on this new copied object. Pass by pointer means that we are passing the memory address of the data object. We will look at this later on in the course. By default, data is passed by value in C++.
In our app, the line ofPixels resultPix = resultImg.getPixels(); creates a copy of the image pixels and stores it in resultPix. Any changes we make to resultPix are changes made on the copy and not on the ofPixels belonging to resultImg.
One way to resolve this would be to save back the modified pixels to resultImg at the end of the loop:
resultImg.setFromPixels(resultPix); Our code finally works, however it is highly unoptimized as we are now making two additional copies of our pixel array every frame.
A better approach is to pass the original pixels by reference using the \u0026amp; operator. The reference ofPixels from the ofImage are then modified directly. We can even go ahead and pass the grabber pixels by reference and avoid making a copy there too.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); } void ofApp::update() { grabber.update(); // Use a reference to the ofPixels in both the grabber and the image. ofPixels\u0026amp; grabberPix = grabber.getPixels(); ofPixels\u0026amp; resultPix = resultImg .getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); resultPix.setColor(x, y, pixColor); } } // Update the internal texture (GPU) with the new pixel data. resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } How would we set the result image to create a negative film effect?
A negative of a pixel of color is the inverse of that color. Because every pixel channel\u0026rsquo;s value has range 0-255, the inverse is the value substracted from 255.
negCol.r = 255 - pixCol.r; negCol.g = 255 - pixCol.g; negCol.b = 255 - pixCol.b; ofColor has an ofColor.invert() method which does this for us.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; // ... void ofApp::update() { grabber.update(); // Use a reference to the ofPixels in both the grabber and the image. ofPixels\u0026amp; grabberPix = grabber.getPixels(); ofPixels\u0026amp; resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); resultPix.setColor(x, y, pixColor.invert()); } } // Update the internal texture (GPU) with the new pixel data. resultImg.update(); } // ... Thresholding # Thresholding is a simple segmentation technique where a pixel value is either on or off. If it is on we will color it white and if it is off we will color it black. Thresholded images can be used as masks into our input image, used to discard any pixels we want to ignore.
night lights from zach lieberman on Vimeo. Let\u0026rsquo;s write a simple thresholding algorithm that only keeps the brightest parts of an image. We will use the brightness of each pixel color to determine if it should be on or off in our result image.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); } void ofApp::update() { grabber.update(); int brightnessThreshold = 128; ofPixels\u0026amp; grabberPix = grabber.getPixels(); ofPixels\u0026amp; resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); if (pixColor.getBrightness() \u0026gt; brightnessThreshold) { // Set the pixel white if its value is above the threshold. resultPix.setColor(x, y, ofColor(255)); } else { // Set the pixel black if its value is below the threshold. resultPix.setColor(x, y, ofColor(0)); } } } resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } We should make our threshold value editable, as we do not know what environment this app will run in.
Let\u0026rsquo;s make brightnessThreshold a class variable by moving the declaration to the header file. We can then use the mouse position to adjust the value in every update loop. We will use the ofMap() function to easily convert our mouse position (from 0 to the width of the window) to our brightness range (from 0 to 255).
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage resultImg; int brightnessThreshold; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); } void ofApp::update() { grabber.update(); brightnessThreshold = ofMap(mouseX, 0, ofGetWidth(), 255, 0); ofPixels\u0026amp; grabberPix = grabber.getPixels(); ofPixels\u0026amp; resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); if (pixColor.getBrightness() \u0026gt; brightnessThreshold) { // Set the pixel white if its value is above the threshold. resultPix.setColor(x, y, ofColor(255)); } else { // Set the pixel black if its value is below the threshold. resultPix.setColor(x, y, ofColor(0)); } } } resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } This is functional, but the way we are setting the threshold is a little clunky. We want better control and feedback for our threshold variable, and we can do this by replacing the mouse position by a GUI with a slider.
We will achieve this using two new elements: the ofxGui addon and the ofParameter template class.
ofxGui # The ofxGui addon ships with OF and is used for creating GUI elements.
\u0026ldquo;Addon\u0026rdquo; means it is not part of the OF core files. We need additional files in our project to use the addon. This can be complex if we do it manually, but thankfully we can select addons in the Project Generator and let it take care of the hard work.
When we regenerate our project files, we will now have access to all the ofxGui classes.
For this example, we will use ofxPanel, which is simply a container that can hold other GUI controls like buttons and sliders.
ofParameter # ofParameter is a wrapper class that is used to give other data types super powers. For example:
Min and max values can be defined and the value will always stay within that range. A notification gets triggered whenever the value is changed. This is especially useful for GUIs where we need to respond right away when a variable changes. ofParameter uses the template syntax, meaning that whatever type they hold is set between the \u0026lt; \u0026gt; symbols. In our case, since the threshold is an int, our ofParameter will be defined using ofParameter\u0026lt;int\u0026gt;.
âœŒï¸ What is a template?
C++ has a concept of templates. The idea with templates is to use types as parameters, similar to how we use values as parameters. If a class is templated, it can work with various data types without having to write the same code multiple times.
We will usually see classes or functions be templated.
In fact, we have already been using templates with ofPixels! The ofPixels type is actually a shorthand for ofPixels_\u0026lt;unsigned char\u0026gt;. This means it is an ofPixels_ template where the data type of the pixels is unsigned char (and that is why our values go from 0 to 255).
ofPixels_ can also be used with float and unsigned short pixels, using ofPixels_\u0026lt;float\u0026gt; or ofPixels_\u0026lt;unsigned short\u0026gt;. The shorthands ofFloatPixels and ofShortPixels are also available, and they represent exactly the same thing.
ofParameter works in a similar way, it is a template class. We need to specify the type and precision of the data it will control, in this case int. This is done when declaring the variable with type ofParameter\u0026lt;int\u0026gt;.
Our code now looks like the following, and our app window has a slider in the top-left corner we can use to edit the threshold value.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage resultImg; ofParameter\u0026lt;int\u0026gt; brightnessThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); // Initialize the threshold parameter with range [0, 255]. brightnessThreshold.set(\u0026quot;Bri Thresh\u0026quot;, 120, 0, 255); // Setup the GUI panel and add the threshold parameter. guiPanel.setup(\u0026quot;Threshold\u0026quot;); guiPanel.add(brightnessThreshold); } void ofApp::update() { grabber.update(); ofPixels\u0026amp; grabberPix = grabber.getPixels(); ofPixels\u0026amp; resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor pixColor = grabberPix.getColor(x, y); if (pixColor.getBrightness() \u0026gt; brightnessThreshold) { // Set the pixel white if its value is above the threshold. resultPix.setColor(x, y, ofColor(255)); } else { // Set the pixel black if its value is below the threshold. resultPix.setColor(x, y, ofColor(0)); } } } resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); guiPanel.draw(); } Background Subtraction # Background subtraction is a segmentation technique where the background pixels of an image are removed, leaving only the foreground data for processing.
âœŒï¸ What defines the background?
This varies depending on the type of sensor used, the environment, and the application.
When using a depth sensor, we can actually use a pixel\u0026rsquo;s distance from the camera to determine if it\u0026rsquo;s in the background or not. In general for 2D video, a background pixel is one that is considered stable, i.e. that does not change its value much or at all.
Background subtraction requires that we have a background frame as a reference. We can do this by saving a video frame in memory, and comparing future frames to it in our update() loop.
Video pixel values change slightly over time, so we cannot expect them to be identical frame by frame. We will add a threshold value and compare the difference between the pixels to determine if it should be on or off.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage backgroundImg; ofImage resultImg; ofParameter\u0026lt;bool\u0026gt; captureBackground; ofParameter\u0026lt;int\u0026gt; colorThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); resultImg.allocate(1280, 720, OF_IMAGE_COLOR); captureBackground.set(\u0026quot;Capture BG\u0026quot;, true); colorThreshold.set(\u0026quot;Color Thresh\u0026quot;, 120, 0, 255); guiPanel.setup(\u0026quot;BG Subtraction\u0026quot;); guiPanel.add(captureBackground); guiPanel.add(colorThreshold); } void ofApp::update() { grabber.update(); ofPixels\u0026amp; grabberPix = grabber.getPixels(); if (captureBackground) { backgroundImg.setFromPixels(grabber.getPixels()); captureBackground = false; } ofPixels\u0026amp; resultPix = resultImg.getPixels(); for (int y = 0; y \u0026lt; grabberPix.getHeight(); y++) { for (int x = 0; x \u0026lt; grabberPix.getWidth(); x++) { ofColor grabColor = grabberPix.getColor(x, y); ofColor bgColor = backgroundImg.getColor(x, y); if (abs(grabColor.r - bgColor.r) \u0026gt; colorThreshold || abs(grabColor.g - bgColor.g) \u0026gt; colorThreshold || abs(grabColor.b - bgColor.b) \u0026gt; colorThreshold) { resultPix.setColor(x, y, grabColor); } else { resultPix.setColor(x, y, ofColor(0)); } } } resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); guiPanel.draw(); } Devices # Color # While using built-in webcams is convenient for testing, it is not a great choice for deployed projects. They tend to be low quality and not offer manual controls for white balance, exposure, focus, etc.
Higher end webcams like the Logitech C9XX or Brio series are a good alternative. For best quality, a DSLR or video camera can be used with a capture card, like the Blackmagic UltraStudio Recorder 3G. Infrared # Depending on the application and environment, it might be better to use an alternative to a color camera for capturing images.
Infrared cameras are often used for sensing because they see light that is invisible to humans. They tend to be a more versatile choice as they can be used in a bright room, a pitch dark room, facing a video projector, behind a touch surface, etc.
presence [a.k.a soft \u0026amp; silky] from smallfly on Vimeo. Infrared USB cameras can be hard to come by.
One option is to get a depth sensor like an Intel RealSense. Most of these also include an IR light emitter, which means it will always have enough light to work properly. Another popular option is to \u0026ldquo;hack\u0026rdquo; regular color cameras by adding a piece of processed film in front of the lens (which acts as an IR filter). There are a few tutorials on Instructables for doing this. A popular device for this hack is the PS3 Eye camera. Finally, you can opt for security cameras. These tend to have emitters around the lens to ensure there is enough light for the sensor. However, the quality is not great and they usually do not have USB connectivity and will require an adapter. Thermographic cameras, commonly known as FLIR, can be an interesting option. These infrared cameras sense radiation/heat and represent it as a color map. This can be very useful for tracking humans or animals as they can easily be segmented from their surroundings.
FLIR Scout TK `}),e.add({id:11,href:"/docs/class-3/",title:"Class 3",description:"Class 3",content:""}),e.add({id:12,href:"/docs/class-3/intro-to-opencv/",title:"Intro to OpenCV",description:`OpenCV is an open-source library for performing computer vision operations.
OpenCV was originally released in 2000 and has gone through many updates and revisions since then. The library is cross-platform and available for all major platforms. It includes interfaces in C++, Java, and Python. OpenCV includes hundreds of algorithms, for performing a variety of tasks like image conversion, object tracking, feature recognition, camera calibration, etc. OpenCV uses its own image type, called cv::Mat.`,content:`OpenCV is an open-source library for performing computer vision operations.
OpenCV was originally released in 2000 and has gone through many updates and revisions since then. The library is cross-platform and available for all major platforms. It includes interfaces in C++, Java, and Python. OpenCV includes hundreds of algorithms, for performing a variety of tasks like image conversion, object tracking, feature recognition, camera calibration, etc. OpenCV uses its own image type, called cv::Mat. The word \u0026ldquo;Mat\u0026rdquo; is short for matrix, which is what we call a multi-dimensional array of values in mathematics. cv::Mat is similar to ofPixels, as it holds an array of pixel values, but it is also much more powerful as it can perform all types of operations on matrices. For example, we can add or multiply two cv::Mat objects directly, without needing to loop through the pixels one at a time.
âœŒï¸ What does the :: mean in this context?
We have already covered that :: is a scope resolution operator in C++. We first encountered :: to show the relationship between methods and classes, for example ofApp::setup() means that the method setup() is part of ofApp.
In the case of cv::Mat, the :: is used to show the relationship between classes and namespaces.
A namespace is a top-level group that holds classes that are related to each other. This is similar to how programs are organized using packages in JavaScript, Java, or Python. In this case, cv::Mat is a reference to the Mat class that belongs to the cv namespace.
Classes in the same namespace can refer to each other directly, but classes outside of that namespace need to specify the namespace using the :: notation to refer to its classes.
Most classes in OF do not belong to a namespace, which is why we have not seen this yet, but this is gradually changing. New additions to the framework, like the glm math library are keeping their namespace visible, so we will encounter types like glm::vec2 and glm::vec3 when manipulating vectors.
OpenCV for openFrameworks # While you can use the OpenCV library directly in OF (since both are written in C++), this would require image type conversions every time we need to move data from one framework to the other. For example, to convert our background subtraction algorithm we would:
Capture a frame from the camera in \u0026ldquo;OF space\u0026rdquo;, using ofVideoGrabber and ofPixels. Convert the ofPixels to a cv::Mat to use the pixels in \u0026ldquo;OpenCV space\u0026rdquo;. Perform the background subtraction using OpenCV functions. Convert the result cv::Mat back to an ofImage to draw it to the screen. This would also apply to other types which we might use. For example:
Points (cv::Point vs glm::vec2) Rectangles (cv::Rect vs ofRectangle) We will use an openFrameworks addon to interface with OpenCV. This will take care of handling these conversions and give us additional OF-specific methods we can use.
There are two options for addons:
ofxOpenCv is the built-in OpenCV addon. This wrapper hides most of the native OpenCV structures and methods and allows us to work strictly in \u0026ldquo;OF space\u0026rdquo;. While this simplifies our work, we are limited by the data types and algorithms that are included in the addon. ofxCv is a user-contributed addon that takes a more transparent approach. Interchange between OF and CV is facilitated with helper functions, but the majority of the work is done using native OpenCV calls. While this may seem more complicated, it reduces our dependency to OF as we are learning how to use OpenCV directly. We will use ofxCv for this class. As it is a user-contributed addon, we will need to download it and unzip it in the OF addons directory at /path/to/OF/addons/. Once that is done, the Project Generator will automatically (after a restart) detect the addon and allow us to include it in our projects.
ofxCv uses the OpenCV files from ofxOpenCv, so make sure to include both addons in your projects.
Background Subtraction # Let\u0026rsquo;s write a new background subtraction example using OpenCV. We will focus on pixel brightness for our operations, so we will convert our images from RGB color to grayscale before processing.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage backgroundImg; ofImage resultImg; ofParameter\u0026lt;bool\u0026gt; captureBackground; ofParameter\u0026lt;int\u0026gt; briThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); captureBackground.set(\u0026quot;Capture BG\u0026quot;, true); briThreshold.set(\u0026quot;Bri Thresh\u0026quot;, 120, 0, 255); guiPanel.setup(\u0026quot;BG Subtraction\u0026quot;); guiPanel.add(captureBackground); guiPanel.add(briThreshold); } void ofApp::update() { grabber.update(); ofImage grabberColorImg; grabberColorImg.setFromPixels(grabber.getPixels()); // Convert input image to grayscale. ofImage grabberGrayImg; ofxCv::copyGray(grabberColorImg, grabberGrayImg); if (captureBackground) { // Copy input image to background. backgroundImg = grabberGrayImg; captureBackground = false; } // Compute the difference image between the background and grabber. ofxCv::absdiff(backgroundImg, grabberGrayImg, resultImg); // Threshold the difference image. ofxCv::threshold(resultImg, briThreshold); // Update the image to draw it. resultImg.update(); } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); guiPanel.draw(); } Note the use of the ofxCv::absdiff() and ofxCv::threshold() functions for processing all our pixels in a single line of code. These are wrappers for cv::absdiff() and cv::threshold() allowing us to use ofImage objects in \u0026ldquo;OpenCV\u0026rdquo; space. ofxCv handles all the necessary conversions behind the scenes.
We could also skip some of the conversions between \u0026ldquo;OF space\u0026rdquo; and \u0026ldquo;OpenCV\u0026rdquo; space to optimize our code. This can be useful for more complex apps that need better performance. Let\u0026rsquo;s rewrite the previous example using cv objects for our CV operations, i.e. replace ofImage with cv::Mat.
We will also make two additional optimizations:
Cache the variables used for grabber images so that they don\u0026rsquo;t get reallocated every frame. We do this by adding two new variables grabberColorMat and grabberGrayMat to the ofApp class. Only run through our algorithm when a new video frame is captured. This is an important step as the video camera will usually run at much lower framerate than the application itself. This is achieved by checking ofVideoGrabber.isFrameNew() before running through our CV code. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; cv::Mat grabberColorMat; cv::Mat grabberGrayMat; cv::Mat backgroundMat; cv::Mat resultMat; ofImage resultImg; ofParameter\u0026lt;bool\u0026gt; captureBackground; ofParameter\u0026lt;int\u0026gt; briThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { grabber.setup(1280, 720); captureBackground.set(\u0026quot;Capture BG\u0026quot;, true); briThreshold.set(\u0026quot;Bri Thresh\u0026quot;, 120, 0, 255); guiPanel.setup(\u0026quot;BG Subtraction\u0026quot;); guiPanel.add(captureBackground); guiPanel.add(briThreshold); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { // Convert the grabber image to CV space. grabberColorMat = ofxCv::toCv(grabber.getPixels()); // Convert input image to grayscale. ofxCv::copyGray(grabberColorMat, grabberGrayMat); if (captureBackground) { // Copy input image to background. // Note that the = operator copies cv::Mat by reference, // but we need an actual copy here. This is why we use // the cv::Mat.clone() method. backgroundMat = grabberGrayMat.clone(); captureBackground = false; } // Compute the difference image between the background and grabber. cv::absdiff(backgroundMat, grabberGrayMat, resultMat); // Threshold the difference image. ofxCv::threshold(resultMat, briThreshold); // Convert the result CV image back to OF space. ofxCv::toOf(resultMat, resultImg); // Update the image to draw it. resultImg.update(); } } void ofApp::draw() { resultImg.draw(0, 0, ofGetWidth(), ofGetHeight()); guiPanel.draw(); } Face Detection # OpenCV includes a powerful object detection algorithm which is often used for finding faces in images, using something called Haar cascade classifiers.
This is a machine learning algorithm where a classifier is trained on many sample images, both positive (with faces) and negative (without faces). It uses this to generate a model, which it can then use to detect faces in new images by looking for similar patterns.
The features are in the shape of black and white patterns (Haar features), which are searched for in an image. If the patterns are arranged in a recognizable way, we have a match!
Haar Features Let\u0026rsquo;s have a look at the face example that ships with ofxCv. Note that it does not come with the Haar cascade file, you\u0026rsquo;ll need to add it yourself to the OF/addons/ofxCv/example-face/bin/data/ folder. You can get the face file in the OF/examples/computer_vision/opencvHaarFinderExample/bin/data/ folder, or you can try some of the other ones from the OpenCV repository.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber cam; ofxCv::ObjectFinder finder; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; using namespace ofxCv; using namespace cv; void ofApp::setup() { ofSetVerticalSync(true); ofSetFrameRate(120); finder.setup(\u0026quot;haarcascade_frontalface_default.xml\u0026quot;); finder.setPreset(ObjectFinder::Fast); cam.setup(640, 480); } void ofApp::update() { cam.update(); if (cam.isFrameNew()) { finder.update(cam); } } void ofApp::draw() { cam.draw(0, 0); finder.draw(); ofDrawBitmapStringHighlight(ofToString(finder.size()), 10, 20); } âœŒï¸ What does using namespace do?
A using directive can be used in C++ to indicate that classes and methods from the specified namespace will be used in the file.
When adding using namespace ofxCv; at the top of the file, we can use classes from ofxCv without having to prefix them with ofxCv::. This can be seen with ObjectFinder::Fast in the code above, which if fully defined is ofxCv::ObjectFinder::Fast.
Some people prefer using directives because it makes the code more concise. Others prefer being explicit and referring the namespace throughout. Both options are fine, it\u0026rsquo;s a matter of style.
Sharing Faces from Kyle McDonald on Vimeo. `}),e.add({id:13,href:"/docs/class-3/object-tracking/",title:"Object Tracking",description:"Object Tracking",content:`Let\u0026rsquo;s build an app together that tracks an object in the frame over time.
We are going to track a red playing card and can assume that the card will be the \u0026ldquo;most red\u0026rdquo; element in the frame.
Contour Finding # In order to do this, we will use a Contour Finding algorithm.
Contour finding consists of identifying regions of images matching a particular pattern. These patterns can be defined by color, size, shape, speed, etc. Contour finding can be used to follow objects or people in an image.
BloodBank - Video game from Vincent de Vevey on Vimeo. If we break it down, this usually consist of many steps:
Convert the pixel data to an appropriate color space. Threshold the image based on a target color and offset. As we saw previously, video pixel colors vary over frames, so we need to use an offset to look for a color range. Run the OpenCV contour finding operation on the image. Filter the array of contours and only keep the ones that match the requested parameters. This is where ofxCv comes in very handy, as we can use the ofxCv::ContourFinder class to handle a big part of the work.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { // Update parameters. contourFinder.setTargetColor(colorTarget); contourFinder.setThreshold(colorOffset); // Find contours. contourFinder.findContours(grabber); } } void ofApp::draw() { ofSetColor(255); // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); // Draw the found contours. contourFinder.draw(); // Draw the gui. guiPanel.draw(); } Color Space # While this application technically works, it is hard to get the settings right. Let\u0026rsquo;s modify it to make it easier to use.
We often get better results when comparing colors in HSV rather than RGB space.
RGB defines how much red, green, and blue is in an image. A small change in values might result in a greater difference seen, and vice-versa. When working near the grayscale range (white to black), it is hard to evaluate how much red, green, and blue is actually in the pixel. HSV defines colors as levels of hue, saturation, and brightness. This is closer to how humans perceive color, and differentiate objects they see in space. It is easier to isolate parameters. We will often just care about the brightness of an image, particularly in the grayscale range. It is a more logical set of parameters for many image analysis algorithms. We can tell ofxCv::ContourFinder to use HSV by passing a second parameter to the setTargetColor() method:
contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); Mouse Selector # We can use the mouse to interactively select the color under the cursor. This will remove any guesswork from setting the target color accurately.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void mousePressed(int x, int y, int button); ofVideoGrabber grabber; ofImage processImg; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofColor colorUnderMouse; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { processImg.setFromPixels(grabber.getPixels()); // Save the color of the pixel under the mouse. colorUnderMouse = processImg.getColor(ofGetMouseX(), ofGetMouseY()); // Update parameters. contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); contourFinder.setThreshold(colorOffset); // Find contours. contourFinder.findContours(processImg); } } void ofApp::draw() { ofSetColor(255); // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); // Draw the found contours. contourFinder.draw(); // Draw the color under the mouse. ofPushStyle(); ofSetColor(colorUnderMouse); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofNoFill(); ofSetColor(colorUnderMouse.getInverted()); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofPopStyle(); // Draw the gui. guiPanel.draw(); } void ofApp::mousePressed(int x, int y, int button) { if (!guiPanel.getShape().inside(x, y)) { // Track the color under the mouse. colorTarget = colorUnderMouse; } } Area Range # The contour finder seems to be working too well. We are getting many blobs, and most are too big or too small to consider.
We can limit the range of blob sizes to look for, and only match the ones within this range.
ofxCv::ContourFinder gives us a few different options for this. In all cases, we are looking at the area of the blob bounding box. The difference is simply in how this gets calculated.
setMinArea() / setMaxArea() set the range in pixels. The max range depends on the size of the image as a larger image will have more pixel area. e.g. If a blob\u0026rsquo;s dimensions are 20px width by 10px height, the area is w x h = 200px2. setMinAreaRadius() / setMaxAreaRadius() set the area using the blob radius. This can be useful if the blobs we are looking for are circular in shape. setMinAreaNorm() / setMaxAreaNorm() set the area using normalized coordinates. This means the range is always between 0.0 and 1.0, no matter what the image size is. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void mousePressed(int x, int y, int button); ofVideoGrabber grabber; ofImage processImg; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofColor colorUnderMouse; ofParameter\u0026lt;float\u0026gt; minArea; ofParameter\u0026lt;float\u0026gt; maxArea; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); minArea.set(\u0026quot;Min Area\u0026quot;, 0.01f, 0, 0.5f); maxArea.set(\u0026quot;Max Area\u0026quot;, 0.05f, 0, 0.5f); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); guiPanel.add(minArea); guiPanel.add(maxArea); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { processImg.setFromPixels(grabber.getPixels()); // Save the color of the pixel under the mouse. colorUnderMouse = processImg.getColor(ofGetMouseX(), ofGetMouseY()); // Update parameters. contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); contourFinder.setThreshold(colorOffset); contourFinder.setMinAreaNorm(minArea); contourFinder.setMaxAreaNorm(maxArea); // Find contours. contourFinder.findContours(processImg); } } void ofApp::draw() { ofSetColor(255); // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); // Draw the found contours. contourFinder.draw(); // Draw the color under the mouse. ofPushStyle(); ofSetColor(colorUnderMouse); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofNoFill(); ofSetColor(colorUnderMouse.getInverted()); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofPopStyle(); // Draw the gui. guiPanel.draw(); } void ofApp::mousePressed(int x, int y, int button) { if (!guiPanel.getShape().inside(x, y)) { // Track the color under the mouse. colorTarget = colorUnderMouse; } } Royal Opera House - Audience: \u0026quot;chatting and following\u0026quot; from RANDOM INTERNATIONAL on Vimeo. Filtering # The pattern on the back of the card is making it hard to get a clean blob. It is getting separated into different parts. Let\u0026rsquo;s try to filter the image before sending it to the contour finder to get better results.
The following operations are called convolution operations. A convolution is a process by which a pixel looks at its neighbours values to calculate its own value. The rules to calculate this value are set in a kernel.
A kernel has a size and weights.
The size, also called the window, specifies how many neighbours to look at when making the calculation. For example, a 3x3 kernel will consider the 8 direct neighbours and the pixel itself. The weights represent how much of each pixel in the kernel to take in when calculating the final value. A normalized kernel will have the same weights throughout, while a non-normalized one will have different weight values. The window is usually odd and square, also called box, (e.g. 3x3 or 5x5) and the pixel in question is in the middle of it. [ 1 4 6 4 1 ] [ 1 1 1 ] [ 1 2 1 ] [ 4 16 24 16 4 ] [ 1 1 1 ] [ 2 4 2 ] [ 6 24 36 24 6 ] [ 1 1 1 ] [ 1 2 1 ] [ 4 16 24 16 4 ] [ 1 4 6 4 1 ] Blur # Blurring the image is a good first step to smooth out unwanted details in an image and make it more homogenous.
ofxCv offers a few options for blurring:
cv::blur() uses a normalized kernel. cv::GaussianBlur() uses a weighted Gaussian kernel. cv::medianBlur() uses the median value of the neighbours in the kernel. For this application, it makes the most sense to use the median blur, as that will allow the larger red part of the card to overtake the smaller white parts.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void mousePressed(int x, int y, int button); ofVideoGrabber grabber; ofImage processImg; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofColor colorUnderMouse; ofParameter\u0026lt;float\u0026gt; minArea; ofParameter\u0026lt;float\u0026gt; maxArea; ofParameter\u0026lt;int\u0026gt; blurAmount; ofParameter\u0026lt;bool\u0026gt; debugProcess; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); minArea.set(\u0026quot;Min Area\u0026quot;, 0.01f, 0, 0.5f); maxArea.set(\u0026quot;Max Area\u0026quot;, 0.05f, 0, 0.5f); blurAmount.set(\u0026quot;Blur Amount\u0026quot;, 0, 0, 100); debugProcess.set(\u0026quot;Debug Process\u0026quot;, false); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); guiPanel.add(minArea); guiPanel.add(maxArea); guiPanel.add(blurAmount); guiPanel.add(debugProcess); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { processImg.setFromPixels(grabber.getPixels()); // Filter the image. if (blurAmount \u0026gt; 0) { //ofxCv::blur(processImg, blurAmount); //ofxCv::GaussianBlur(processImg, blurAmount); ofxCv::medianBlur(processImg, blurAmount); processImg.update(); } // Save the color of the pixel under the mouse. colorUnderMouse = processImg.getColor(ofGetMouseX(), ofGetMouseY()); // Update parameters. contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); contourFinder.setThreshold(colorOffset); contourFinder.setMinAreaNorm(minArea); contourFinder.setMaxAreaNorm(maxArea); // Find contours. contourFinder.findContours(processImg); } } void ofApp::draw() { ofSetColor(255); if (debugProcess) { // Draw the process image. processImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } else { // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); } // Draw the found contours. contourFinder.draw(); // Draw the color under the mouse. ofPushStyle(); ofSetColor(colorUnderMouse); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofNoFill(); ofSetColor(colorUnderMouse.getInverted()); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofPopStyle(); // Draw the gui. guiPanel.draw(); } void ofApp::mousePressed(int x, int y, int button) { if (!guiPanel.getShape().inside(x, y)) { // Track the color under the mouse. colorTarget = colorUnderMouse; } } Dilate / Erode # Dilation and erosion are morphology based operations, meaning that they are based on shapes in the image. These tend to be used to remove noise, or to find features in an image like holes.
Dilation and erosion are counterparts of each other.
dilate() grows the bright regions of the image to take up more pixels. Eroding and Dilating erode() grows the dark regions of the image to take up more pixels. Eroding and Dilating For this application, we should use erosion as we want the darker red parts of the card to overtake the brighter white parts.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void mousePressed(int x, int y, int button); ofVideoGrabber grabber; ofImage processImg; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofColor colorUnderMouse; ofParameter\u0026lt;float\u0026gt; minArea; ofParameter\u0026lt;float\u0026gt; maxArea; ofParameter\u0026lt;int\u0026gt; blurAmount; ofParameter\u0026lt;int\u0026gt; erodeIterations; ofParameter\u0026lt;bool\u0026gt; debugProcess; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); minArea.set(\u0026quot;Min Area\u0026quot;, 0.01f, 0, 0.5f); maxArea.set(\u0026quot;Max Area\u0026quot;, 0.05f, 0, 0.5f); blurAmount.set(\u0026quot;Blur Amount\u0026quot;, 0, 0, 100); erodeIterations.set(\u0026quot;Erode Iterations\u0026quot;, 0, 0, 10); debugProcess.set(\u0026quot;Debug Process\u0026quot;, false); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); guiPanel.add(minArea); guiPanel.add(maxArea); guiPanel.add(blurAmount); guiPanel.add(erodeIterations); guiPanel.add(debugProcess); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { processImg.setFromPixels(grabber.getPixels()); // Filter the image. if (blurAmount \u0026gt; 0) { //ofxCv::blur(processImg, blurAmount); //ofxCv::GaussianBlur(processImg, blurAmount); ofxCv::medianBlur(processImg, blurAmount); } if (erodeIterations \u0026gt; 0) { ofxCv::erode(processImg, erodeIterations.get()); } processImg.update(); // Save the color of the pixel under the mouse. colorUnderMouse = processImg.getColor(ofGetMouseX(), ofGetMouseY()); // Update parameters. contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); contourFinder.setThreshold(colorOffset); contourFinder.setMinAreaNorm(minArea); contourFinder.setMaxAreaNorm(maxArea); // Find contours. contourFinder.findContours(processImg); } } void ofApp::draw() { ofSetColor(255); if (debugProcess) { // Draw the process image. processImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } else { // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); } // Draw the found contours. contourFinder.draw(); // Draw the color under the mouse. ofPushStyle(); ofSetColor(colorUnderMouse); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofNoFill(); ofSetColor(colorUnderMouse.getInverted()); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofPopStyle(); // Draw the gui. guiPanel.draw(); } void ofApp::mousePressed(int x, int y, int button) { if (!guiPanel.getShape().inside(x, y)) { // Track the color under the mouse. colorTarget = colorUnderMouse; } } Tracking # Blob tracking means to follow and track objects over time. Instead of interpreting each frame as a unique \u0026ldquo;event\u0026rdquo;, we can compare the current blobs detected to previous ones and see if there are correspondences. By tracking blobs over time, we can assign them parameters like an age, a direction of movement, a velocity, and use those parameters to build rich interactive applications.
collimation from Brad Todd on Vimeo. Tracking is not part of the OpenCV library but it is such a common operation to perform post contour finding that a set of tracking functions are included with ofxCv.
ofxCv::Tracker is a powerful blob tracker that can be used on its own or with ofxCv::ContourFinder. In fact, ofxCv::ContourFinder has a tracker embedded in it which can be accessed using ofxCv::ContourFinder.getTracker().
Tracked blobs are identified using a label. If two blobs from different frames have the same label, then they are considered the same.
The tracker takes in parameters to set how it operates.
Persistence is the amount of time a blob can disappear before it is actually considered dead. This is set in frames. If it is set to 0 then a blob will stop being tracked as soon as it disappears. If it is set higher, for example to 15, then a blob can disappear for up to 15 frames and then reappear and keep its previous label. This can be helpful if the video feed is not stable and blobs seem to flicker on and off. Max distance is the maximum distance a blob can travel between two frames. This is set in pixels. If a blob is less than the max distance away from a previous blob, it is considered one and the same and keeps its label. If a blob is further away than max distance from any previous blobs, it is considered new and gets a new label. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); void mousePressed(int x, int y, int button); ofVideoGrabber grabber; ofImage processImg; ofxCv::ContourFinder contourFinder; ofParameter\u0026lt;ofColor\u0026gt; colorTarget; ofParameter\u0026lt;int\u0026gt; colorOffset; ofColor colorUnderMouse; ofParameter\u0026lt;float\u0026gt; minArea; ofParameter\u0026lt;float\u0026gt; maxArea; ofParameter\u0026lt;int\u0026gt; blurAmount; ofParameter\u0026lt;int\u0026gt; erodeIterations; ofParameter\u0026lt;int\u0026gt; persistence; ofParameter\u0026lt;float\u0026gt; maxDistance; ofParameter\u0026lt;bool\u0026gt; showLabels; ofParameter\u0026lt;bool\u0026gt; debugProcess; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640, 480); // Setup the grabber. grabber.setup(640, 480); // Setup the contour finder and parameters. contourFinder.setUseTargetColor(true); colorTarget.set(\u0026quot;Color Target\u0026quot;, ofColor(255, 0, 0)); colorOffset.set(\u0026quot;Color Offset\u0026quot;, 10, 0, 255); minArea.set(\u0026quot;Min Area\u0026quot;, 0.01f, 0, 0.5f); maxArea.set(\u0026quot;Max Area\u0026quot;, 0.05f, 0, 0.5f); blurAmount.set(\u0026quot;Blur Amount\u0026quot;, 0, 0, 100); erodeIterations.set(\u0026quot;Erode Iterations\u0026quot;, 0, 0, 10); persistence.set(\u0026quot;Persistence\u0026quot;, 15, 0, 60); maxDistance.set(\u0026quot;Max Distance\u0026quot;, 64, 0, 640); showLabels.set(\u0026quot;Show Labels\u0026quot;, false); debugProcess.set(\u0026quot;Debug Process\u0026quot;, false); // Setup the gui. guiPanel.setup(\u0026quot;Color Tracker\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(colorTarget); guiPanel.add(colorOffset); guiPanel.add(minArea); guiPanel.add(maxArea); guiPanel.add(blurAmount); guiPanel.add(erodeIterations); guiPanel.add(persistence); guiPanel.add(maxDistance); guiPanel.add(showLabels); guiPanel.add(debugProcess); } void ofApp::update() { grabber.update(); if (grabber.isFrameNew()) { processImg.setFromPixels(grabber.getPixels()); // Filter the image. if (blurAmount \u0026gt; 0) { //ofxCv::blur(processImg, blurAmount); //ofxCv::GaussianBlur(processImg, blurAmount); ofxCv::medianBlur(processImg, blurAmount); } if (erodeIterations \u0026gt; 0) { ofxCv::erode(processImg, erodeIterations.get()); } processImg.update(); // Save the color of the pixel under the mouse. colorUnderMouse = processImg.getColor(ofGetMouseX(), ofGetMouseY()); // Update parameters. contourFinder.setTargetColor(colorTarget, ofxCv::TRACK_COLOR_HSV); contourFinder.setThreshold(colorOffset); contourFinder.setMinAreaNorm(minArea); contourFinder.setMaxAreaNorm(maxArea); contourFinder.getTracker().setPersistence(persistence); contourFinder.getTracker().setMaximumDistance(maxDistance); // Find contours. contourFinder.findContours(processImg); } } void ofApp::draw() { ofSetColor(255); if (debugProcess) { // Draw the process image. processImg.draw(0, 0, ofGetWidth(), ofGetHeight()); } else { // Draw the grabber image. grabber.draw(0, 0, ofGetWidth(), ofGetHeight()); } // Draw the found contours. contourFinder.draw(); if (showLabels) { ofxCv::RectTracker\u0026amp; tracker = contourFinder.getTracker(); ofSetColor(255); for (int i = 0; i \u0026lt; contourFinder.size(); i++) { ofPoint center = ofxCv::toOf(contourFinder.getCenter(i)); int label = contourFinder.getLabel(i); string msg = ofToString(label) + \u0026quot;:\u0026quot; + ofToString(tracker.getAge(label)); ofDrawBitmapString(msg, center.x, center.y); ofVec2f velocity = ofxCv::toOf(contourFinder.getVelocity(i)); ofDrawLine(center.x, center.y, center.x + velocity.x, center.y + velocity.y); } } // Draw the color under the mouse. ofPushStyle(); ofSetColor(colorUnderMouse); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofNoFill(); ofSetColor(colorUnderMouse.getInverted()); ofDrawRectangle(ofGetMouseX() - 25, ofGetMouseY() - 25, 50, 50); ofPopStyle(); // Draw the gui. guiPanel.draw(); } void ofApp::mousePressed(int x, int y, int button) { if (!guiPanel.getShape().inside(x, y)) { // Track the color under the mouse. colorTarget = colorUnderMouse; } } `}),e.add({id:14,href:"/docs/class-4/",title:"Class 4",description:"Class 4",content:""}),e.add({id:15,href:"/docs/class-4/logging/",title:"Logging",description:`openFrameworks has an advanced logging system that can be useful for reporting and debugging your applications.
Log Level # Log messages have a \u0026ldquo;severity\u0026rdquo; level:
OF_LOG_VERBOSE: For extra information (TMI) OF_LOG_NOTICE: For normal reporting, like status updates, state changes, etc. OF_LOG_WARNING: For minor errors that can be ignored, like an image that is not the dimensions you expected. OF_LOG_ERROR: For major errors that should be handled, like an image that fails to load because the file does not exist.`,content:`openFrameworks has an advanced logging system that can be useful for reporting and debugging your applications.
Log Level # Log messages have a \u0026ldquo;severity\u0026rdquo; level:
OF_LOG_VERBOSE: For extra information (TMI) OF_LOG_NOTICE: For normal reporting, like status updates, state changes, etc. OF_LOG_WARNING: For minor errors that can be ignored, like an image that is not the dimensions you expected. OF_LOG_ERROR: For major errors that should be handled, like an image that fails to load because the file does not exist. OF_LOG_FATAL_ERROR: For showstopper errors, like not finding a camera for a vision based app. Messages are output to the console using the ofLog(...) method and passing a log level.
ofLog(OF_LOG_NOTICE) \u0026lt;\u0026lt; \u0026quot;Hey this is a message!\u0026quot;; Note the use of the left-shift operator \u0026lt;\u0026lt;. This is similar to how we have been logging messages using cout up to now.
This special \u0026lt;\u0026lt; operator tends to be used when we are \u0026ldquo;pushing\u0026rdquo; or \u0026ldquo;adding\u0026rdquo; something to something else. In this case, we are pushing string objects to the logger, which writes them all out in a single line.
The \u0026ldquo;something\u0026rdquo; we are pushing must either be a string or an object that the compiler knows how to convert to a string. It already knows how to handle numbers (int and float) and simple objects (ofPoint and ofRectangle) but for anything custom or more complex, we will need to break that out ourselves.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofLog(OF_LOG_VERBOSE) \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLog(OF_LOG_WARNING) \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLog(OF_LOG_NOTICE) \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLog(OF_LOG_ERROR) \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLog(OF_LOG_FATAL_ERROR) \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } Notice that all messages are output to the console except the first VERBOSE message.
[ error ] ofImage: loadImage(): couldn't load image from \u0026quot;\u0026quot;dog-grass.jpg\u0026quot;\u0026quot; [ error ] Unable to load dog image, make sure it's in the data folder! [ fatal ] Unable to open camera, there's no reason to keep going :( This is because the default log level is OF_LOG_NOTICE. Any log messages using a level above this will not be printed. This can be changed with a call to ofSetLogLevel(). The parameter is any of the log levels listed above, or OF_LOG_SILENT which will disable all logging no matter what the severity is.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //ofSetLogLevel(OF_LOG_VERBOSE); //ofSetLogLevel(OF_LOG_NOTICE); // default ofSetLogLevel(OF_LOG_WARNING); //ofSetLogLevel(OF_LOG_ERROR); //ofSetLogLevel(OF_LOG_FATAL_ERROR); //ofSetLogLevel(OF_LOG_SILENT); ofLog(OF_LOG_VERBOSE) \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLog(OF_LOG_WARNING) \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLog(OF_LOG_NOTICE) \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLog(OF_LOG_ERROR) \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLog(OF_LOG_FATAL_ERROR) \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } We would change the log level depending on if we are debugging our app, running it for a demo, or preparing a build for release. Instead of having to go through all our code and commenting or deleting all the log lines, a simple call to ofSetLogLevel() can set the appropriate level of console logging.
Log Modules # The ofLog() methods can be swapped out for their ofLogXXX() counterpart. So ofLog(OF_LOG_NOTICE) becomes ofLogNotice(), ofLog(OF_LOG_ERROR) becomes ofLogError() and so on, which is a little less verbose.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //ofSetLogLevel(OF_LOG_VERBOSE); //ofSetLogLevel(OF_LOG_NOTICE); // default ofSetLogLevel(OF_LOG_WARNING); //ofSetLogLevel(OF_LOG_ERROR); //ofSetLogLevel(OF_LOG_FATAL_ERROR); //ofSetLogLevel(OF_LOG_SILENT); ofLogVerbose() \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLogWarning() \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLogNotice() \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLogError() \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLogFatalError() \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } Another advantage of this version is that we can add an optional module as a parameter to the function. The module is just a string that can represent the part / section of the code we are dealing with.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //ofSetLogLevel(OF_LOG_VERBOSE); //ofSetLogLevel(OF_LOG_NOTICE); // default ofSetLogLevel(OF_LOG_WARNING); //ofSetLogLevel(OF_LOG_ERROR); //ofSetLogLevel(OF_LOG_FATAL_ERROR); //ofSetLogLevel(OF_LOG_SILENT); ofLogVerbose(\u0026quot;App\u0026quot;) \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLogWarning(\u0026quot;Image Load\u0026quot;) \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLogNotice(\u0026quot;Image Load\u0026quot;) \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLogError(\u0026quot;Image Load\u0026quot;) \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLogFatalError(\u0026quot;Grabber Init\u0026quot;) \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } The module will prepend your log message in the output.
[ error ] ofImage: loadImage(): couldn't load image from \u0026quot;\u0026quot;dog-grass.jpg\u0026quot;\u0026quot; [ error ] Image Load: Unable to load dog image, make sure it's in the data folder! [ fatal ] Grabber Setup: Unable to open camera, there's no reason to keep going :( Instead of coming up with a module name every time, we can also use the __FUNCTION__ macro, which will automatically get replaced by the full method name during compilation.
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //ofSetLogLevel(OF_LOG_VERBOSE); //ofSetLogLevel(OF_LOG_NOTICE); // default ofSetLogLevel(OF_LOG_WARNING); //ofSetLogLevel(OF_LOG_ERROR); //ofSetLogLevel(OF_LOG_FATAL_ERROR); //ofSetLogLevel(OF_LOG_SILENT); ofLogVerbose(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLogWarning(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLogNotice(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLogError(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLogFatalError(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } [ error ] ofImage: loadImage(): couldn't load image from \u0026quot;\u0026quot;dog-grass.jpg\u0026quot;\u0026quot; [ error ] ofApp::setup: Unable to load dog image, make sure it's in the data folder! [ fatal ] ofApp::setup: Unable to open camera, there's no reason to keep going :( Log Channels # ofLog() does not necessarily need to log to the console. It might make more sense to log to a file that can be examined later, especially in the context of an installation going live. This can be done using the function ofLogToFile().
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //ofSetLogLevel(OF_LOG_VERBOSE); //ofSetLogLevel(OF_LOG_NOTICE); // default ofSetLogLevel(OF_LOG_WARNING); //ofSetLogLevel(OF_LOG_ERROR); //ofSetLogLevel(OF_LOG_FATAL_ERROR); //ofSetLogLevel(OF_LOG_SILENT); ofLogToFile(\u0026quot;log-\u0026quot; + ofGetTimestampString() + \u0026quot;.txt\u0026quot;, true); ofLogVerbose(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;The app is now starting!\u0026quot;; if (image.load(\u0026quot;dog-grass.jpg\u0026quot;)) { float imageRatio = image.getWidth() / image.getHeight(); if (imageRatio != 16.0 / 9.0) { ofLogWarning(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Dog image has the wrong aspect ratio \u0026quot; \u0026lt;\u0026lt; imageRatio \u0026lt;\u0026lt; \u0026quot;, it will be stretched!\u0026quot;; } else { ofLogNotice(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Dog image loaded successfully with dimensions \u0026quot; \u0026lt;\u0026lt; image.getWidth() \u0026lt;\u0026lt; \u0026quot;x\u0026quot; \u0026lt;\u0026lt; image.getHeight() \u0026lt;\u0026lt; \u0026quot;.\u0026quot;; } } else { ofLogError(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Unable to load dog image, make sure it's in the data folder!\u0026quot;; } if (!grabber.setup(1280, 720)) { ofLogFatalError(__FUNCTION__) \u0026lt;\u0026lt; \u0026quot;Unable to open camera, there's no reason to keep going :(\u0026quot;; } } We can switch back to console logging with ofLogToConsole().
`}),e.add({id:16,href:"/docs/class-4/depth-sensing/",title:"Depth Sensing",description:`Most of the apps we have built up to now are using video as input. Video is two-dimensional, it has a width and a height.
Depth sensors are special types of cameras that add a third dimension to the mix; they can see width, height, and depth. Depth in this context means the distance away from the camera where a captured pixel is situated.
It is important to note the difference between full 3D and depth.`,content:`Most of the apps we have built up to now are using video as input. Video is two-dimensional, it has a width and a height.
Depth sensors are special types of cameras that add a third dimension to the mix; they can see width, height, and depth. Depth in this context means the distance away from the camera where a captured pixel is situated.
It is important to note the difference between full 3D and depth. Just like a conventional 2D camera, a depth sensor can only capture what it \u0026ldquo;sees\u0026rdquo;. If an object is obstructing another one behind it, the camera will only be able to measure the depth of the one closest to it. We can only have one depth measurement per pixel.
Metal Pin Art Pin Point Impression 3D Frame Toy MegaFaces: Kinetic Facade Shows Giant 3D \u0026#039;Selfies\u0026#039; from iart on Vimeo. Technologies # There are a few different technologies used for depth sensing, each with their own pros and cons depending on the application.
Most depth cameras include both color and depth sensors, providiing a pair of images per frame. These are then combined to form a colored point cloud as the result.
Coded (Structured) Light # Coded (or structured) light works by projecting a known pattern onto the world, then analyzing the deformations of the pattern to determine the shapes of the surfaces it is projected on.
Coded light technology and the IntelÂ­Â® RealSenseâ„¢ Depth Camera SR305 Pros: Precise. High resolution, usually as high as the camera resolution it is using. Cons: Can be slow as it needs multiple patterns projected then captured to generate a single frame. Subject to interference from outside light, so usually better for indoor sensing. Time of Flight # Like the name implies, time of flight works by measuring the time it takes for a beam of light to do a round-trip to the sensor. This involves complex calculations using the speed of light, and the shorter the travel time, the nearer the object.
3D time of flight cameras Pros: Compact. (This is the technology used in many current generation phones) Fast, and ideal for real-time processing. Cons: Mid-level accuracy. Low resolution as it uses a custom sensor. Subject to interference from other devices. Stereo Vision # Stereo vision works like human depth perception, where two cameras (or eyes) are placed side by side looking in the same direction. Differences in the two captured images, called disparity, is used to determine how far from the sensors these different pixels are.
Semi-global Matching Method Pros: Tends to be cheap, as any off the shelf cameras can be used. Image representation is intuitive to humans. No interference. Cons: Low accuracy. Complex implementation requiring feature extraction and matching. Microsoft Kinect # Microsoft was the first company to bring depth sensors to the mass market with the Kinect for Xbox 360 in 2010. This was originally designed as a game controller, but gained a lot of interest from robotics engineers and creative coders, who hacked the device to use for custom Desktop applications.
Microsoft originally was against the practice, even threatening legal action against the hackers, but eventually changed course and embraced the community building around it.
ofxKinect 3D draw 001 from Memo Akten on Vimeo. Interactive Puppet Prototype with Xbox Kinect from Theo Watson on Vimeo. Kinect for Xbox 360 # Even though it was discontinued in 2013, this device is still used because of its long range and ease of use on all major OSes.
Technology: Coded light Range: 1.2-3.5m / 3.9-11.5ft Color resolution: 640x480px Depth resolution: 640x480px FOV: 57Â°x43Â° Extra features:
Microphone Motorized base to adjust device angle OF Support
ofxKinect Windows, Mac, Linux README for setup instructions Based on libfreenect Starfield from Lab212 on Vimeo. Kinect V2 (aka Kinect for Xbox One) # Released in 2012, the second version of the Kinect is said to have 3x the fidelity of its predecessor and a 60% wider field of view.
While still originally labeled as an Xbox controller, Microsoft shipped a Windows SDK with the device, providing access to advanced features to Desktop applications.
This device was discontinued in 2017 but is also still widely used for long-term installations.
Technology: Time-of-flight Range: 0.5-4.5m / 1.6-11.5ft Color resolution: 1920x1080px Depth resolution: 512x424px FOV: 70Â°x60Â° Extra features using Microsoft SDK:
Body tracking (up to 6 people) Facial expression recognition Hand gesture recognition Heart rate tracking Speech recognition OF Support
ofxKinectV2 Windows, Mac, Linux Based on libfreenect2 Note that this does not support any Microsoft SDK features like Body Tracking ofxKinectForWindows2 Windows only! Requires Microsoft SDK Parade - Dancing Shadow Sculptures from Dpt. on Vimeo. Kinect for Azure # Just released in 2019, the Kinect for Azure is based on technologies of the previous Kinect and the HoloLens. This is the first Kinect device marketed to developers at launch, and the first device to have an open-source SDK with official support for non-Windows platforms.
Technology: Time-of-flight Range: 0.25-5.5m / 0.8-11.5ft (mode dependent) Color resolution: Up to 3840x2160px Depth resolution: Up to 1024x1024px FOV: Up to 120Â°x120Â° Extra features using Microsoft SDK:
Orientation sensors 360Â° microphone array Body tracking (requires NVIDIA GPU) OF Support
ofxAzureKinect Windows, Linux Requires Azure Kinect Sensor SDK Intel RealSense # The Intel RealSense started off as a compact depth camera aimed at video conferencing, gesture-based interaction, and 3D scanning. The 200 series included many devices, both standalone and embedded in tablet computers and laptops.
The quality was not up-to-par with other depth cameras, and the original RealSense was rarely used for interactive installations.
In 2018, Intel released the 400 series devices, which were a major improvement on the previous generation. Low cost, small form-factor and portability make these devices a viable choice for many applications.
D415 / D435 / D455 / D457 # Technology: Stereo IR Range: 0.10-10m / 0.3-32ft (depends on conditions) Color resolution: 1920x1080px Depth resolution: 1280x720px FOV: 65Â°x40Â° (D415), 87Â°x58Â° (D435, D455, D457) Extra features:
USB powered Orientation sensors (on some models) OF Support
ofxLibRealSense2 Mac (Windows, Linux in theory) ofxRealSense2 Windows (Mac, Linux in theory) Other Options # Stereolabs ZED # Stereolabs are the newest addition to this list, and are worth mentioning because of their high quality ZED 2 sensors.
Technology: Stereo Color (range is virtually unlimited) Waterproof / dustproof options makes them great for outdoor use SDK uses machine learning models to provide a robust depth map and body tracking features Works on Windows and Linux but requires an NVIDIA GPU Orbbec Astra # Orbbec released the Astra Series as a response to the Microsoft Kinect. The goal was to create an open, cross-platform SDK which included body tracking with OpenNI.
Technology: Strucutured Light Leap Motion # The Leap Motion Controller is a depth sensor that focuses on hand and finger tracking. It can be used for both Desktop and VR applications.
Technology: Stereo IR USB Connections # We will often find ourselves wanting to connect many sensors to a single computer, or wanting to position our sensors far from the computer. This can be achieved using USB hubs and USB extenders, but one thing to remember is that not all USBs are created equal, and like most things in life, you get what you pay for.
In all cases, the most important thing we can do is test our setup with all the hardware connected to make sure everything is working as expected.
Bandwidth # USB bandwidth (amount of data over time) should be planned carefully:
Only enable feeds that are necessary to the application (e.g. Disable the RGB color stream if we are only interested in the depth data). Use a lower image resolution if it provides enough information (e.g. Test a lower resolution stream to see if it provides enough precision). If using multiple devices, connect them to different USB channels when possible (e.g. Connect one on the front and the other on the back). Hubs # USB hubs can help, but have limitations:
Powered USB hubs will have higher bandwidth as they can use more power. External hubs that connect to a PC using a USB connection will create a bottleneck (since all the data still needs to go through a single bus). Internal PCIe hubs with dedicated channels per port will work the best. Recommendation:
StarTech PCI Express Adapter Card Cables # USB cables will deteriorate the signal over distance.
Use cables that are close in length to what is needed. Cables that are too long will weaken the signal. Thicker and insulated cables tend to reduce interference. Active (powered) cables can boost the data signal, especially over long distances. Recommendations:
Cable Matters Active USB 3.0 extension cable Newnex active cables `}),e.add({id:17,href:"/docs/class-5/",title:"Class 5",description:"Class 5",content:""}),e.add({id:18,href:"/docs/class-5/pointers/",title:"Pointers",description:`So far in this course, every time we have wanted to reference some data, we have done so directly using variables.
ofImage dogImg; dogImg is a variable that can hold an ofImage. The variable is referencing the object itself, and we can access the variables and functions inside of it using the dot . operator.
dogImg.load(\u0026quot;dog-grass.jpg\u0026quot;); dogImg.draw(0, 0); Operators # In C++, we can also reference an object using a pointer variable.`,content:`So far in this course, every time we have wanted to reference some data, we have done so directly using variables.
ofImage dogImg; dogImg is a variable that can hold an ofImage. The variable is referencing the object itself, and we can access the variables and functions inside of it using the dot . operator.
dogImg.load(\u0026quot;dog-grass.jpg\u0026quot;); dogImg.draw(0, 0); Operators # In C++, we can also reference an object using a pointer variable. A pointer does not reference the object itself, but the address in memory where the object is stored.
Pointers have a special notation. To create a pointer variable, we need to add a star * right after the type when declaring the variable.
ofImage* dogImgPtr; âœŒï¸ Star Position
The star * can be positioned right after the variable type, right before the variable name, or by itself between the variable type and name.
The following are all equivalent and it is up to the programmer to determine what style they want to adhere to.
ofImage* dogImgPtr; // OK ofImage *dogImgPtr; // OK ofImage * dogImgPtr; // OK Reference variables are automatically allocated and created as soon as they are declared. All variables we have put in our ofApp header so far have been created right as the program started running.
Pointers on the other hand are not automatically created. They must be explicitly instantiated using the new operator.
dogImgPtr = new ofImage(); Pointers are also not automatically deleted. They must be explicitly destroyed using the delete operator when we are done with them. As a general rule, anything we create with new, we should eventually destroy with delete down the line.
delete dogImgPtr; To access the variables and functions contained inside an object referenced by a pointer, we have to use the arrow -\u0026gt; operator.
dogImgPtr-\u0026gt;load(\u0026quot;dog-grass.jpg\u0026quot;); dogImgPtr-\u0026gt;draw(0, 0); We can convert a reference to a pointer, this is called address-of . Putting an ampersand \u0026amp; operator in front of a reference variable will return its address in memory.
ofImage dogImg; ofImage* dogImgPtr = \u0026amp;dogImg; Conversely, we can access the variable referenced by a pointer directly, which is called dereferencing. Putting a star * in front of a pointer variable will return the object it references.
ofImage* dogImgPtr = new ofImage(); ofImage dogImg = *dogImgPtr; These operators are useful for passing variables to functions, when we are working with pointers but the function expects a reference or vice-versa.
Arrays # Arrays have a close relationship with pointers. In fact, these are interchangeable in most cases.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofVideoGrabber grabber; ofImage resultImg; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640 * 2, 480); grabber.setup(640, 480); resultImg.allocate(640, 480, OF_IMAGE_GRAYSCALE); } void ofApp::update() { grabber.update(); int threshold = 127; unsigned char* grabberData = grabber.getPixels().getData(); unsigned char* resultData = resultImg.getPixels().getData(); for (int i = 0; i \u0026lt; grabber.getWidth() * grabber.getHeight(); i++) { int r = grabberData[i * 3 + 0]; int g = grabberData[i * 3 + 1]; int b = grabberData[i * 3 + 2]; if (r \u0026gt; threshold \u0026amp;\u0026amp; g \u0026gt; threshold \u0026amp;\u0026amp; b \u0026gt; threshold) { resultData[i] = 255; } else { resultData[i] = 0; } } resultImg.update(); } void ofApp::draw() { grabber.draw(0, 0, 640, 480); resultImg.draw(640, 0, 640, 480); } An array reference is just a pointer to its first element. We can use pointer arithmetic to move this reference, and even to iterate through the array!
// ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(640 * 2, 480); grabber.setup(640, 480); resultImg.allocate(640, 480, OF_IMAGE_GRAYSCALE); } void ofApp::update() { grabber.update(); int threshold = 127; unsigned char* grabberPtr = grabber.getPixels().getData(); unsigned char* resultPtr = resultImg.getPixels().getData(); for (int i = 0; i \u0026lt; grabber.getWidth() * grabber.getHeight(); i++) { int r = *(grabberPtr++); int g = *(grabberPtr++); int b = *(grabberPtr++); if (r \u0026gt; threshold \u0026amp;\u0026amp; g \u0026gt; threshold \u0026amp;\u0026amp; b \u0026gt; threshold) { *resultPtr = 255; } else { *resultPtr = 0; } resultPtr++; } resultImg.update(); } void ofApp::draw() { grabber.draw(0, 0, 640, 480); resultImg.draw(640, 0, 640, 480); } Pros # More control over object creation.
We can choose when to create an object. For example, create a set of ofImage objects every time a new camera is plugged into the computer. We can choose how to create an object. Reference variables will use the default object constructor but pointer variables can use any available constructor. // Create an ofImage and load a file in one line. ofImage* dogImgPtr = new ofImage(\u0026quot;dog-grass.jpg\u0026quot;); Simple variable passing.
We have already seen that variables are passed by value by default. Whenever we use the assignment operator = or pass variables to a function, we are making copies of those variables. In some situations, we can pass the variable by reference with the \u0026amp; operator, but this is not always supported. When manipulating pointers, we are always just working with memory addresses, so we never inadvertently make copies of objects. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void draw(); bool loadImageVal(ofImage img, string file); bool loadImageRef(ofImage\u0026amp; img, string file); bool loadImagePtr(ofImage* img, string file); ofImage dogImg; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { loadImageVal(dogImg, \u0026quot;dog-grass.jpg\u0026quot;); //loadImageRef(dogImg, \u0026quot;dog-grass.jpg\u0026quot;); //loadImagePtr(\u0026amp;dogImg, \u0026quot;dog-grass.jpg\u0026quot;); } void ofApp::draw() { dogImg.draw(0, 0); } bool ofApp::loadImageVal(ofImage img, string file) { return img.load(file); } bool ofApp::loadImageRef(ofImage\u0026amp; img, string file) { return img.load(file); } bool ofApp::loadImagePtr(ofImage* img, string file) { return img-\u0026gt;load(file); } Cons # Memory management is up to the programmer.
It is up to us to make sure a pointer is referencing a valid object when using it. If we try to access an uninitialized pointer, our app will either crash or we will corrupt memory in other parts of the app. It is up to us to properly deallocate the memory when we are done. If we re-allocate a pointer before deleting its previous contents, we end up with a chunk of memory that is reserved but inaccessible. This is called a memory leak. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void draw(); void exit(); bool loadImageVal(ofImage img, string file); bool loadImageRef(ofImage\u0026amp; img, string file); bool loadImagePtr(ofImage* img, string file); ofImage* dogImgPtr; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { //dogImgPtr = new ofImage(); //loadImageVal(*dogImgPtr, \u0026quot;dog-grass.jpg\u0026quot;); //loadImageRef(*dogImgPtr, \u0026quot;dog-grass.jpg\u0026quot;); loadImagePtr(dogImgPtr, \u0026quot;dog-grass.jpg\u0026quot;); } void ofApp::exit() { delete dogImgPtr; } void ofApp::draw() { dogImgPtr-\u0026gt;draw(0, 0); } bool ofApp::loadImageVal(ofImage img, string file) { return img.load(file); } bool ofApp::loadImageRef(ofImage\u0026amp; img, string file) { return img.load(file); } bool ofApp::loadImagePtr(ofImage* img, string file) { return img-\u0026gt;load(file); } âœŒï¸ The exit() function in ofApp
The example above includes a new built-in ofApp method called exit(). As the name implies, this function is called right before the app quits. It is like a counterpart to the setup() function and is only called once.
This is a good place to do any cleanup before the app closes completely.
It is also up to us to make sure our pointer references are valid, which can become tricky when we have multiple pointers referencing the same object. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void draw(); void exit(); void mousePressed(int x, int y, int button); bool loadImagePtr(ofImage* img, string file); ofImage* dogImgPtrLoad; ofImage* dogImgPtrDraw; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { dogImgPtrLoad = new ofImage(); loadImagePtr(dogImgPtrLoad, \u0026quot;dog-grass.jpg\u0026quot;); dogImgPtrDraw = dogImgPtrLoad; } void ofApp::exit() { delete dogImgPtrLoad; dogImgPtrLoad = nullptr; } void ofApp::draw() { if (dogImgPtrDraw != nullptr) { dogImgPtrDraw-\u0026gt;draw(0, 0); } } void ofApp::mousePressed(int x, int y, int button) { delete dogImgPtrLoad; dogImgPtrLoad = nullptr; } bool ofApp::loadImagePtr(ofImage* img, string file) { return img-\u0026gt;load(file); } Shared Pointers # Shared pointers are a sort of middle ground between references and pointers. They are actually wrappers around pointers that handle all the memory management and reference counting for us, essentially getting rid of the cons listed above. This comes at a performance cost, but it will be negligeable in most cases.
Shared pointers belong to a family of smart pointers, but we will only focus on shared pointers as they are the most common.
In a nutshell, they work the following way:
A shared pointer is a templated type belonging to the std namespace: std::shared_ptr\u0026lt;T\u0026gt;. A pointer is created using std::make_shared\u0026lt;\u0026gt;() or by assigning another pointer to it. A pointer is destroyed using std::shared_ptr\u0026lt;T\u0026gt;.reset() or by simply setting it to nullptr. Every time a new pointer to the same object is added, the reference count increases. Every time a pointer to that object is removed, the reference count decreases. When the reference count hits 0, the object is destroyed. // ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void draw(); void mousePressed(int x, int y, int button); bool loadImagePtr(std::shared_ptr\u0026lt;ofImage\u0026gt; img, string file); shared_ptr\u0026lt;ofImage\u0026gt; dogImgPtrLoad; shared_ptr\u0026lt;ofImage\u0026gt; dogImgPtrDraw; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { dogImgPtrLoad = std::make_shared\u0026lt;ofImage\u0026gt;(); loadImagePtr(dogImgPtrLoad, \u0026quot;dog-grass.jpg\u0026quot;); dogImgPtrDraw = dogImgPtrLoad; } void ofApp::draw() { if (dogImgPtrDraw) { dogImgPtrDraw-\u0026gt;draw(0, 0); } } void ofApp::mousePressed(int x, int y, int button) { if (button == 0) { dogImgPtrLoad.reset(); } else { dogImgPtrDraw.reset(); } } bool ofApp::loadImagePtr(std::shared_ptr\u0026lt;ofImage\u0026gt; img, string file) { return img-\u0026gt;load(file); } `}),e.add({id:19,href:"/docs/class-5/depth-images/",title:"Depth Images",description:`Depth Grabbers # While depth sensors come in a variety of shapes, sizes, and technologies, the method to interface with them is usually pretty similar. Just like color cameras, depth cameras deliver images at the requested framerate. These images come in as arrays of pixels, which can then be manipulated and uploaded to a texture for rendering.
As these are specialty devices, we cannot use an ofVideoGrabber to retrieve data from them.`,content:`Depth Grabbers # While depth sensors come in a variety of shapes, sizes, and technologies, the method to interface with them is usually pretty similar. Just like color cameras, depth cameras deliver images at the requested framerate. These images come in as arrays of pixels, which can then be manipulated and uploaded to a texture for rendering.
As these are specialty devices, we cannot use an ofVideoGrabber to retrieve data from them. We will need to use a special \u0026ldquo;grabber\u0026rdquo; tailored to each device and its SDK.
âœŒï¸ How come ofVideoGrabber works for all webcams?
USB Video Class, or UVC, is a standard describing formats by which a device can stream video frames. Most USB webcams follow this standard and that is why we can use a common interface to access their streams.
ofVideoGrabber is therefore able to work with any USB video device that complies with the UVC standard. It is actually a wrapper with specific implementations based on the type of system we are on, for example AVFoundation on macOS, GStreamer on Linux, and Windows Media Foundation on Windows.
We fortunately will not have to implement this ourselves. In the same way that ofxCv acts as a bridge between OpenCV and openFrameworks, there are many OF addons we can use that manage the interface between the sensor SDK and openFrameworks.
Intel RealSense # ofxRealSense2 is a good choice for the Intel RealSense, as it gives us both pixel and texture access, as well as control over many of the SDK\u0026rsquo;s filtering options.
âš ï¸ Unfortunately, there seems to be an incompatibility with the Intel RealSense and macOS Monterey related to permissions. You may be unable to run the following code if you are using this operating system.
I am actively looking for solutions and will let the class know once I have more information.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxRealSense2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxRealSense2::Context rsContext; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 360); // true parameter starts the camera automatically. rsContext.setup(true); } void ofApp::update() { rsContext.update(); } void ofApp::draw() { // Try to get a pointer to a device. std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { // Draw the depth texture. rsDevice-\u0026gt;getDepthTex().draw(0, 0); } } A depth image will usually be single-channel, and therefore rendered in grayscale. Each gray value represents the distance of that pixel to the camera. The convention is usually brighter for nearer objects, but this is just representative.
RealSense Depth Image We can read the actual depth value using the SDK function ofxRealSense2::Device.getDistance(x, y). We will read the value under the mouse, and display it using ofDrawBitmapStringHighlight() for debugging.
// ofApp.cpp include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 360); rsContext.setup(true); } void ofApp::update() { rsContext.update(); } void ofApp::draw() { std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { rsDevice-\u0026gt;getDepthTex().draw(0, 0); float distAtMouse = rsDevice-\u0026gt;getDistance(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY()); } } The value returned by the SDK is probably read from the depth texture. Let\u0026rsquo;s try to read it directly from the pixels array and see if the values match.
Note that there are usually two available depth readings:
The raw depth buffer contains the actual depth measurement. The value can be read directly from the pixel value. The value is metric, usually in millimeters (1mm = 0.001m). The pixel format is unsigned short, with range 0 to 65535 (16-bit). The depth buffer contains a scaled representation of the pixel data. This is just for us to make sure everything is working, as the raw depth image is usually very dark or very bright. We should not use this for any depth readings. The pixel format is unsigned char, with range 0 to 255 (8-bit). This is the same as most RGB images. This is sometimes called the scaled or mapped image. We will therefore read our value from the raw depth texture.
// ofApp.cpp include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 360); rsContext.setup(true); } void ofApp::update() { rsContext.update(); } void ofApp::draw() { std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { rsDevice-\u0026gt;getDepthTex().draw(0, 0); // Get the point distance using the SDK function. float distAtMouse = rsDevice-\u0026gt;getDistance(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY() - 10); // Get the point depth using the texture directly. ofShortPixels rawDepthPix = rsDevice-\u0026gt;getRawDepthPix(); int depthAtMouse = rawDepthPix.getColor(ofGetMouseX(), ofGetMouseY()).r; ofDrawBitmapStringHighlight(ofToString(depthAtMouse), ofGetMouseX() + 16, ofGetMouseY() + 10); } } Note that we are getting an ofColor from the depth pixels, and reading the red channel with ofColor.r to get the depth value. We could use any of the red, green, blue channels here; as our data is in a single grayscale channel, all the colors represent the same value.
The Intel RealSense raw image tends to be very noisy, and needs some filtering to clean it up and make it usable. The SDK includes many options for filtering and these are available in the addon. To use them with ofxGui, we just need to add the ofxRealSense2::Device.params object to the ofxPanel.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; #include \u0026quot;ofxRealSense2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxRealSense2::Context rsContext; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 360); guiPanel.setup(\u0026quot;Depth\u0026quot;, \u0026quot;settings.json\u0026quot;); rsContext.setup(true); std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { guiPanel.add(rsDevice-\u0026gt;params); } } void ofApp::update() { rsContext.update(); } void ofApp::draw() { std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { rsDevice-\u0026gt;getDepthTex().draw(0, 0); float distAtMouse = rsDevice-\u0026gt;getDistance(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY()); } guiPanel.draw(); } Microsoft Kinect # ofxKinect is the best choice for the original Microsoft Kinect, as it ships with OF and gives us all the data we need.
Notice that the code looks almost similar to what we just did for the RealSense!
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxKinect.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxKinect kinect; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { kinect.init(); kinect.open(); } void ofApp::update() { kinect.update(); } void ofApp::draw() { if (kinect.isFrameNew()) { kinect.getDepthTexture().draw(0, 0); // Get the point distance using the SDK function. float distAtMouse = kinect.getDistanceAt(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY() - 10); // Get the point depth using the texture directly. ofShortPixels rawDepthPix = kinect.getRawDepthPixels(); int depthAtMouse = rawDepthPix.getColor(ofGetMouseX(), ofGetMouseY()).r; ofDrawBitmapStringHighlight(ofToString(depthAtMouse), ofGetMouseX() + 16, ofGetMouseY() + 10); } } Microsoft Kinect V2 # ofxKinectForWindows2 is a good choice for the Kinect V2. It works with the [Microsoft Kinect for Windows 2.0 SDK], which means it supports all Kinect features (including body tracking). However, note that this only works on Windows!
ofxKinectForWindows2 does not include a function to get distance from a coordinate, so we will need to sample the depth texture directly.
// ofApp.h #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxKinectForWindows2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxKFW2::Device kinect; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { kinect.open(); kinect.initDepthSource(); kinect.initColorSource(); } void ofApp::update() { kinect.update(); } void ofApp::draw() { if (kinect.isFrameNew()) { std::shared_ptr\u0026lt;ofxKFW2::Source::Depth\u0026gt; depthSource = kinect.getDepthSource(); // Clamp the mouse coordinate to ensure it stays within the data bounds. int readX = ofClamp(ofGetMouseX(), 0, depthSource-\u0026gt;getWidth() - 1); int readY = ofClamp(ofGetMouseY(), 0, depthSource-\u0026gt;getHeight() - 1); // Get the point depth using the texture directly. ofShortPixels rawDepthPix = depthSource-\u0026gt;getPixels(); int depthAtMouse = rawDepthPix.getColor(readX, readY).r; ofDrawBitmapStringHighlight(ofToString(depthAtMouse), ofGetMouseX(), ofGetMouseY()); } } Alternatively, ofxKinectV2 is a cross-platform solution that works similarly to ofxKinect. The code to sample the distance under the mouse is very similar.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxKinectV2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxKinectV2 kinect; ofTexture depthTex; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(512, 424); // Use a settings object to configure the device. ofxKinectV2::Settings settings; settings.enableRGB = false; settings.enableDepth = true; kinect.open(0, settings); } void ofApp::update() { kinect.update(); // Only load the data if there is a new frame to process. if (kinect.isFrameNew()) { depthTex.loadData(kinect.getDepthPixels()); } } void ofApp::draw() { depthTex.draw(0, 0); // Get the point distance using the SDK function (in meters). float distAtMouse = kinect.getDistanceAt(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY() - 10); // Get the point depth using the texture directly (in millimeters). const ofFloatPixels\u0026amp; rawDepthPix = kinect.getRawDepthPixels(); int depthAtMouse = rawDepthPix.getColor(ofGetMouseX(), ofGetMouseY()).r; ofDrawBitmapStringHighlight(ofToString(depthAtMouse), ofGetMouseX() + 16, ofGetMouseY() + 10); } Some notes to consider:
The device is configured using a settings object of type ofxKinectV2::Settings. This is a common pattern in openFrameworks we will encounter again. ofxKinectV2 does not provide textures for the data, so we need to use our own and load it with pixel data in update(). We use isFrameNew() to check if there is new data to upload on each frame. The SDK function getDistanceAt() returns the distance in meters but the raw pixel data returns the data in millimeters. The depth data is also using float pixels instead of the more common short. âœŒï¸ The const qualifier
const is a qualifier that can be used on a variable to indicate that it will not change; that it will remain constant.
In the example above, we are creating a temporary variable for the pixel data from getRawDepthPixels(). We do not want to make a copy of this data, so we use the \u0026amp; when declaring the variable to indicate it will be a reference. This data should not be modified by the programmer because it comes directly from the device. getRawDepthPixels() indicates this in its return type; it requires any reference to be constant.
Depth Threshold # Depth pixels are very useful for thresholding images. This tends to be much more precise than using brightness or color (as we have been doing previously) since we eliminate any issues with change in lighting or with similarities between background and foreground colors. We can set a depth range that valid pixels belong to and discard anything that\u0026rsquo;s nearer or farther than this range.
Let\u0026rsquo;s first attempt to do this manually by iterating through the pixel array.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; #include \u0026quot;ofxRealSense2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxRealSense2::Context rsContext; ofImage thresholdImg; ofParameter\u0026lt;int\u0026gt; nearThreshold; ofParameter\u0026lt;int\u0026gt; farThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 720); // Start the sensor. rsContext.setup(true); // Allocate the image. thresholdImg.allocate(640, 360, OF_IMAGE_GRAYSCALE); // Setup the parameters. nearThreshold.set(\u0026quot;Near Threshold\u0026quot;, 10, 0, 4000); farThreshold.set(\u0026quot;Far Threshold\u0026quot;, 1000, 0, 4000); // Setup the gui. guiPanel.setup(\u0026quot;Depth Threshold\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(nearThreshold); guiPanel.add(farThreshold); } void ofApp::update() { rsContext.update(); } void ofApp::draw() { std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { rsDevice-\u0026gt;getDepthTex().draw(0, 0); // Get the point distance using the SDK function. float distAtMouse = rsDevice-\u0026gt;getDistance(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY()); // Threshold the depth. ofShortPixels rawDepthPix = rsDevice-\u0026gt;getRawDepthPix(); ofPixels\u0026amp; thresholdPix = thresholdImg.getPixels(); for (int y = 0; y \u0026lt; rawDepthPix.getHeight(); y++) { for (int x = 0; x \u0026lt; rawDepthPix.getWidth(); x++) { int depth = rawDepthPix.getColor(x, y).r; if (nearThreshold \u0026lt; depth \u0026amp;\u0026amp; depth \u0026lt; farThreshold) { thresholdPix.setColor(x, y, ofColor(255)); } else { thresholdPix.setColor(x, y, ofColor(0)); } } } // Upload pixels to texture. thresholdImg.update(); // Draw the result image. thresholdImg.draw(0, 360); } // Draw the gui. guiPanel.draw(); } RealSense Threshold We can also achieve the same effect using OpenCV with two consecutive calls to cv::threshold().
The first will be for the near value, and will keep everything greater than the threshold value. The second will be for the far value, and will be inverted so that we keep everything smaller than the threshold value. We then combine the result of both operations using cv::bitwise_and(), which just adds both textures together. Unfortunately, OpenCV does not work with unsigned short images, so we cannot use our array of depth pixels directly. We first need to convert it either to an array of unsigned char or float.
If we go with unsigned char, we will lose precision because we will need to pack 65536 possible values into 256. We should therefore use float and ofFloatPixels. In both cases, if we let OF do the conversion automatically, it will rescale the values to fit into the new range. So [0, 65535] becomes [0, 255] or [0.0, 1.0]. We need to remain aware of this as it will change the range of our near/far parameters. Here is a second thresholding attempt using OpenCV.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; #include \u0026quot;ofxRealSense2.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxRealSense2::Context rsContext; ofImage thresholdImg; ofParameter\u0026lt;float\u0026gt; nearThreshold; ofParameter\u0026lt;float\u0026gt; farThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { // Default RS resolution. ofSetWindowShape(640, 720); // Start the sensor. rsContext.setup(true); // Setup the parameters. nearThreshold.set(\u0026quot;Near Threshold\u0026quot;, 0.01f, 0.0f, 0.1f); farThreshold.set(\u0026quot;Far Threshold\u0026quot;, 0.02f, 0.0f, 0.1f); // Setup the gui. guiPanel.setup(\u0026quot;Depth Threshold\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(nearThreshold); guiPanel.add(farThreshold); } void ofApp::update() { rsContext.update(); } void ofApp::draw() { std::shared_ptr\u0026lt;ofxRealSense2::Device\u0026gt; rsDevice = rsContext.getDevice(0); if (rsDevice) { rsDevice-\u0026gt;getDepthTex().draw(0, 0); // Get the point distance using the SDK function. float distAtMouse = rsDevice-\u0026gt;getDistance(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY()); // Threshold the depth. ofFloatPixels rawDepthPix = rsDevice-\u0026gt;getRawDepthPix(); ofFloatPixels thresholdNear, thresholdFar, thresholdResult; ofxCv::threshold(rawDepthPix, thresholdNear, nearThreshold); ofxCv::threshold(rawDepthPix, thresholdFar, farThreshold, true); ofxCv::bitwise_and(thresholdNear, thresholdFar, thresholdResult); // Upload pixels to image. thresholdImg.setFromPixels(thresholdResult); // Draw the result image. thresholdImg.draw(0, 360); } // Draw the gui. guiPanel.draw(); } And here is that same example using a Kinect and OpenCV.
// ofApp.h #pragma once #include \u0026quot;ofMain.h\u0026quot; #include \u0026quot;ofxCv.h\u0026quot; #include \u0026quot;ofxGui.h\u0026quot; #include \u0026quot;ofxKinect.h\u0026quot; class ofApp : public ofBaseApp { public: void setup(); void update(); void draw(); ofxKinect kinect; ofImage thresholdImg; ofParameter\u0026lt;float\u0026gt; nearThreshold; ofParameter\u0026lt;float\u0026gt; farThreshold; ofxPanel guiPanel; }; // ofApp.cpp #include \u0026quot;ofApp.h\u0026quot; void ofApp::setup() { ofSetWindowShape(1280, 480); // Start the depth sensor. kinect.setRegistration(true); kinect.init(); kinect.open(); // Setup the parameters. nearThreshold.set(\u0026quot;Near Threshold\u0026quot;, 0.01f, 0.0f, 0.1f); farThreshold.set(\u0026quot;Far Threshold\u0026quot;, 0.02f, 0.0f, 0.1f); // Setup the gui. guiPanel.setup(\u0026quot;Depth Threshold\u0026quot;, \u0026quot;settings.json\u0026quot;); guiPanel.add(nearThreshold); guiPanel.add(farThreshold); } void ofApp::update() { kinect.update(); } void ofApp::draw() { if (kinect.isFrameNew()) { // Get the point distance using the SDK function. float distAtMouse = kinect.getDistanceAt(ofGetMouseX(), ofGetMouseY()); ofDrawBitmapStringHighlight(ofToString(distAtMouse, 3), ofGetMouseX(), ofGetMouseY()); // Threshold the depth. ofFloatPixels rawDepthPix = kinect.getRawDepthPixels(); ofFloatPixels thresholdNear, thresholdFar, thresholdResult; ofxCv::threshold(rawDepthPix, thresholdNear, nearThreshold); ofxCv::threshold(rawDepthPix, thresholdFar, farThreshold, true); ofxCv::bitwise_and(thresholdNear, thresholdFar, thresholdResult); // Upload pixels to image. thresholdImg.setFromPixels(thresholdResult); } // Draw the source image. kinect.getDepthTexture().draw(0, 0); // Draw the result image. thresholdImg.draw(640, 0); // Draw the gui. guiPanel.draw(); } `}),e.add({id:20,href:"/docs/assignments/",title:"Assignments",description:"Assignments",content:""}),e.add({id:21,href:"/docs/",title:"Docs",description:"Seeing Machines Docs",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()